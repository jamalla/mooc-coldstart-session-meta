{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f67192f",
   "metadata": {},
   "source": [
    "Bootstrap: locate repo root (Windows-safe) + basic env info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5f9f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\\notebooks\n",
      "Python: 3.11.14\n",
      "Platform: Windows-10-10.0.22621-SP0\n",
      "pandas: 2.3.3\n",
      "REPO_ROOT: D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\n",
      "DATA_DIR: D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\\data\n",
      "SUP_DIR: D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\\data\\processed\\supervised\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05A-00] Bootstrap: locate repo root (Windows-safe) + basic env info\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CWD = Path.cwd().resolve()\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(CWD)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "SUP_DIR  = PROC_DIR / \"supervised\"\n",
    "SUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"SUP_DIR:\", SUP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2c7c0",
   "metadata": {},
   "source": [
    "Config: input prefix-samples file + derive RUN_TAG + split params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0159e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_PREFIX: D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\\data\\processed\\supervised\\target_prefix_samples_20251229_163357.parquet\n",
      "RUN_TAG: 20251229_163357\n",
      "OUT_TRAIN: target_prefix_train_20251229_163357.parquet\n",
      "OUT_VAL: target_prefix_val_20251229_163357.parquet\n",
      "OUT_TEST: target_prefix_test_20251229_163357.parquet\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05A-01] Config: input prefix-samples file + derive RUN_TAG + split params\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Use your real produced file (from screenshot)\n",
    "IN_PREFIX = SUP_DIR / \"target_prefix_samples_20251229_163357.parquet\"\n",
    "\n",
    "if not IN_PREFIX.exists():\n",
    "    raise FileNotFoundError(f\"Missing IN_PREFIX: {IN_PREFIX.resolve()}\")\n",
    "\n",
    "# Derive RUN_TAG from filename suffix like ..._YYYYMMDD_HHMMSS\n",
    "m = re.search(r\"target_prefix_samples_(\\d{8}_\\d{6})$\", IN_PREFIX.stem)\n",
    "RUN_TAG = m.group(1) if m else \"run\"\n",
    "print(\"IN_PREFIX:\", IN_PREFIX.resolve())\n",
    "print(\"RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "# Split ratios (session-level, no leakage)\n",
    "SPLIT = {\n",
    "    \"train\": 0.80,\n",
    "    \"val\":   0.10,\n",
    "    \"test\":  0.10,\n",
    "}\n",
    "assert abs(sum(SPLIT.values()) - 1.0) < 1e-9\n",
    "\n",
    "# Deterministic assignment seed (only used for hashing fallback)\n",
    "SEED = 20251229\n",
    "\n",
    "# Output files\n",
    "OUT_TRAIN = SUP_DIR / f\"target_prefix_train_{RUN_TAG}.parquet\"\n",
    "OUT_VAL   = SUP_DIR / f\"target_prefix_val_{RUN_TAG}.parquet\"\n",
    "OUT_TEST  = SUP_DIR / f\"target_prefix_test_{RUN_TAG}.parquet\"\n",
    "\n",
    "print(\"OUT_TRAIN:\", OUT_TRAIN.name)\n",
    "print(\"OUT_VAL:\", OUT_VAL.name)\n",
    "print(\"OUT_TEST:\", OUT_TEST.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8ba7f",
   "metadata": {},
   "source": [
    "Load prefix samples + schema checks + quick profiling  (CHECKPOINT 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822a89c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (2333, 9)\n",
      "Columns: ['domain', 'user_id', 'session_id', 't', 'prefix_items', 'prefix_len', 'label_item', 'start_ts', 'end_ts']\n",
      "\n",
      "Optional present: {'session_length': False, 't': True, 'start_ts': True, 'end_ts': True}\n",
      "\n",
      "Session duration (sec) quantiles: {0.5: 1814.0, 0.9: 6018.0, 0.99: 13098.0}\n",
      "\n",
      "Stats:\n",
      "  rows: 2,333\n",
      "  sessions: 561\n",
      "  users: 378\n",
      "  unique label items: 704\n",
      "\n",
      "Domain counts: {'target': 2333}\n",
      "\n",
      "Head (selected cols):\n",
      "   domain  user_id  session_id  t  prefix_len  label_item  \\\n",
      "0  target   104074  t_104074_2  1           1       52616   \n",
      "1  target   104074  t_104074_2  2           2       52615   \n",
      "2  target   104074  t_104074_2  3           3       52610   \n",
      "\n",
      "                   start_ts                    end_ts  \n",
      "0 2019-07-22 15:27:08+00:00 2019-07-22 15:52:02+00:00  \n",
      "1 2019-07-22 15:27:08+00:00 2019-07-22 15:52:02+00:00  \n",
      "2 2019-07-22 15:27:08+00:00 2019-07-22 15:52:02+00:00  \n",
      "\n",
      "Load+checks seconds: 0.17\n",
      "\n",
      "CHECKPOINT 1 ✅\n",
      "Please paste the output of this cell (Loaded/Columns/Stats/Domain counts + Optional present).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05A-02] Load prefix samples + schema checks + quick profiling  (CHECKPOINT 1) — FIXED\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "t0 = time.time()\n",
    "df = pd.read_parquet(IN_PREFIX)\n",
    "print(\"Loaded:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Required columns (minimum viable for split + training later)\n",
    "required = [\n",
    "    \"domain\", \"user_id\", \"session_id\",\n",
    "    \"prefix_items\", \"label_item\", \"prefix_len\"\n",
    "]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Optional columns we may have\n",
    "optional_cols = [\"session_length\", \"t\", \"start_ts\", \"end_ts\"]\n",
    "print(\"\\nOptional present:\", {c: (c in df.columns) for c in optional_cols})\n",
    "\n",
    "# Parse timestamps if present (for duration logs)\n",
    "if \"start_ts\" in df.columns and \"end_ts\" in df.columns:\n",
    "    df[\"start_ts\"] = pd.to_datetime(df[\"start_ts\"], utc=True, errors=\"coerce\")\n",
    "    df[\"end_ts\"]   = pd.to_datetime(df[\"end_ts\"], utc=True, errors=\"coerce\")\n",
    "    bad = int(df[\"start_ts\"].isna().sum() + df[\"end_ts\"].isna().sum())\n",
    "    if bad:\n",
    "        print(f\"⚠️  Warning: {bad} bad timestamps in start_ts/end_ts (NaT).\")\n",
    "    else:\n",
    "        df[\"session_duration_sec\"] = (df[\"end_ts\"] - df[\"start_ts\"]).dt.total_seconds()\n",
    "        dur = df[\"session_duration_sec\"].dropna()\n",
    "        if len(dur):\n",
    "            print(\"\\nSession duration (sec) quantiles:\",\n",
    "                  dur.quantile([0.5, 0.9, 0.99]).to_dict())\n",
    "\n",
    "# High-level stats\n",
    "n_rows = len(df)\n",
    "n_sessions = df[\"session_id\"].nunique()\n",
    "n_users = df[\"user_id\"].nunique()\n",
    "n_items_label = df[\"label_item\"].nunique()\n",
    "\n",
    "print(\"\\nStats:\")\n",
    "print(\"  rows:\", f\"{n_rows:,}\")\n",
    "print(\"  sessions:\", f\"{n_sessions:,}\")\n",
    "print(\"  users:\", f\"{n_users:,}\")\n",
    "print(\"  unique label items:\", f\"{n_items_label:,}\")\n",
    "\n",
    "# Domain distribution\n",
    "print(\"\\nDomain counts:\", df[\"domain\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Extra debug: peek at first 3 rows (selected cols)\n",
    "peek_cols = [c for c in [\"domain\",\"user_id\",\"session_id\",\"t\",\"prefix_len\",\"label_item\",\"start_ts\",\"end_ts\"] if c in df.columns]\n",
    "print(\"\\nHead (selected cols):\")\n",
    "print(df[peek_cols].head(3))\n",
    "\n",
    "print(\"\\nLoad+checks seconds:\", round(time.time() - t0, 2))\n",
    "\n",
    "print(\"\\nCHECKPOINT 1 ✅\")\n",
    "print(\"Please paste the output of this cell (Loaded/Columns/Stats/Domain counts + Optional present).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db134556",
   "metadata": {},
   "source": [
    "Deterministic session-level split assignment (no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139fd2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sessions: 561\n",
      "Split counts (rows): {'train': 1944, 'test': 200, 'val': 189}\n",
      "Split counts (sessions): {'train': 446, 'val': 60, 'test': 55}\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05A-03] Deterministic session-level split assignment (no leakage)\n",
    "\n",
    "import hashlib\n",
    "\n",
    "# We split by session_id so no prefix from same session leaks into different splits.\n",
    "sessions = df[\"session_id\"].astype(str).unique()\n",
    "print(\"Unique sessions:\", len(sessions))\n",
    "\n",
    "# Deterministic hash -> uniform [0,1)\n",
    "def session_u01(s: str) -> float:\n",
    "    h = hashlib.md5((str(SEED) + \"||\" + s).encode(\"utf-8\")).hexdigest()\n",
    "    # use first 8 hex chars -> 32-bit int\n",
    "    v = int(h[:8], 16)\n",
    "    return v / 0xFFFFFFFF\n",
    "\n",
    "u = np.array([session_u01(s) for s in sessions], dtype=np.float64)\n",
    "\n",
    "# Map to split buckets\n",
    "train_cut = SPLIT[\"train\"]\n",
    "val_cut = SPLIT[\"train\"] + SPLIT[\"val\"]\n",
    "\n",
    "sess_split = {}\n",
    "for sid, r in zip(sessions, u):\n",
    "    if r < train_cut:\n",
    "        sess_split[sid] = \"train\"\n",
    "    elif r < val_cut:\n",
    "        sess_split[sid] = \"val\"\n",
    "    else:\n",
    "        sess_split[sid] = \"test\"\n",
    "\n",
    "# Attach split column\n",
    "df[\"split\"] = df[\"session_id\"].astype(str).map(sess_split)\n",
    "\n",
    "# Sanity: any unmapped?\n",
    "null_split = df[\"split\"].isna().sum()\n",
    "if null_split:\n",
    "    raise ValueError(f\"Found {null_split} rows with NULL split (unexpected).\")\n",
    "\n",
    "print(\"Split counts (rows):\", df[\"split\"].value_counts().to_dict())\n",
    "print(\"Split counts (sessions):\", df.drop_duplicates(\"session_id\")[\"split\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d928e",
   "metadata": {},
   "source": [
    "Split QA: overlaps, per-split stats, label coverage  (CHECKPOINT 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e8a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'rows': 1944, 'sessions': 446, 'users': 307, 'label_items': 650, 'domain_counts': {'target': 1944}, 'avg_prefix_len': 6.370884773662551, 'max_prefix_len': 20, 'median_session_duration_sec': 1785.0, 'p90_session_duration_sec': 5873.0}\n",
      "Val: {'rows': 189, 'sessions': 60, 'users': 58, 'label_items': 153, 'domain_counts': {'target': 189}, 'avg_prefix_len': 5.243386243386244, 'max_prefix_len': 20, 'median_session_duration_sec': 2146.0, 'p90_session_duration_sec': 6448.0}\n",
      "Test: {'rows': 200, 'sessions': 55, 'users': 51, 'label_items': 158, 'domain_counts': {'target': 200}, 'avg_prefix_len': 7.355, 'max_prefix_len': 20, 'median_session_duration_sec': 1934.0, 'p90_session_duration_sec': 13098.0}\n",
      "\n",
      "Leakage checks (sessions):\n",
      "  train∩val: 0\n",
      "  train∩test: 0\n",
      "  val∩test: 0\n",
      "\n",
      "Label coverage:\n",
      "  train labels: 650\n",
      "  val labels: 153 | not in train: 23\n",
      "  test labels: 158 | not in train: 31\n",
      "\n",
      "CHECKPOINT 2 ✅\n",
      "Please paste the output of this cell (stats + leakage checks + label coverage).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05A-04] Split QA: overlaps, per-split stats, label coverage  (CHECKPOINT 2) — FIXED\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def split_stats(name: str, part: pd.DataFrame):\n",
    "    d = {\n",
    "        \"rows\": len(part),\n",
    "        \"sessions\": part[\"session_id\"].nunique(),\n",
    "        \"users\": part[\"user_id\"].nunique(),\n",
    "        \"label_items\": part[\"label_item\"].nunique(),\n",
    "        \"domain_counts\": part[\"domain\"].value_counts().to_dict(),\n",
    "        \"avg_prefix_len\": float(part[\"prefix_len\"].mean()) if len(part) else 0.0,\n",
    "        \"max_prefix_len\": int(part[\"prefix_len\"].max()) if len(part) else 0,\n",
    "    }\n",
    "\n",
    "    # duration if available\n",
    "    if \"start_ts\" in part.columns and \"end_ts\" in part.columns:\n",
    "        st = pd.to_datetime(part[\"start_ts\"], utc=True, errors=\"coerce\")\n",
    "        et = pd.to_datetime(part[\"end_ts\"], utc=True, errors=\"coerce\")\n",
    "        dur = (et - st).dt.total_seconds().dropna()\n",
    "        if len(dur):\n",
    "            d[\"median_session_duration_sec\"] = float(dur.median())\n",
    "            d[\"p90_session_duration_sec\"] = float(dur.quantile(0.9))\n",
    "    return d\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"].copy()\n",
    "val_df   = df[df[\"split\"] == \"val\"].copy()\n",
    "test_df  = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "print(\"Train:\", split_stats(\"train\", train_df))\n",
    "print(\"Val:\",   split_stats(\"val\",   val_df))\n",
    "print(\"Test:\",  split_stats(\"test\",  test_df))\n",
    "\n",
    "# No leakage check: session sets disjoint\n",
    "s_tr = set(train_df[\"session_id\"].astype(str).unique())\n",
    "s_va = set(val_df[\"session_id\"].astype(str).unique())\n",
    "s_te = set(test_df[\"session_id\"].astype(str).unique())\n",
    "\n",
    "print(\"\\nLeakage checks (sessions):\")\n",
    "print(\"  train∩val:\", len(s_tr & s_va))\n",
    "print(\"  train∩test:\", len(s_tr & s_te))\n",
    "print(\"  val∩test:\", len(s_va & s_te))\n",
    "\n",
    "if len(s_tr & s_va) or len(s_tr & s_te) or len(s_va & s_te):\n",
    "    raise AssertionError(\"Session leakage detected: splits share session_id(s).\")\n",
    "\n",
    "# Label coverage gaps (val/test labels not seen in train)\n",
    "train_labels = set(train_df[\"label_item\"].astype(str).unique())\n",
    "val_labels   = set(val_df[\"label_item\"].astype(str).unique())\n",
    "test_labels  = set(test_df[\"label_item\"].astype(str).unique())\n",
    "\n",
    "print(\"\\nLabel coverage:\")\n",
    "print(\"  train labels:\", len(train_labels))\n",
    "print(\"  val labels:\", len(val_labels), \"| not in train:\", len(val_labels - train_labels))\n",
    "print(\"  test labels:\", len(test_labels), \"| not in train:\", len(test_labels - train_labels))\n",
    "\n",
    "print(\"\\nCHECKPOINT 2 ✅\")\n",
    "print(\"Please paste the output of this cell (stats + leakage checks + label coverage).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e87a35",
   "metadata": {},
   "source": [
    "Write split parquets (train/val/test) + final confirmations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4ff4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      "  D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\\data\\processed\\supervised\\target_prefix_train_20251229_163357.parquet\n",
      "  D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\\data\\processed\\supervised\\target_prefix_val_20251229_163357.parquet\n",
      "  D:\\00_DS-ML-Workspace\\mooc-coldstart-session-meta\\data\\processed\\supervised\\target_prefix_test_20251229_163357.parquet\n",
      "\n",
      "Re-load counts:\n",
      "  train: (1944, 11) sessions: 446\n",
      "  val: (189, 11) sessions: 60\n",
      "  test: (200, 11) sessions: 55\n",
      "\n",
      "Done ✅ 05A split created.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05A-05] Write split parquets (train/val/test) + final confirmations\n",
    "\n",
    "# Drop helper column before writing if you want; I keep it because it helps debugging.\n",
    "# If you prefer clean schema, uncomment the next 3 lines.\n",
    "# train_df = train_df.drop(columns=[\"split\"])\n",
    "# val_df   = val_df.drop(columns=[\"split\"])\n",
    "# test_df  = test_df.drop(columns=[\"split\"])\n",
    "\n",
    "train_df.to_parquet(OUT_TRAIN, index=False)\n",
    "val_df.to_parquet(OUT_VAL, index=False)\n",
    "test_df.to_parquet(OUT_TEST, index=False)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" \", OUT_TRAIN.resolve())\n",
    "print(\" \", OUT_VAL.resolve())\n",
    "print(\" \", OUT_TEST.resolve())\n",
    "\n",
    "# Confirm readable and counts match\n",
    "rt = pd.read_parquet(OUT_TRAIN)\n",
    "rv = pd.read_parquet(OUT_VAL)\n",
    "rs = pd.read_parquet(OUT_TEST)\n",
    "\n",
    "print(\"\\nRe-load counts:\")\n",
    "print(\"  train:\", rt.shape, \"sessions:\", rt[\"session_id\"].nunique())\n",
    "print(\"  val:\",   rv.shape, \"sessions:\", rv[\"session_id\"].nunique())\n",
    "print(\"  test:\",  rs.shape, \"sessions:\", rs[\"session_id\"].nunique())\n",
    "\n",
    "print(\"\\nDone ✅ 05A split created.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
