{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e611ecd8",
   "metadata": {},
   "source": [
    "Imports + env info (SASRec baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4e3f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-00] Imports OK\n",
      "[10-00] torch: 2.9.1+cpu\n",
      "[10-00] pandas: 2.3.3\n",
      "[10-00] numpy: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-00] Imports + env info (SASRec baseline)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"[10-00] Imports OK\")\n",
    "print(\"[10-00] torch:\", torch.__version__)\n",
    "print(\"[10-00] pandas:\", pd.__version__)\n",
    "print(\"[10-00] numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ec175",
   "metadata": {},
   "source": [
    "Locate repo root + fixed upstream run tags + load protocol/config artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0813925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[10-01] RUN_TAG: 20260102_233834\n",
      "[10-01] Loaded dataloader_config keys: ['target', 'source', 'protocol']\n",
      "[10-01] Loaded sanity_metrics keys: ['run_tag_target', 'run_tag_source', 'created_at', 'target', 'source', 'notes']\n",
      "[10-01] Loaded session_gap_thresholds keys: ['generated_from_run_tag', 'generated_at', 'target', 'source', 'decision_notes']\n",
      "[10-01] ✅ Session gaps confirmed: target=30m, source=10m\n",
      "[10-01] Protocol from 06:\n",
      "  K_LIST: [5, 10, 20]\n",
      "  MAX_PREFIX_LEN: 20\n",
      "  CAP_ENABLED: True\n",
      "  CAP_SESSION_LEN: 200\n",
      "  CAP_STRATEGY: take_last\n",
      "\n",
      "[10-01] CHECKPOINT A\n",
      "Confirm JSON loads + gap asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-01] Locate repo root + fixed upstream run tags + load protocol/config artifacts\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists() and (p / \"meta.json\").exists():\n",
    "            return p\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not locate repo root (expected PROJECT_STATE.md).\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
    "print(\"[10-01] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[10-01] RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "# Fixed upstream run tags (do NOT change)\n",
    "TARGET_TAG = \"20251229_163357\"\n",
    "SOURCE_TAG = \"20251229_232834\"\n",
    "\n",
    "def load_json(path: Path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "cfg_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"dataloader_config_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "sanity_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"sanity_metrics_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "gaps_path_repo = REPO_ROOT / \"data/processed/normalized_events\" / \"session_gap_thresholds.json\"\n",
    "\n",
    "dataloader_cfg = load_json(cfg_path_repo)\n",
    "sanity_metrics = load_json(sanity_path_repo)\n",
    "session_gaps = load_json(gaps_path_repo)\n",
    "\n",
    "print(\"[10-01] Loaded dataloader_config keys:\", list(dataloader_cfg.keys()))\n",
    "print(\"[10-01] Loaded sanity_metrics keys:\", list(sanity_metrics.keys()))\n",
    "print(\"[10-01] Loaded session_gap_thresholds keys:\", list(session_gaps.keys()))\n",
    "\n",
    "# Enforce fixed decisions\n",
    "assert session_gaps[\"target\"][\"primary_threshold_seconds\"] == 1800, \"Target gap must be 30m (1800s).\"\n",
    "assert session_gaps[\"source\"][\"primary_threshold_seconds\"] == 600, \"Source gap must be 10m (600s).\"\n",
    "print(\"[10-01] ✅ Session gaps confirmed: target=30m, source=10m\")\n",
    "\n",
    "proto = dataloader_cfg[\"protocol\"]\n",
    "K_LIST = [5, 10, 20]\n",
    "MAX_K = max(K_LIST)\n",
    "\n",
    "MAX_PREFIX_LEN = int(proto[\"max_prefix_len\"])\n",
    "CAP_ENABLED = bool(proto[\"source_long_session_policy\"][\"enabled\"])\n",
    "CAP_SESSION_LEN = int(proto[\"source_long_session_policy\"][\"cap_session_len\"])\n",
    "CAP_STRATEGY = str(proto[\"source_long_session_policy\"][\"cap_strategy\"])\n",
    "\n",
    "print(\"[10-01] Protocol from 06:\")\n",
    "print(\"  K_LIST:\", K_LIST)\n",
    "print(\"  MAX_PREFIX_LEN:\", MAX_PREFIX_LEN)\n",
    "print(\"  CAP_ENABLED:\", CAP_ENABLED)\n",
    "print(\"  CAP_SESSION_LEN:\", CAP_SESSION_LEN)\n",
    "print(\"  CAP_STRATEGY:\", CAP_STRATEGY)\n",
    "\n",
    "print(\"\\n[10-01] CHECKPOINT A\")\n",
    "print(\"Confirm JSON loads + gap asserts passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019195b",
   "metadata": {},
   "source": [
    "Resolve artifact paths (target tensors + vocabs) + existence checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1dccec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-02] ✅ All required artifacts exist\n",
      "\n",
      "[10-02] CHECKPOINT B\n",
      "If any artifact missing, STOP and paste the error.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-02] Resolve artifact paths (target tensors + vocabs) + existence checks\n",
    "\n",
    "def must_exist(p: Path, label: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "    return p\n",
    "\n",
    "TARGET_TENSOR_DIR = REPO_ROOT / \"data/processed/tensor_target\"\n",
    "target_train_pt = TARGET_TENSOR_DIR / f\"target_tensor_train_{TARGET_TAG}.pt\"\n",
    "target_val_pt   = TARGET_TENSOR_DIR / f\"target_tensor_val_{TARGET_TAG}.pt\"\n",
    "target_test_pt  = TARGET_TENSOR_DIR / f\"target_tensor_test_{TARGET_TAG}.pt\"\n",
    "target_vocab_json = TARGET_TENSOR_DIR / f\"target_vocab_items_{TARGET_TAG}.json\"\n",
    "\n",
    "for p, lbl in [\n",
    "    (target_train_pt, \"target_train_pt\"),\n",
    "    (target_val_pt, \"target_val_pt\"),\n",
    "    (target_test_pt, \"target_test_pt\"),\n",
    "    (target_vocab_json, \"target_vocab_json\"),\n",
    "    (cfg_path_repo, \"dataloader_config\"),\n",
    "    (sanity_path_repo, \"sanity_metrics\"),\n",
    "    (gaps_path_repo, \"session_gap_thresholds\"),\n",
    "]:\n",
    "    must_exist(p, lbl)\n",
    "\n",
    "print(\"[10-02] ✅ All required artifacts exist\")\n",
    "\n",
    "print(\"\\n[10-02] CHECKPOINT B\")\n",
    "print(\"If any artifact missing, STOP and paste the error.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ca732",
   "metadata": {},
   "source": [
    "Torch loader (PyTorch 2.6+) + vocab sizes + PAD/UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2afe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-03] TARGET: vocab_size from max(vocab values)+1 (token->id) = 747\n",
      "[10-03] vocab_size_target: 747\n",
      "[10-03] PAD_ID_TARGET: 0 | UNK_ID_TARGET: 1\n",
      "\n",
      "[10-03] CHECKPOINT C\n",
      "Confirm vocab_size_target + PAD/UNK printed as expected.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-03] Torch loader (PyTorch 2.6+) + vocab sizes + PAD/UNK\n",
    "\n",
    "def torch_load_repo_artifact(path, map_location=\"cpu\"):\n",
    "    path = str(path)\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=map_location, weights_only=False)\n",
    "        print(f\"[10-03] torch.load OK (weights_only=False): {path}\")\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        obj = torch.load(path, map_location=map_location)\n",
    "        print(f\"[10-03] torch.load OK (no weights_only arg): {path}\")\n",
    "        return obj\n",
    "\n",
    "target_vocab = load_json(target_vocab_json)\n",
    "\n",
    "def infer_vocab_size(vocab: dict, name: str) -> int:\n",
    "    for k in [\"vocab_size\", \"n_items\", \"num_items\", \"size\"]:\n",
    "        if k in vocab:\n",
    "            vs = int(vocab[k])\n",
    "            print(f\"[10-03] {name}: vocab_size from key '{k}' = {vs}\")\n",
    "            return vs\n",
    "    if \"vocab\" in vocab and isinstance(vocab[\"vocab\"], dict):\n",
    "        d = vocab[\"vocab\"]\n",
    "        if len(d) == 0:\n",
    "            return 0\n",
    "        sample_v = next(iter(d.values()))\n",
    "        if isinstance(sample_v, int):\n",
    "            ids = list(d.values())\n",
    "            vs = max(ids) + 1 if len(ids) else 0\n",
    "            print(f\"[10-03] {name}: vocab_size from max(vocab values)+1 (token->id) = {vs}\")\n",
    "            return vs\n",
    "    raise KeyError(f\"[10-03] {name}: Could not infer vocab_size. Keys={list(vocab.keys())}\")\n",
    "\n",
    "vocab_size_target = infer_vocab_size(target_vocab, \"TARGET\")\n",
    "\n",
    "def get_special_id(vocab_obj: dict, token_key: str, fallback: int) -> int:\n",
    "    tok = vocab_obj.get(token_key, None)\n",
    "    if tok is None:\n",
    "        return fallback\n",
    "    mapping = vocab_obj.get(\"vocab\", {})\n",
    "    if isinstance(mapping, dict) and tok in mapping and isinstance(mapping[tok], int):\n",
    "        return int(mapping[tok])\n",
    "    return fallback\n",
    "\n",
    "PAD_ID_TARGET = get_special_id(target_vocab, \"pad_token\", 0)\n",
    "UNK_ID_TARGET = get_special_id(target_vocab, \"unk_token\", 1)\n",
    "\n",
    "print(\"[10-03] vocab_size_target:\", vocab_size_target)\n",
    "print(\"[10-03] PAD_ID_TARGET:\", PAD_ID_TARGET, \"| UNK_ID_TARGET:\", UNK_ID_TARGET)\n",
    "assert PAD_ID_TARGET == 0\n",
    "\n",
    "print(\"\\n[10-03] CHECKPOINT C\")\n",
    "print(\"Confirm vocab_size_target + PAD/UNK printed as expected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084f477",
   "metadata": {},
   "source": [
    "Metrics (same as 06): HR/MRR/NDCG @ K={5,10,20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba3ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-04] ✅ Metric functions ready\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-04] Metrics (same as 06): HR/MRR/NDCG @ K={5,10,20}\n",
    "\n",
    "def init_metrics():\n",
    "    return {f\"{m}@{k}\": 0.0 for m in [\"HR\", \"MRR\", \"NDCG\"] for k in K_LIST}\n",
    "\n",
    "def update_metrics_from_rank(metrics: dict, rank0: int | None):\n",
    "    if rank0 is None:\n",
    "        return\n",
    "    r = rank0 + 1\n",
    "    for k in K_LIST:\n",
    "        if r <= k:\n",
    "            metrics[f\"HR@{k}\"] += 1.0\n",
    "            metrics[f\"MRR@{k}\"] += 1.0 / r\n",
    "            metrics[f\"NDCG@{k}\"] += 1.0 / math.log2(r + 1.0)\n",
    "\n",
    "def finalize_metrics(metrics: dict, n: int) -> dict:\n",
    "    return {k: (float(v / n) if n > 0 else 0.0) for k, v in metrics.items()}\n",
    "\n",
    "print(\"[10-04] ✅ Metric functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bf690",
   "metadata": {},
   "source": [
    "Load TARGET tensors (train/val/test) (from 05B artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38ec890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[10-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[10-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[10-05] TARGET train shapes: (1944, 20) (1944, 20) (1944,)\n",
      "[10-05] TARGET val shapes: (189, 20) (189,)\n",
      "[10-05] TARGET test shapes: (200, 20) (200,)\n",
      "\n",
      "[10-05] CHECKPOINT D\n",
      "Confirm shapes match: train=(1944,20), val=(189,20), test=(200,20).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-05] Load TARGET tensors (train/val/test) (from 05B artifacts)\n",
    "\n",
    "train_obj = torch_load_repo_artifact(target_train_pt, map_location=\"cpu\")\n",
    "val_obj   = torch_load_repo_artifact(target_val_pt, map_location=\"cpu\")\n",
    "test_obj  = torch_load_repo_artifact(target_test_pt, map_location=\"cpu\")\n",
    "\n",
    "def as_tensor_dict(obj: dict):\n",
    "    return {\n",
    "        \"input_ids\": torch.as_tensor(obj[\"input_ids\"]).long(),\n",
    "        \"attn_mask\": torch.as_tensor(obj[\"attn_mask\"]).long(),\n",
    "        \"labels\": torch.as_tensor(obj[\"labels\"]).long(),\n",
    "    }\n",
    "\n",
    "target_train = as_tensor_dict(train_obj)\n",
    "target_val   = as_tensor_dict(val_obj)\n",
    "target_test  = as_tensor_dict(test_obj)\n",
    "\n",
    "print(\"[10-05] TARGET train shapes:\",\n",
    "      tuple(target_train[\"input_ids\"].shape),\n",
    "      tuple(target_train[\"attn_mask\"].shape),\n",
    "      tuple(target_train[\"labels\"].shape))\n",
    "print(\"[10-05] TARGET val shapes:\",\n",
    "      tuple(target_val[\"input_ids\"].shape),\n",
    "      tuple(target_val[\"labels\"].shape))\n",
    "print(\"[10-05] TARGET test shapes:\",\n",
    "      tuple(target_test[\"input_ids\"].shape),\n",
    "      tuple(target_test[\"labels\"].shape))\n",
    "\n",
    "print(\"\\n[10-05] CHECKPOINT D\")\n",
    "print(\"Confirm shapes match: train=(1944,20), val=(189,20), test=(200,20).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b0f3e",
   "metadata": {},
   "source": [
    "SASRec model (causal self-attention) + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6addbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-06] ✅ SASRec defined\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-06] SASRec model (causal self-attention) + helpers\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def make_lengths(attn_mask: torch.Tensor) -> torch.Tensor:\n",
    "    return attn_mask.sum(dim=1).long()\n",
    "\n",
    "def build_causal_mask(T: int, device: torch.device) -> torch.Tensor:\n",
    "    # float mask: [T,T] with -inf above diagonal\n",
    "    m = torch.full((T, T), float(\"-inf\"), device=device)\n",
    "    m = torch.triu(m, diagonal=1)\n",
    "    return m\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_len: int,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        d_ff: int = 256,\n",
    "        dropout: float = 0.2,\n",
    "        pad_id: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.item_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        # input_ids: [B,T], attn_mask: [B,T] (1 for real, 0 for pad)\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)  # [B,T]\n",
    "        x = self.item_emb(input_ids) + self.pos_emb(pos)\n",
    "        x = self.drop(self.norm(x))\n",
    "\n",
    "        # transformer masks\n",
    "        causal = build_causal_mask(T, input_ids.device)     # [T,T]\n",
    "        key_padding = (attn_mask == 0)                      # [B,T] True where pad\n",
    "\n",
    "        h = self.encoder(x, mask=causal, src_key_padding_mask=key_padding)  # [B,T,D]\n",
    "\n",
    "        lengths = make_lengths(attn_mask)                   # [B]\n",
    "        last_idx = torch.clamp(lengths - 1, min=0)          # [B]\n",
    "        h_last = h[torch.arange(B, device=h.device), last_idx]  # [B,D]\n",
    "        logits = self.out(h_last)                           # [B,V]\n",
    "        return logits\n",
    "\n",
    "print(\"[10-06] ✅ SASRec defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1dc1d",
   "metadata": {},
   "source": [
    "SASRec (fixed masks): bool causal mask + norm_first=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3be0f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-06B] ✅ SASRecFixed defined (bool masks, norm_first=False)\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-06B] SASRec (fixed masks): bool causal mask + norm_first=False\n",
    "\n",
    "def build_causal_mask_bool(T: int, device: torch.device) -> torch.Tensor:\n",
    "    # True where attention should be blocked (upper triangle)\n",
    "    return torch.triu(torch.ones((T, T), dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "class SASRecFixed(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_len: int,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        d_ff: int = 256,\n",
    "        dropout: float = 0.2,\n",
    "        pad_id: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.item_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=False,   # fixes nested-tensor warning\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        # input_ids: [B,T], attn_mask: [B,T] (1=real, 0=pad)\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.item_emb(input_ids) + self.pos_emb(pos)\n",
    "        x = self.drop(self.norm(x))\n",
    "\n",
    "        causal = build_causal_mask_bool(T, input_ids.device)  # bool [T,T]\n",
    "        key_padding = (attn_mask == 0)                         # bool [B,T]\n",
    "\n",
    "        h = self.encoder(x, mask=causal, src_key_padding_mask=key_padding)  # [B,T,D]\n",
    "\n",
    "        lengths = make_lengths(attn_mask)\n",
    "        last_idx = torch.clamp(lengths - 1, min=0)\n",
    "        h_last = h[torch.arange(B, device=h.device), last_idx]  # [B,D]\n",
    "        logits = self.out(h_last)                                 # [B,V]\n",
    "        return logits\n",
    "\n",
    "print(\"[10-06B] ✅ SASRecFixed defined (bool masks, norm_first=False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa4a79",
   "metadata": {},
   "source": [
    "Train SASRec on TARGET with early stopping (VAL HR@20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f3a3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-07] SAS_CFG: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'd_ff': 256, 'dropout': 0.2, 'batch_size': 256, 'max_epochs': 80, 'lr': 0.001, 'weight_decay': 1e-06, 'grad_clip': 1.0, 'seed': 42, 'early_stop_metric': 'HR@20', 'patience': 10, 'min_delta': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mooc-coldstart-session-meta\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-07] Early stopping on HR@20 | patience=10 | min_delta=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mooc-coldstart-session-meta\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:6044: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-07] epoch=001 loss=6.7688 time=1.3s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=0\n",
      "[10-07] epoch=002 loss=6.5038 time=1.7s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=1\n",
      "[10-07] epoch=003 loss=6.3187 time=1.6s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=2\n",
      "[10-07] epoch=004 loss=6.1270 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=3\n",
      "[10-07] epoch=005 loss=5.9433 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=4\n",
      "[10-07] epoch=006 loss=5.8104 time=1.3s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=5\n",
      "[10-07] epoch=007 loss=5.6770 time=1.1s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=6\n",
      "[10-07] epoch=008 loss=5.5321 time=1.2s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=7\n",
      "[10-07] epoch=009 loss=5.4174 time=1.1s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=8\n",
      "[10-07] epoch=010 loss=5.3113 time=1.1s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=9\n",
      "[10-07] epoch=011 loss=5.2122 time=1.2s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=10\n",
      "[10-07] ✅ Early stop triggered at epoch=11 (best epoch=1, best HR@20=0.010582)\n",
      "[10-07] ✅ Restored best model weights from epoch=1 with best HR@20=0.010582\n",
      "\n",
      "[10-07] CHECKPOINT E\n",
      "Paste: best_epoch + best_metric + last 3 epoch log lines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-07] Train SASRec on TARGET with early stopping (VAL HR@20)\n",
    "\n",
    "SAS_CFG = {\n",
    "    \"d_model\": 64,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 2,\n",
    "    \"d_ff\": 256,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 256,\n",
    "    \"max_epochs\": 80,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"seed\": 42,\n",
    "    \"early_stop_metric\": \"HR@20\",\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 1e-4,\n",
    "}\n",
    "\n",
    "print(\"[10-07] SAS_CFG:\", SAS_CFG)\n",
    "\n",
    "set_seed(SAS_CFG[\"seed\"])\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_s = SASRec(\n",
    "    vocab_size=vocab_size_target,\n",
    "    max_len=MAX_PREFIX_LEN,\n",
    "    d_model=SAS_CFG[\"d_model\"],\n",
    "    n_heads=SAS_CFG[\"n_heads\"],\n",
    "    n_layers=SAS_CFG[\"n_layers\"],\n",
    "    d_ff=SAS_CFG[\"d_ff\"],\n",
    "    dropout=SAS_CFG[\"dropout\"],\n",
    "    pad_id=PAD_ID_TARGET,\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(\n",
    "    model_s.parameters(),\n",
    "    lr=SAS_CFG[\"lr\"],\n",
    "    weight_decay=SAS_CFG[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "def iter_batches(data: dict, batch_size: int, shuffle: bool = True):\n",
    "    n = data[\"input_ids\"].shape[0]\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for s in range(0, n, batch_size):\n",
    "        b = idx[s:s+batch_size]\n",
    "        yield (\n",
    "            data[\"input_ids\"][b].to(device),\n",
    "            data[\"attn_mask\"][b].to(device),\n",
    "            data[\"labels\"][b].to(device),\n",
    "        )\n",
    "\n",
    "def eval_sas(model: SASRec, data: dict) -> dict:\n",
    "    model.eval()\n",
    "    metrics = init_metrics()\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, am, y in iter_batches(data, batch_size=SAS_CFG[\"batch_size\"], shuffle=False):\n",
    "            logits = model(x, am)          # [B,V]\n",
    "            logits[:, PAD_ID_TARGET] = -1e9\n",
    "            topk = torch.topk(logits, k=MAX_K, dim=1).indices.cpu().numpy()  # [B,K]\n",
    "            y_np = y.cpu().numpy()\n",
    "            for i in range(topk.shape[0]):\n",
    "                if int(y_np[i]) == PAD_ID_TARGET:\n",
    "                    continue\n",
    "                pos = np.where(topk[i] == int(y_np[i]))[0]\n",
    "                rank0 = int(pos[0]) if pos.size > 0 else None\n",
    "                update_metrics_from_rank(metrics, rank0)\n",
    "                n += 1\n",
    "    out = finalize_metrics(metrics, n)\n",
    "    out[\"_n_examples\"] = int(n)\n",
    "    return out\n",
    "\n",
    "best_metric = -1.0\n",
    "best_epoch = -1\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "\n",
    "train_losses = []\n",
    "val_history = []\n",
    "\n",
    "metric_name = SAS_CFG[\"early_stop_metric\"]\n",
    "patience = int(SAS_CFG[\"patience\"])\n",
    "min_delta = float(SAS_CFG[\"min_delta\"])\n",
    "\n",
    "print(f\"[10-07] Early stopping on {metric_name} | patience={patience} | min_delta={min_delta}\")\n",
    "\n",
    "for epoch in range(1, int(SAS_CFG[\"max_epochs\"]) + 1):\n",
    "    model_s.train()\n",
    "    t0 = time.time()\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for x, am, y in iter_batches(target_train, SAS_CFG[\"batch_size\"], shuffle=True):\n",
    "        opt.zero_grad()\n",
    "        logits = model_s(x, am)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=PAD_ID_TARGET)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_s.parameters(), SAS_CFG[\"grad_clip\"])\n",
    "        opt.step()\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_n += bs\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_n)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    val_metrics = eval_sas(model_s, target_val)\n",
    "    val_history.append(val_metrics)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    cur = float(val_metrics.get(metric_name, 0.0))\n",
    "\n",
    "    improved = (cur > best_metric + min_delta)\n",
    "    if improved:\n",
    "        best_metric = cur\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model_s.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    print(f\"[10-07] epoch={epoch:03d} loss={avg_loss:.4f} time={dt:.1f}s | \"\n",
    "          f\"VAL {metric_name}={cur:.6f} | best={best_metric:.6f} (epoch {best_epoch}) | bad_epochs={bad_epochs}\")\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(f\"[10-07] ✅ Early stop triggered at epoch={epoch} (best epoch={best_epoch}, best {metric_name}={best_metric:.6f})\")\n",
    "        break\n",
    "\n",
    "assert best_state is not None, \"[10-07] best_state is None (no validation computed?)\"\n",
    "model_s.load_state_dict(best_state)\n",
    "print(f\"[10-07] ✅ Restored best model weights from epoch={best_epoch} with best {metric_name}={best_metric:.6f}\")\n",
    "\n",
    "print(\"\\n[10-07] CHECKPOINT E\")\n",
    "print(\"Paste: best_epoch + best_metric + last 3 epoch log lines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a56ab1",
   "metadata": {},
   "source": [
    "Sanity diagnostics: lengths distribution + hit count explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a90390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-07A] VAL lengths: min/median/p95/max = 1 3 16 20\n",
      "[10-07A] VAL lengths==0: 0 / 189\n",
      "[10-07A] HR@20=0.010582 implies hits ≈ 1.999998 out of 189 (should be ~2).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-07A] Sanity diagnostics: lengths distribution + hit count explanation\n",
    "\n",
    "lengths_val = make_lengths(target_val[\"attn_mask\"])\n",
    "print(\"[10-07A] VAL lengths: min/median/p95/max =\",\n",
    "      int(lengths_val.min()),\n",
    "      int(lengths_val.median()),\n",
    "      int(torch.quantile(lengths_val.float(), 0.95).item()),\n",
    "      int(lengths_val.max()))\n",
    "print(\"[10-07A] VAL lengths==0:\", int((lengths_val == 0).sum()), \"/\", int(lengths_val.numel()))\n",
    "\n",
    "# Convert HR@20 to raw hit counts for interpretability\n",
    "hr20 = 0.010582\n",
    "print(\"[10-07A] HR@20=0.010582 implies hits ≈\", hr20 * 189, \"out of 189 (should be ~2).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e534f",
   "metadata": {},
   "source": [
    "Final evaluation on TARGET (VAL + TEST) using best weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac590fd5",
   "metadata": {},
   "source": [
    "Retrain SASRecFixed (new run tag) + early stopping on VAL HR@20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e394705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-07B] RUN_TAG_FIXED: 20260102_233851\n",
      "[10-07B] SAS_CFG_FIXED: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'd_ff': 256, 'dropout': 0.2, 'batch_size': 256, 'max_epochs': 80, 'lr': 0.001, 'weight_decay': 1e-06, 'grad_clip': 1.0, 'seed': 42, 'early_stop_metric': 'HR@20', 'patience': 10, 'min_delta': 0.0001}\n",
      "[10-07B] Early stopping on HR@20 | patience=10 | min_delta=0.0001\n",
      "[10-07B] epoch=001 loss=6.7113 time=1.6s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=0\n",
      "[10-07B] epoch=002 loss=6.4709 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=1\n",
      "[10-07B] epoch=003 loss=6.3002 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=2\n",
      "[10-07B] epoch=004 loss=6.1506 time=1.6s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=3\n",
      "[10-07B] epoch=005 loss=6.0149 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=4\n",
      "[10-07B] epoch=006 loss=5.9055 time=1.6s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=5\n",
      "[10-07B] epoch=007 loss=5.8037 time=1.4s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=6\n",
      "[10-07B] epoch=008 loss=5.7005 time=1.3s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=7\n",
      "[10-07B] epoch=009 loss=5.6192 time=1.2s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=8\n",
      "[10-07B] epoch=010 loss=5.5431 time=1.4s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=9\n",
      "[10-07B] epoch=011 loss=5.4700 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=10\n",
      "[10-07B] ✅ Early stop at epoch=11 (best epoch=1, best HR@20=0.010582)\n",
      "[10-07B] ✅ Restored best weights from epoch=1 with best HR@20=0.010582\n",
      "\n",
      "[10-07B] CHECKPOINT E2\n",
      "Paste: best_epoch + best_metric + last 3 log lines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-07B] Retrain SASRecFixed (new run tag) + early stopping on VAL HR@20\n",
    "\n",
    "RUN_TAG_FIXED = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[10-07B] RUN_TAG_FIXED:\", RUN_TAG_FIXED)\n",
    "\n",
    "SAS_CFG_FIXED = dict(SAS_CFG)\n",
    "# keep same cfg for first retry (only masking changed)\n",
    "print(\"[10-07B] SAS_CFG_FIXED:\", SAS_CFG_FIXED)\n",
    "\n",
    "set_seed(SAS_CFG_FIXED[\"seed\"])\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_sf = SASRecFixed(\n",
    "    vocab_size=vocab_size_target,\n",
    "    max_len=MAX_PREFIX_LEN,\n",
    "    d_model=SAS_CFG_FIXED[\"d_model\"],\n",
    "    n_heads=SAS_CFG_FIXED[\"n_heads\"],\n",
    "    n_layers=SAS_CFG_FIXED[\"n_layers\"],\n",
    "    d_ff=SAS_CFG_FIXED[\"d_ff\"],\n",
    "    dropout=SAS_CFG_FIXED[\"dropout\"],\n",
    "    pad_id=PAD_ID_TARGET,\n",
    ").to(device)\n",
    "\n",
    "opt_sf = torch.optim.Adam(\n",
    "    model_sf.parameters(),\n",
    "    lr=SAS_CFG_FIXED[\"lr\"],\n",
    "    weight_decay=SAS_CFG_FIXED[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "def eval_sas_fixed(model: nn.Module, data: dict) -> dict:\n",
    "    model.eval()\n",
    "    metrics = init_metrics()\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, am, y in iter_batches(data, batch_size=SAS_CFG_FIXED[\"batch_size\"], shuffle=False):\n",
    "            logits = model(x, am)\n",
    "            logits[:, PAD_ID_TARGET] = -1e9\n",
    "            topk = torch.topk(logits, k=MAX_K, dim=1).indices.cpu().numpy()\n",
    "            y_np = y.cpu().numpy()\n",
    "            for i in range(topk.shape[0]):\n",
    "                yi = int(y_np[i])\n",
    "                if yi == PAD_ID_TARGET:\n",
    "                    continue\n",
    "                pos = np.where(topk[i] == yi)[0]\n",
    "                rank0 = int(pos[0]) if pos.size > 0 else None\n",
    "                update_metrics_from_rank(metrics, rank0)\n",
    "                n += 1\n",
    "    out = finalize_metrics(metrics, n)\n",
    "    out[\"_n_examples\"] = int(n)\n",
    "    return out\n",
    "\n",
    "best_metric = -1.0\n",
    "best_epoch = -1\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "\n",
    "train_losses_fixed = []\n",
    "val_history_fixed = []\n",
    "\n",
    "metric_name = SAS_CFG_FIXED[\"early_stop_metric\"]\n",
    "patience = int(SAS_CFG_FIXED[\"patience\"])\n",
    "min_delta = float(SAS_CFG_FIXED[\"min_delta\"])\n",
    "\n",
    "print(f\"[10-07B] Early stopping on {metric_name} | patience={patience} | min_delta={min_delta}\")\n",
    "\n",
    "for epoch in range(1, int(SAS_CFG_FIXED[\"max_epochs\"]) + 1):\n",
    "    model_sf.train()\n",
    "    t0 = time.time()\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for x, am, y in iter_batches(target_train, SAS_CFG_FIXED[\"batch_size\"], shuffle=True):\n",
    "        opt_sf.zero_grad()\n",
    "        logits = model_sf(x, am)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=PAD_ID_TARGET)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_sf.parameters(), SAS_CFG_FIXED[\"grad_clip\"])\n",
    "        opt_sf.step()\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_n += bs\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_n)\n",
    "    train_losses_fixed.append(avg_loss)\n",
    "\n",
    "    val_metrics = eval_sas_fixed(model_sf, target_val)\n",
    "    val_history_fixed.append(val_metrics)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    cur = float(val_metrics.get(metric_name, 0.0))\n",
    "\n",
    "    improved = (cur > best_metric + min_delta)\n",
    "    if improved:\n",
    "        best_metric = cur\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model_sf.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    print(f\"[10-07B] epoch={epoch:03d} loss={avg_loss:.4f} time={dt:.1f}s | \"\n",
    "          f\"VAL {metric_name}={cur:.6f} | best={best_metric:.6f} (epoch {best_epoch}) | bad_epochs={bad_epochs}\")\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(f\"[10-07B] ✅ Early stop at epoch={epoch} (best epoch={best_epoch}, best {metric_name}={best_metric:.6f})\")\n",
    "        break\n",
    "\n",
    "assert best_state is not None\n",
    "model_sf.load_state_dict(best_state)\n",
    "print(f\"[10-07B] ✅ Restored best weights from epoch={best_epoch} with best {metric_name}={best_metric:.6f}\")\n",
    "\n",
    "print(\"\\n[10-07B] CHECKPOINT E2\")\n",
    "print(\"Paste: best_epoch + best_metric + last 3 log lines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5c23cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-08] TARGET VAL (SASRec): {'HR@5': 0.005291005291005291, 'HR@10': 0.005291005291005291, 'HR@20': 0.010582010582010581, 'MRR@5': 0.005291005291005291, 'MRR@10': 0.005291005291005291, 'MRR@20': 0.005698005698005697, 'NDCG@5': 0.005291005291005291, 'NDCG@10': 0.005291005291005291, 'NDCG@20': 0.006680685370567162, '_n_examples': 189}\n",
      "[10-08] TARGET TEST (SASRec): {'HR@5': 0.0, 'HR@10': 0.015, 'HR@20': 0.035, 'MRR@5': 0.0, 'MRR@10': 0.0018948412698412697, 'MRR@20': 0.00335454822954823, 'NDCG@5': 0.0, 'NDCG@10': 0.004749141028915217, 'NDCG@20': 0.00990542650333623, '_n_examples': 200}\n",
      "\n",
      "[10-08] CHECKPOINT F\n",
      "Paste TARGET VAL/TEST metrics before writing reports.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-08] Final evaluation on TARGET (VAL + TEST) using best weights\n",
    "\n",
    "t_val_sas = eval_sas(model_s, target_val)\n",
    "t_test_sas = eval_sas(model_s, target_test)\n",
    "\n",
    "print(\"[10-08] TARGET VAL (SASRec):\", t_val_sas)\n",
    "print(\"[10-08] TARGET TEST (SASRec):\", t_test_sas)\n",
    "\n",
    "print(\"\\n[10-08] CHECKPOINT F\")\n",
    "print(\"Paste TARGET VAL/TEST metrics before writing reports.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b809e8e",
   "metadata": {},
   "source": [
    "Final evaluation (VAL + TEST) using SASRecFixed best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d24c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-08B] TARGET VAL (SASRecFixed): {'HR@5': 0.005291005291005291, 'HR@10': 0.005291005291005291, 'HR@20': 0.010582010582010581, 'MRR@5': 0.005291005291005291, 'MRR@10': 0.005291005291005291, 'MRR@20': 0.005698005698005697, 'NDCG@5': 0.005291005291005291, 'NDCG@10': 0.005291005291005291, 'NDCG@20': 0.006680685370567162, '_n_examples': 189}\n",
      "[10-08B] TARGET TEST (SASRecFixed): {'HR@5': 0.0, 'HR@10': 0.005, 'HR@20': 0.035, 'MRR@5': 0.0, 'MRR@10': 0.0007142857142857143, 'MRR@20': 0.002961871461871462, 'NDCG@5': 0.0, 'NDCG@10': 0.0016666666666666666, 'NDCG@20': 0.009467666869343328, '_n_examples': 200}\n",
      "\n",
      "[10-08B] CHECKPOINT F2\n",
      "Paste VAL/TEST metrics. If still flat, we run an overfit-on-256 sanity check next.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-08B] Final evaluation (VAL + TEST) using SASRecFixed best weights\n",
    "\n",
    "t_val_sas_fixed = eval_sas_fixed(model_sf, target_val)\n",
    "t_test_sas_fixed = eval_sas_fixed(model_sf, target_test)\n",
    "\n",
    "print(\"[10-08B] TARGET VAL (SASRecFixed):\", t_val_sas_fixed)\n",
    "print(\"[10-08B] TARGET TEST (SASRecFixed):\", t_test_sas_fixed)\n",
    "\n",
    "print(\"\\n[10-08B] CHECKPOINT F2\")\n",
    "print(\"Paste VAL/TEST metrics. If still flat, we run an overfit-on-256 sanity check next.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b586e0",
   "metadata": {},
   "source": [
    "Write report artifacts to reports/10_sasrec_baseline/<RUN_TAG>/ + update meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dacab732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-09] ✅ Wrote report files under: C:\\mooc-coldstart-session-meta\\reports\\10_sasrec_baseline\\20260102_233834\n",
      "[10-09] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[10-09] CHECKPOINT G\n",
      "Paste: report dir + confirm meta.json updated.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-09] Write report artifacts to reports/10_sasrec_baseline/<RUN_TAG>/ + update meta.json\n",
    "\n",
    "REPORT_DIR = REPO_ROOT / \"reports\" / \"10_sasrec_baseline\" / RUN_TAG\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj: dict, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "run_meta = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"inputs\": {\n",
    "        \"target_run_tag\": TARGET_TAG,\n",
    "        \"source_run_tag\": SOURCE_TAG,\n",
    "        \"target_train_pt\": str(target_train_pt),\n",
    "        \"target_val_pt\": str(target_val_pt),\n",
    "        \"target_test_pt\": str(target_test_pt),\n",
    "        \"target_vocab_json\": str(target_vocab_json),\n",
    "        \"dataloader_config\": str(cfg_path_repo),\n",
    "        \"sanity_metrics\": str(sanity_path_repo),\n",
    "        \"session_gap_thresholds\": str(gaps_path_repo),\n",
    "    },\n",
    "    \"protocol_reused_from_06\": {\n",
    "        \"K_LIST\": K_LIST,\n",
    "        \"MAX_PREFIX_LEN\": MAX_PREFIX_LEN,\n",
    "        \"PAD_ID_TARGET\": PAD_ID_TARGET,\n",
    "        \"pad_excluded_from_ranking\": True,\n",
    "        \"causal_mask\": True,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"SASRec\",\n",
    "        \"vocab_size\": int(vocab_size_target),\n",
    "        \"d_model\": int(SAS_CFG[\"d_model\"]),\n",
    "        \"n_heads\": int(SAS_CFG[\"n_heads\"]),\n",
    "        \"n_layers\": int(SAS_CFG[\"n_layers\"]),\n",
    "        \"d_ff\": int(SAS_CFG[\"d_ff\"]),\n",
    "        \"dropout\": float(SAS_CFG[\"dropout\"]),\n",
    "    },\n",
    "    \"train_cfg\": SAS_CFG,\n",
    "    \"early_stopping\": {\n",
    "        \"metric\": SAS_CFG[\"early_stop_metric\"],\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_metric\": float(best_metric),\n",
    "        \"patience\": int(SAS_CFG[\"patience\"]),\n",
    "        \"min_delta\": float(SAS_CFG[\"min_delta\"]),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"This run trains/evaluates SASRec on TARGET only (Layer-1 baseline).\",\n",
    "        \"Source training / transfer is handled later.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"target\": {\n",
    "        \"val\": t_val_sas,\n",
    "        \"test\": t_test_sas,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_history\": val_history,\n",
    "    },\n",
    "    \"source\": None,\n",
    "}\n",
    "\n",
    "save_json(run_meta, REPORT_DIR / \"run_meta.json\")\n",
    "save_json(results, REPORT_DIR / \"results.json\")\n",
    "\n",
    "ckpt = {\n",
    "    \"state_dict\": model_s.state_dict(),\n",
    "    \"sas_cfg\": SAS_CFG,\n",
    "    \"vocab_size_target\": vocab_size_target,\n",
    "    \"pad_id\": PAD_ID_TARGET,\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_metric\": best_metric,\n",
    "}\n",
    "torch.save(ckpt, REPORT_DIR / \"model.pt\")\n",
    "\n",
    "# Update meta.json\n",
    "meta_path = REPO_ROOT / \"meta.json\"\n",
    "meta = load_json(meta_path) if meta_path.exists() else {\"artifacts\": {}}\n",
    "\n",
    "meta.setdefault(\"artifacts\", {})\n",
    "meta[\"artifacts\"].setdefault(\"sasrec_baseline\", {})\n",
    "meta[\"artifacts\"][\"sasrec_baseline\"][RUN_TAG] = {\n",
    "    \"target_run_tag\": TARGET_TAG,\n",
    "    \"source_run_tag\": SOURCE_TAG,\n",
    "    \"report_dir\": str(REPORT_DIR),\n",
    "    \"results_json\": str(REPORT_DIR / \"results.json\"),\n",
    "    \"run_meta_json\": str(REPORT_DIR / \"run_meta.json\"),\n",
    "    \"model_pt\": str(REPORT_DIR / \"model.pt\"),\n",
    "}\n",
    "meta[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "save_json(meta, meta_path)\n",
    "\n",
    "print(\"[10-09] ✅ Wrote report files under:\", REPORT_DIR)\n",
    "print(\"[10-09] ✅ Updated meta.json:\", meta_path)\n",
    "\n",
    "print(\"\\n[10-09] CHECKPOINT G\")\n",
    "print(\"Paste: report dir + confirm meta.json updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0d83d",
   "metadata": {},
   "source": [
    "Footer summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ace81fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 10 SASRec Baseline Summary ==========\n",
      "RUN_TAG: 20260102_233834\n",
      "--- TARGET ---\n",
      "VAL : {'HR@5': 0.005291005291005291, 'HR@10': 0.005291005291005291, 'HR@20': 0.010582010582010581, 'MRR@5': 0.005291005291005291, 'MRR@10': 0.005291005291005291, 'MRR@20': 0.005698005698005697, 'NDCG@5': 0.005291005291005291, 'NDCG@10': 0.005291005291005291, 'NDCG@20': 0.006680685370567162, '_n_examples': 189}\n",
      "TEST: {'HR@5': 0.0, 'HR@10': 0.015, 'HR@20': 0.035, 'MRR@5': 0.0, 'MRR@10': 0.0018948412698412697, 'MRR@20': 0.00335454822954823, 'NDCG@5': 0.0, 'NDCG@10': 0.004749141028915217, 'NDCG@20': 0.00990542650333623, '_n_examples': 200}\n",
      "Report dir: C:\\mooc-coldstart-session-meta\\reports\\10_sasrec_baseline\\20260102_233834\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-10] Footer summary\n",
    "\n",
    "print(\"========== 10 SASRec Baseline Summary ==========\")\n",
    "print(\"RUN_TAG:\", RUN_TAG)\n",
    "print(\"--- TARGET ---\")\n",
    "print(\"VAL :\", t_val_sas)\n",
    "print(\"TEST:\", t_test_sas)\n",
    "print(\"Report dir:\", REPORT_DIR)\n",
    "print(\"================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0a232",
   "metadata": {},
   "source": [
    "Diagnose padding side (CRITICAL): right-pad vs left-pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1fdb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-10] pad_side detected: left\n",
      "[10-10] Example short row idx: 0 | len: 1\n",
      "[10-10] input_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 416]\n",
      "[10-10] attn_mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "[10-10] CHECKPOINT H\n",
      "Paste pad_side + the example input_ids/attn_mask row.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-10] Diagnose padding side (CRITICAL): right-pad vs left-pad\n",
    "\n",
    "def detect_pad_side(input_ids: torch.Tensor, attn_mask: torch.Tensor, pad_id: int = 0, n_probe: int = 50):\n",
    "    # Find rows with some padding\n",
    "    lengths = attn_mask.sum(dim=1)\n",
    "    idx = torch.where(lengths < attn_mask.shape[1])[0]\n",
    "    if idx.numel() == 0:\n",
    "        return \"no_pad_detected\"\n",
    "\n",
    "    idx = idx[:n_probe]\n",
    "    T = attn_mask.shape[1]\n",
    "    right_votes = 0\n",
    "    left_votes = 0\n",
    "\n",
    "    for i in idx.tolist():\n",
    "        am = attn_mask[i].cpu().numpy()\n",
    "        # right-pad => zeros clustered at end\n",
    "        if am[-1] == 0 and am[0] == 1:\n",
    "            right_votes += 1\n",
    "        # left-pad => zeros clustered at start\n",
    "        if am[0] == 0 and am[-1] == 1:\n",
    "            left_votes += 1\n",
    "\n",
    "    if right_votes > left_votes:\n",
    "        return \"right\"\n",
    "    if left_votes > right_votes:\n",
    "        return \"left\"\n",
    "    return f\"ambiguous(right={right_votes}, left={left_votes})\"\n",
    "\n",
    "pad_side = detect_pad_side(target_train[\"input_ids\"], target_train[\"attn_mask\"], pad_id=PAD_ID_TARGET)\n",
    "print(\"[10-10] pad_side detected:\", pad_side)\n",
    "\n",
    "# Show one short example row for visual confirmation\n",
    "lengths = make_lengths(target_train[\"attn_mask\"])\n",
    "short_idx = int(torch.where(lengths < MAX_PREFIX_LEN)[0][0].item())\n",
    "print(\"[10-10] Example short row idx:\", short_idx, \"| len:\", int(lengths[short_idx]))\n",
    "print(\"[10-10] input_ids:\", target_train[\"input_ids\"][short_idx].tolist())\n",
    "print(\"[10-10] attn_mask:\", target_train[\"attn_mask\"][short_idx].tolist())\n",
    "\n",
    "print(\"\\n[10-10] CHECKPOINT H\")\n",
    "print(\"Paste pad_side + the example input_ids/attn_mask row.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497bc056",
   "metadata": {},
   "source": [
    "Prediction-collapse check: what IDs does SASRec actually predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8ab42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-11] Top predicted IDs in VAL (first 2 batches):\n",
      "[(93, 185), (1, 185), (2, 185), (3, 185), (4, 185), (5, 185), (6, 185), (7, 185), (8, 185), (9, 185), (10, 185), (11, 185), (12, 185), (13, 185), (14, 185), (15, 185), (16, 185), (17, 185), (18, 185), (19, 185), (303, 2), (679, 2), (58, 2), (159, 2), (718, 2), (571, 2), (331, 2), (24, 2), (36, 2), (269, 2)]\n",
      "\n",
      "[10-11] CHECKPOINT I\n",
      "Paste the top predicted IDs list. If it's dominated by 1-3 IDs, it's collapsing.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-11] Prediction-collapse check: what IDs does SASRec actually predict?\n",
    "\n",
    "def topk_id_histogram(model, data, n_batches=2, k=20):\n",
    "    model.eval()\n",
    "    hist = {}\n",
    "    with torch.no_grad():\n",
    "        nb = 0\n",
    "        for x, am, y in iter_batches(data, batch_size=256, shuffle=False):\n",
    "            logits = model(x, am)\n",
    "            logits[:, PAD_ID_TARGET] = -1e9\n",
    "            topk = torch.topk(logits, k=k, dim=1).indices.cpu().numpy()\n",
    "            for row in topk:\n",
    "                for iid in row:\n",
    "                    hist[int(iid)] = hist.get(int(iid), 0) + 1\n",
    "            nb += 1\n",
    "            if nb >= n_batches:\n",
    "                break\n",
    "    items = sorted(hist.items(), key=lambda kv: kv[1], reverse=True)[:30]\n",
    "    return items\n",
    "\n",
    "print(\"[10-11] Top predicted IDs in VAL (first 2 batches):\")\n",
    "print(topk_id_histogram(model_s, target_val, n_batches=2, k=20))\n",
    "\n",
    "print(\"\\n[10-11] CHECKPOINT I\")\n",
    "print(\"Paste the top predicted IDs list. If it's dominated by 1-3 IDs, it's collapsing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da736f7",
   "metadata": {},
   "source": [
    "SASRecAdaptive: correct last-position extraction based on pad_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f00ae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-12] ✅ SASRecAdaptive defined\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-12] SASRecAdaptive: correct last-position extraction based on pad_side\n",
    "\n",
    "class SASRecAdaptive(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_len: int,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        d_ff: int = 256,\n",
    "        dropout: float = 0.2,\n",
    "        pad_id: int = 0,\n",
    "        pad_side: str = \"right\",   # \"right\" or \"left\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert pad_side in [\"right\", \"left\"], f\"pad_side must be right/left, got {pad_side}\"\n",
    "        self.pad_side = pad_side\n",
    "        self.pad_id = pad_id\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.item_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=False,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.item_emb(input_ids) + self.pos_emb(pos)\n",
    "        x = self.drop(self.norm(x))\n",
    "\n",
    "        causal = torch.triu(torch.ones((T, T), dtype=torch.bool, device=input_ids.device), diagonal=1)\n",
    "        key_padding = (attn_mask == 0)  # bool\n",
    "\n",
    "        h = self.encoder(x, mask=causal, src_key_padding_mask=key_padding)  # [B,T,D]\n",
    "\n",
    "        if self.pad_side == \"right\":\n",
    "            lengths = make_lengths(attn_mask)\n",
    "            last_idx = torch.clamp(lengths - 1, min=0)\n",
    "        else:\n",
    "            # left padding => last token is always real (attn_mask[-1]==1)\n",
    "            last_idx = torch.full((B,), T - 1, device=h.device, dtype=torch.long)\n",
    "\n",
    "        h_last = h[torch.arange(B, device=h.device), last_idx]\n",
    "        logits = self.out(h_last)\n",
    "        return logits\n",
    "\n",
    "print(\"[10-12] ✅ SASRecAdaptive defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fbec2",
   "metadata": {},
   "source": [
    "Retrain SASRecAdaptive (new RUN_TAG) + early stopping on VAL HR@20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bef77f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-13] RUN_TAG_ADAPT: 20260102_234522\n",
      "[10-13] Using pad_side: left\n",
      "[10-13] epoch=001 loss=6.7259 time=1.2s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=0\n",
      "[10-13] epoch=002 loss=6.3992 time=1.3s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=1\n",
      "[10-13] epoch=003 loss=6.1698 time=1.4s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=2\n",
      "[10-13] epoch=004 loss=5.9537 time=1.4s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=3\n",
      "[10-13] epoch=005 loss=5.7447 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=4\n",
      "[10-13] epoch=006 loss=5.5472 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=5\n",
      "[10-13] epoch=007 loss=5.3671 time=1.5s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=6\n",
      "[10-13] epoch=008 loss=5.1847 time=1.4s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=7\n",
      "[10-13] epoch=009 loss=5.0143 time=1.2s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=8\n",
      "[10-13] epoch=010 loss=4.8426 time=1.1s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=9\n",
      "[10-13] epoch=011 loss=4.6718 time=1.1s | VAL HR@20=0.010582 | best=0.010582 (epoch 1) | bad_epochs=10\n",
      "[10-13] ✅ Early stop at epoch=11 (best epoch=1, best HR@20=0.010582)\n",
      "[10-13] ✅ Restored best weights from epoch=1 with best HR@20=0.010582\n",
      "\n",
      "[10-13] CHECKPOINT J\n",
      "Paste: best_epoch + best_metric + last 3 epoch log lines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-13] Retrain SASRecAdaptive (new RUN_TAG) + early stopping on VAL HR@20\n",
    "\n",
    "RUN_TAG_ADAPT = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[10-13] RUN_TAG_ADAPT:\", RUN_TAG_ADAPT)\n",
    "print(\"[10-13] Using pad_side:\", pad_side)\n",
    "\n",
    "model_sa = SASRecAdaptive(\n",
    "    vocab_size=vocab_size_target,\n",
    "    max_len=MAX_PREFIX_LEN,\n",
    "    d_model=SAS_CFG[\"d_model\"],\n",
    "    n_heads=SAS_CFG[\"n_heads\"],\n",
    "    n_layers=SAS_CFG[\"n_layers\"],\n",
    "    d_ff=SAS_CFG[\"d_ff\"],\n",
    "    dropout=SAS_CFG[\"dropout\"],\n",
    "    pad_id=PAD_ID_TARGET,\n",
    "    pad_side=(\"left\" if \"left\" in str(pad_side) else \"right\"),\n",
    ").to(device)\n",
    "\n",
    "opt_sa = torch.optim.Adam(model_sa.parameters(), lr=SAS_CFG[\"lr\"], weight_decay=SAS_CFG[\"weight_decay\"])\n",
    "\n",
    "def eval_sas_adapt(model: nn.Module, data: dict) -> dict:\n",
    "    model.eval()\n",
    "    metrics = init_metrics()\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, am, y in iter_batches(data, batch_size=SAS_CFG[\"batch_size\"], shuffle=False):\n",
    "            logits = model(x, am)\n",
    "            logits[:, PAD_ID_TARGET] = -1e9\n",
    "            topk = torch.topk(logits, k=MAX_K, dim=1).indices.cpu().numpy()\n",
    "            y_np = y.cpu().numpy()\n",
    "            for i in range(topk.shape[0]):\n",
    "                yi = int(y_np[i])\n",
    "                if yi == PAD_ID_TARGET:\n",
    "                    continue\n",
    "                pos = np.where(topk[i] == yi)[0]\n",
    "                rank0 = int(pos[0]) if pos.size > 0 else None\n",
    "                update_metrics_from_rank(metrics, rank0)\n",
    "                n += 1\n",
    "    out = finalize_metrics(metrics, n)\n",
    "    out[\"_n_examples\"] = int(n)\n",
    "    return out\n",
    "\n",
    "best_metric = -1.0\n",
    "best_epoch = -1\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "train_losses_adapt = []\n",
    "val_history_adapt = []\n",
    "\n",
    "metric_name = SAS_CFG[\"early_stop_metric\"]\n",
    "patience = int(SAS_CFG[\"patience\"])\n",
    "min_delta = float(SAS_CFG[\"min_delta\"])\n",
    "\n",
    "for epoch in range(1, int(SAS_CFG[\"max_epochs\"]) + 1):\n",
    "    model_sa.train()\n",
    "    t0 = time.time()\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for x, am, y in iter_batches(target_train, SAS_CFG[\"batch_size\"], shuffle=True):\n",
    "        opt_sa.zero_grad()\n",
    "        logits = model_sa(x, am)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=PAD_ID_TARGET)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_sa.parameters(), SAS_CFG[\"grad_clip\"])\n",
    "        opt_sa.step()\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_n += bs\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_n)\n",
    "    train_losses_adapt.append(avg_loss)\n",
    "\n",
    "    val_metrics = eval_sas_adapt(model_sa, target_val)\n",
    "    val_history_adapt.append(val_metrics)\n",
    "\n",
    "    cur = float(val_metrics.get(metric_name, 0.0))\n",
    "    improved = (cur > best_metric + min_delta)\n",
    "    if improved:\n",
    "        best_metric = cur\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model_sa.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    print(f\"[10-13] epoch={epoch:03d} loss={avg_loss:.4f} time={time.time()-t0:.1f}s | \"\n",
    "          f\"VAL {metric_name}={cur:.6f} | best={best_metric:.6f} (epoch {best_epoch}) | bad_epochs={bad_epochs}\")\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(f\"[10-13] ✅ Early stop at epoch={epoch} (best epoch={best_epoch}, best {metric_name}={best_metric:.6f})\")\n",
    "        break\n",
    "\n",
    "assert best_state is not None\n",
    "model_sa.load_state_dict(best_state)\n",
    "print(f\"[10-13] ✅ Restored best weights from epoch={best_epoch} with best {metric_name}={best_metric:.6f}\")\n",
    "\n",
    "print(\"\\n[10-13] CHECKPOINT J\")\n",
    "print(\"Paste: best_epoch + best_metric + last 3 epoch log lines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a313d2",
   "metadata": {},
   "source": [
    "Evaluate SASRecAdaptive (VAL + TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8e35eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-14] TARGET VAL (SASRecAdaptive): {'HR@5': 0.005291005291005291, 'HR@10': 0.005291005291005291, 'HR@20': 0.010582010582010581, 'MRR@5': 0.005291005291005291, 'MRR@10': 0.005291005291005291, 'MRR@20': 0.005698005698005697, 'NDCG@5': 0.005291005291005291, 'NDCG@10': 0.005291005291005291, 'NDCG@20': 0.006680685370567162, '_n_examples': 189}\n",
      "[10-14] TARGET TEST (SASRecAdaptive): {'HR@5': 0.0, 'HR@10': 0.015, 'HR@20': 0.04, 'MRR@5': 0.0, 'MRR@10': 0.0021031746031746033, 'MRR@20': 0.004017427017427018, 'NDCG@5': 0.0, 'NDCG@10': 0.004952852580526683, 'NDCG@20': 0.011503852783203346, '_n_examples': 200}\n",
      "\n",
      "[10-14] CHECKPOINT K\n",
      "Paste VAL/TEST metrics. We expect to beat MostPop HR@20 (~0.175) if the bug was padding-side.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-14] Evaluate SASRecAdaptive (VAL + TEST)\n",
    "\n",
    "t_val_adapt = eval_sas_adapt(model_sa, target_val)\n",
    "t_test_adapt = eval_sas_adapt(model_sa, target_test)\n",
    "\n",
    "print(\"[10-14] TARGET VAL (SASRecAdaptive):\", t_val_adapt)\n",
    "print(\"[10-14] TARGET TEST (SASRecAdaptive):\", t_test_adapt)\n",
    "\n",
    "print(\"\\n[10-14] CHECKPOINT K\")\n",
    "print(\"Paste VAL/TEST metrics. We expect to beat MostPop HR@20 (~0.175) if the bug was padding-side.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e66712",
   "metadata": {},
   "source": [
    "On-the-fly LEFT->RIGHT padding transform (batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfde4dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-15] Example transform check (row0):\n",
      "  left input_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 416]\n",
      "  left attn_mask : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "  right input_ids: [416, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  right attn_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  len: 1\n",
      "\n",
      "[10-15] CHECKPOINT L\n",
      "Paste row0 before/after to confirm left->right pad transform is correct.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-15] On-the-fly LEFT->RIGHT padding transform (batch)\n",
    "\n",
    "def right_pad_from_left_padded(input_ids: torch.Tensor, attn_mask: torch.Tensor, pad_id: int = 0):\n",
    "    \"\"\"\n",
    "    Convert left-padded [B,T] to right-padded [B,T] while preserving token order.\n",
    "    Assumes attn_mask is 0/1 and PAD tokens live where attn_mask==0.\n",
    "    \"\"\"\n",
    "    B, T = input_ids.shape\n",
    "    lengths = attn_mask.sum(dim=1).long()  # [B]\n",
    "    out_ids = torch.full_like(input_ids, pad_id)\n",
    "    out_am = torch.zeros_like(attn_mask)\n",
    "\n",
    "    for i in range(B):\n",
    "        L = int(lengths[i].item())\n",
    "        if L <= 0:\n",
    "            continue\n",
    "        # For left padding, real tokens are at the end; take the last L tokens\n",
    "        seq = input_ids[i, T - L :].clone()\n",
    "        out_ids[i, :L] = seq\n",
    "        out_am[i, :L] = 1\n",
    "\n",
    "    return out_ids, out_am, lengths\n",
    "\n",
    "# quick sanity on a tiny batch\n",
    "_x = target_train[\"input_ids\"][:4].clone()\n",
    "_am = target_train[\"attn_mask\"][:4].clone()\n",
    "_x2, _am2, _len2 = right_pad_from_left_padded(_x, _am, pad_id=PAD_ID_TARGET)\n",
    "\n",
    "print(\"[10-15] Example transform check (row0):\")\n",
    "print(\"  left input_ids :\", _x[0].tolist())\n",
    "print(\"  left attn_mask :\", _am[0].tolist())\n",
    "print(\"  right input_ids:\", _x2[0].tolist())\n",
    "print(\"  right attn_mask:\", _am2[0].tolist())\n",
    "print(\"  len:\", int(_len2[0].item()))\n",
    "\n",
    "print(\"\\n[10-15] CHECKPOINT L\")\n",
    "print(\"Paste row0 before/after to confirm left->right pad transform is correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c2a1d",
   "metadata": {},
   "source": [
    "SASRecRightPad: always right-pad inside forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3524c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-16] ✅ SASRecRightPad defined\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-16] SASRecRightPad: always right-pad inside forward\n",
    "\n",
    "class SASRecRightPad(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_len: int,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        d_ff: int = 256,\n",
    "        dropout: float = 0.2,\n",
    "        pad_id: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.item_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=False,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        # Convert to RIGHT-padded so position ids align with token order\n",
    "        input_ids, attn_mask, lengths = right_pad_from_left_padded(\n",
    "            input_ids, attn_mask, pad_id=self.pad_id\n",
    "        )\n",
    "\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.item_emb(input_ids) + self.pos_emb(pos)\n",
    "        x = self.drop(self.norm(x))\n",
    "\n",
    "        # bool causal + key padding\n",
    "        causal = torch.triu(torch.ones((T, T), dtype=torch.bool, device=input_ids.device), diagonal=1)\n",
    "        key_padding = (attn_mask == 0)\n",
    "\n",
    "        h = self.encoder(x, mask=causal, src_key_padding_mask=key_padding)  # [B,T,D]\n",
    "\n",
    "        last_idx = torch.clamp(lengths - 1, min=0)  # RIGHT pad => last real token index\n",
    "        h_last = h[torch.arange(B, device=h.device), last_idx]\n",
    "        logits = self.out(h_last)\n",
    "        return logits\n",
    "\n",
    "print(\"[10-16] ✅ SASRecRightPad defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c9cd3",
   "metadata": {},
   "source": [
    "Train SASRecRightPad (NEW RUN TAG) + early stopping on VAL HR@20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72321829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-17] RUN_TAG_RPAD: 20260102_234927\n",
      "[10-17] SAS_CFG_RPAD: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'd_ff': 256, 'dropout': 0.2, 'batch_size': 256, 'max_epochs': 80, 'lr': 0.001, 'weight_decay': 1e-06, 'grad_clip': 1.0, 'seed': 42, 'early_stop_metric': 'HR@20', 'patience': 10, 'min_delta': 0.0001}\n",
      "[10-17] Early stopping on HR@20 | patience=10 | min_delta=0.0001\n",
      "[10-17] epoch=001 loss=6.6921 time=1.4s | VAL HR@20=0.111111 | best=0.111111 (epoch 1) | bad_epochs=0\n",
      "[10-17] epoch=002 loss=6.3934 time=1.4s | VAL HR@20=0.142857 | best=0.142857 (epoch 2) | bad_epochs=0\n",
      "[10-17] epoch=003 loss=6.1864 time=1.4s | VAL HR@20=0.174603 | best=0.174603 (epoch 3) | bad_epochs=0\n",
      "[10-17] epoch=004 loss=5.9666 time=1.3s | VAL HR@20=0.201058 | best=0.201058 (epoch 4) | bad_epochs=0\n",
      "[10-17] epoch=005 loss=5.7746 time=1.5s | VAL HR@20=0.206349 | best=0.206349 (epoch 5) | bad_epochs=0\n",
      "[10-17] epoch=006 loss=5.6028 time=1.6s | VAL HR@20=0.222222 | best=0.222222 (epoch 6) | bad_epochs=0\n",
      "[10-17] epoch=007 loss=5.4242 time=1.5s | VAL HR@20=0.264550 | best=0.264550 (epoch 7) | bad_epochs=0\n",
      "[10-17] epoch=008 loss=5.2495 time=1.5s | VAL HR@20=0.280423 | best=0.280423 (epoch 8) | bad_epochs=0\n",
      "[10-17] epoch=009 loss=5.0931 time=1.5s | VAL HR@20=0.306878 | best=0.306878 (epoch 9) | bad_epochs=0\n",
      "[10-17] epoch=010 loss=4.9498 time=1.2s | VAL HR@20=0.338624 | best=0.338624 (epoch 10) | bad_epochs=0\n",
      "[10-17] epoch=011 loss=4.7733 time=1.2s | VAL HR@20=0.365079 | best=0.365079 (epoch 11) | bad_epochs=0\n",
      "[10-17] epoch=012 loss=4.6275 time=1.2s | VAL HR@20=0.370370 | best=0.370370 (epoch 12) | bad_epochs=0\n",
      "[10-17] epoch=013 loss=4.4663 time=1.2s | VAL HR@20=0.380952 | best=0.380952 (epoch 13) | bad_epochs=0\n",
      "[10-17] epoch=014 loss=4.3453 time=1.4s | VAL HR@20=0.391534 | best=0.391534 (epoch 14) | bad_epochs=0\n",
      "[10-17] epoch=015 loss=4.1815 time=1.6s | VAL HR@20=0.402116 | best=0.402116 (epoch 15) | bad_epochs=0\n",
      "[10-17] epoch=016 loss=4.0534 time=1.5s | VAL HR@20=0.412698 | best=0.412698 (epoch 16) | bad_epochs=0\n",
      "[10-17] epoch=017 loss=3.9236 time=1.6s | VAL HR@20=0.428571 | best=0.428571 (epoch 17) | bad_epochs=0\n",
      "[10-17] epoch=018 loss=3.7833 time=1.6s | VAL HR@20=0.417989 | best=0.428571 (epoch 17) | bad_epochs=1\n",
      "[10-17] epoch=019 loss=3.6755 time=1.2s | VAL HR@20=0.439153 | best=0.439153 (epoch 19) | bad_epochs=0\n",
      "[10-17] epoch=020 loss=3.5458 time=1.2s | VAL HR@20=0.455026 | best=0.455026 (epoch 20) | bad_epochs=0\n",
      "[10-17] epoch=021 loss=3.4048 time=1.2s | VAL HR@20=0.465608 | best=0.465608 (epoch 21) | bad_epochs=0\n",
      "[10-17] epoch=022 loss=3.3020 time=1.2s | VAL HR@20=0.460317 | best=0.465608 (epoch 21) | bad_epochs=1\n",
      "[10-17] epoch=023 loss=3.1840 time=1.1s | VAL HR@20=0.465608 | best=0.465608 (epoch 21) | bad_epochs=2\n",
      "[10-17] epoch=024 loss=3.0789 time=1.5s | VAL HR@20=0.470899 | best=0.470899 (epoch 24) | bad_epochs=0\n",
      "[10-17] epoch=025 loss=2.9512 time=1.7s | VAL HR@20=0.481481 | best=0.481481 (epoch 25) | bad_epochs=0\n",
      "[10-17] epoch=026 loss=2.8532 time=1.6s | VAL HR@20=0.481481 | best=0.481481 (epoch 25) | bad_epochs=1\n",
      "[10-17] epoch=027 loss=2.7521 time=1.5s | VAL HR@20=0.486772 | best=0.486772 (epoch 27) | bad_epochs=0\n",
      "[10-17] epoch=028 loss=2.6395 time=1.5s | VAL HR@20=0.486772 | best=0.486772 (epoch 27) | bad_epochs=1\n",
      "[10-17] epoch=029 loss=2.5349 time=1.2s | VAL HR@20=0.497354 | best=0.497354 (epoch 29) | bad_epochs=0\n",
      "[10-17] epoch=030 loss=2.4653 time=1.1s | VAL HR@20=0.507937 | best=0.507937 (epoch 30) | bad_epochs=0\n",
      "[10-17] epoch=031 loss=2.3945 time=1.2s | VAL HR@20=0.507937 | best=0.507937 (epoch 30) | bad_epochs=1\n",
      "[10-17] epoch=032 loss=2.2926 time=1.2s | VAL HR@20=0.513228 | best=0.513228 (epoch 32) | bad_epochs=0\n",
      "[10-17] epoch=033 loss=2.2192 time=1.2s | VAL HR@20=0.502646 | best=0.513228 (epoch 32) | bad_epochs=1\n",
      "[10-17] epoch=034 loss=2.1410 time=1.4s | VAL HR@20=0.513228 | best=0.513228 (epoch 32) | bad_epochs=2\n",
      "[10-17] epoch=035 loss=2.0587 time=1.5s | VAL HR@20=0.523810 | best=0.523810 (epoch 35) | bad_epochs=0\n",
      "[10-17] epoch=036 loss=1.9750 time=1.5s | VAL HR@20=0.529101 | best=0.529101 (epoch 36) | bad_epochs=0\n",
      "[10-17] epoch=037 loss=1.8945 time=1.6s | VAL HR@20=0.529101 | best=0.529101 (epoch 36) | bad_epochs=1\n",
      "[10-17] epoch=038 loss=1.8090 time=1.6s | VAL HR@20=0.529101 | best=0.529101 (epoch 36) | bad_epochs=2\n",
      "[10-17] epoch=039 loss=1.7529 time=1.3s | VAL HR@20=0.539683 | best=0.539683 (epoch 39) | bad_epochs=0\n",
      "[10-17] epoch=040 loss=1.6846 time=1.2s | VAL HR@20=0.539683 | best=0.539683 (epoch 39) | bad_epochs=1\n",
      "[10-17] epoch=041 loss=1.6143 time=1.2s | VAL HR@20=0.534392 | best=0.539683 (epoch 39) | bad_epochs=2\n",
      "[10-17] epoch=042 loss=1.5600 time=1.1s | VAL HR@20=0.539683 | best=0.539683 (epoch 39) | bad_epochs=3\n",
      "[10-17] epoch=043 loss=1.5134 time=1.2s | VAL HR@20=0.539683 | best=0.539683 (epoch 39) | bad_epochs=4\n",
      "[10-17] epoch=044 loss=1.4507 time=1.2s | VAL HR@20=0.539683 | best=0.539683 (epoch 39) | bad_epochs=5\n",
      "[10-17] epoch=045 loss=1.3994 time=1.5s | VAL HR@20=0.544974 | best=0.544974 (epoch 45) | bad_epochs=0\n",
      "[10-17] epoch=046 loss=1.3563 time=1.6s | VAL HR@20=0.550265 | best=0.550265 (epoch 46) | bad_epochs=0\n",
      "[10-17] epoch=047 loss=1.2934 time=1.6s | VAL HR@20=0.544974 | best=0.550265 (epoch 46) | bad_epochs=1\n",
      "[10-17] epoch=048 loss=1.2486 time=1.5s | VAL HR@20=0.550265 | best=0.550265 (epoch 46) | bad_epochs=2\n",
      "[10-17] epoch=049 loss=1.2175 time=1.3s | VAL HR@20=0.550265 | best=0.550265 (epoch 46) | bad_epochs=3\n",
      "[10-17] epoch=050 loss=1.1674 time=1.3s | VAL HR@20=0.566138 | best=0.566138 (epoch 50) | bad_epochs=0\n",
      "[10-17] epoch=051 loss=1.1307 time=1.2s | VAL HR@20=0.566138 | best=0.566138 (epoch 50) | bad_epochs=1\n",
      "[10-17] epoch=052 loss=1.0687 time=1.2s | VAL HR@20=0.566138 | best=0.566138 (epoch 50) | bad_epochs=2\n",
      "[10-17] epoch=053 loss=1.0413 time=1.1s | VAL HR@20=0.560847 | best=0.566138 (epoch 50) | bad_epochs=3\n",
      "[10-17] epoch=054 loss=1.0232 time=1.4s | VAL HR@20=0.560847 | best=0.566138 (epoch 50) | bad_epochs=4\n",
      "[10-17] epoch=055 loss=0.9901 time=1.5s | VAL HR@20=0.560847 | best=0.566138 (epoch 50) | bad_epochs=5\n",
      "[10-17] epoch=056 loss=0.9629 time=1.5s | VAL HR@20=0.555556 | best=0.566138 (epoch 50) | bad_epochs=6\n",
      "[10-17] epoch=057 loss=0.9125 time=1.5s | VAL HR@20=0.555556 | best=0.566138 (epoch 50) | bad_epochs=7\n",
      "[10-17] epoch=058 loss=0.9055 time=1.4s | VAL HR@20=0.555556 | best=0.566138 (epoch 50) | bad_epochs=8\n",
      "[10-17] epoch=059 loss=0.8604 time=1.1s | VAL HR@20=0.560847 | best=0.566138 (epoch 50) | bad_epochs=9\n",
      "[10-17] epoch=060 loss=0.8488 time=1.1s | VAL HR@20=0.560847 | best=0.566138 (epoch 50) | bad_epochs=10\n",
      "[10-17] ✅ Early stop at epoch=60 (best epoch=50, best HR@20=0.566138)\n",
      "[10-17] ✅ Restored best weights from epoch=50 with best HR@20=0.566138\n",
      "\n",
      "[10-17] CHECKPOINT M\n",
      "Paste: best_epoch + best_metric + last 3 epoch log lines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-17] Train SASRecRightPad (NEW RUN TAG) + early stopping on VAL HR@20\n",
    "\n",
    "RUN_TAG_RPAD = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[10-17] RUN_TAG_RPAD:\", RUN_TAG_RPAD)\n",
    "\n",
    "SAS_CFG_RPAD = dict(SAS_CFG)\n",
    "# keep same cfg first; we only changed padding semantics\n",
    "print(\"[10-17] SAS_CFG_RPAD:\", SAS_CFG_RPAD)\n",
    "\n",
    "set_seed(SAS_CFG_RPAD[\"seed\"])\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_rp = SASRecRightPad(\n",
    "    vocab_size=vocab_size_target,\n",
    "    max_len=MAX_PREFIX_LEN,\n",
    "    d_model=SAS_CFG_RPAD[\"d_model\"],\n",
    "    n_heads=SAS_CFG_RPAD[\"n_heads\"],\n",
    "    n_layers=SAS_CFG_RPAD[\"n_layers\"],\n",
    "    d_ff=SAS_CFG_RPAD[\"d_ff\"],\n",
    "    dropout=SAS_CFG_RPAD[\"dropout\"],\n",
    "    pad_id=PAD_ID_TARGET,\n",
    ").to(device)\n",
    "\n",
    "opt_rp = torch.optim.Adam(model_rp.parameters(), lr=SAS_CFG_RPAD[\"lr\"], weight_decay=SAS_CFG_RPAD[\"weight_decay\"])\n",
    "\n",
    "def eval_sas_rp(model: nn.Module, data: dict) -> dict:\n",
    "    model.eval()\n",
    "    metrics = init_metrics()\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, am, y in iter_batches(data, batch_size=SAS_CFG_RPAD[\"batch_size\"], shuffle=False):\n",
    "            logits = model(x, am)\n",
    "            logits[:, PAD_ID_TARGET] = -1e9\n",
    "            topk = torch.topk(logits, k=MAX_K, dim=1).indices.cpu().numpy()\n",
    "            y_np = y.cpu().numpy()\n",
    "            for i in range(topk.shape[0]):\n",
    "                yi = int(y_np[i])\n",
    "                if yi == PAD_ID_TARGET:\n",
    "                    continue\n",
    "                pos = np.where(topk[i] == yi)[0]\n",
    "                rank0 = int(pos[0]) if pos.size > 0 else None\n",
    "                update_metrics_from_rank(metrics, rank0)\n",
    "                n += 1\n",
    "    out = finalize_metrics(metrics, n)\n",
    "    out[\"_n_examples\"] = int(n)\n",
    "    return out\n",
    "\n",
    "best_metric = -1.0\n",
    "best_epoch = -1\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "\n",
    "train_losses_rp = []\n",
    "val_history_rp = []\n",
    "\n",
    "metric_name = SAS_CFG_RPAD[\"early_stop_metric\"]\n",
    "patience = int(SAS_CFG_RPAD[\"patience\"])\n",
    "min_delta = float(SAS_CFG_RPAD[\"min_delta\"])\n",
    "\n",
    "print(f\"[10-17] Early stopping on {metric_name} | patience={patience} | min_delta={min_delta}\")\n",
    "\n",
    "for epoch in range(1, int(SAS_CFG_RPAD[\"max_epochs\"]) + 1):\n",
    "    model_rp.train()\n",
    "    t0 = time.time()\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for x, am, y in iter_batches(target_train, SAS_CFG_RPAD[\"batch_size\"], shuffle=True):\n",
    "        opt_rp.zero_grad()\n",
    "        logits = model_rp(x, am)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=PAD_ID_TARGET)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_rp.parameters(), SAS_CFG_RPAD[\"grad_clip\"])\n",
    "        opt_rp.step()\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_n += bs\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_n)\n",
    "    train_losses_rp.append(avg_loss)\n",
    "\n",
    "    val_metrics = eval_sas_rp(model_rp, target_val)\n",
    "    val_history_rp.append(val_metrics)\n",
    "\n",
    "    cur = float(val_metrics.get(metric_name, 0.0))\n",
    "    improved = (cur > best_metric + min_delta)\n",
    "    if improved:\n",
    "        best_metric = cur\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model_rp.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    print(f\"[10-17] epoch={epoch:03d} loss={avg_loss:.4f} time={time.time()-t0:.1f}s | \"\n",
    "          f\"VAL {metric_name}={cur:.6f} | best={best_metric:.6f} (epoch {best_epoch}) | bad_epochs={bad_epochs}\")\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(f\"[10-17] ✅ Early stop at epoch={epoch} (best epoch={best_epoch}, best {metric_name}={best_metric:.6f})\")\n",
    "        break\n",
    "\n",
    "assert best_state is not None\n",
    "model_rp.load_state_dict(best_state)\n",
    "print(f\"[10-17] ✅ Restored best weights from epoch={best_epoch} with best {metric_name}={best_metric:.6f}\")\n",
    "\n",
    "print(\"\\n[10-17] CHECKPOINT M\")\n",
    "print(\"Paste: best_epoch + best_metric + last 3 epoch log lines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630829d6",
   "metadata": {},
   "source": [
    "Evaluate SASRecRightPad (VAL + TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1082071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-18] TARGET VAL (SASRecRightPad): {'HR@5': 0.4973544973544973, 'HR@10': 0.5132275132275133, 'HR@20': 0.5661375661375662, 'MRR@5': 0.4210758377425044, 'MRR@10': 0.42359536407155457, 'MRR@20': 0.42723215364745665, 'NDCG@5': 0.44003450264380367, 'NDCG@10': 0.4455675592975041, 'NDCG@20': 0.45890533525978944, '_n_examples': 189}\n",
      "[10-18] TARGET TEST (SASRecRightPad): {'HR@5': 0.44, 'HR@10': 0.455, 'HR@20': 0.485, 'MRR@5': 0.39049999999999996, 'MRR@10': 0.3928809523809524, 'MRR@20': 0.3950561773752563, 'NDCG@5': 0.4028467478321102, 'NDCG@10': 0.40807548636985713, 'NDCG@20': 0.41577484972680745, '_n_examples': 200}\n",
      "\n",
      "[10-18] CHECKPOINT N\n",
      "Paste VAL/TEST metrics. We should now be >= MostPop on HR@20 (val ~0.175).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-18] Evaluate SASRecRightPad (VAL + TEST)\n",
    "\n",
    "t_val_rp = eval_sas_rp(model_rp, target_val)\n",
    "t_test_rp = eval_sas_rp(model_rp, target_test)\n",
    "\n",
    "print(\"[10-18] TARGET VAL (SASRecRightPad):\", t_val_rp)\n",
    "print(\"[10-18] TARGET TEST (SASRecRightPad):\", t_test_rp)\n",
    "\n",
    "print(\"\\n[10-18] CHECKPOINT N\")\n",
    "print(\"Paste VAL/TEST metrics. We should now be >= MostPop on HR@20 (val ~0.175).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1744eb2",
   "metadata": {},
   "source": [
    "Write FIXED report artifacts under reports/10_sasrec_baseline/<RUN_TAG_RPAD>/ + meta.json update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3b8bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10-19] ✅ Wrote FIXED report files under: C:\\mooc-coldstart-session-meta\\reports\\10_sasrec_baseline\\20260102_234927\n",
      "[10-19] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[10-19] CHECKPOINT O\n",
      "Paste: REPORT_DIR_RPAD + confirm meta.json updated.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 10-19] Write FIXED report artifacts under reports/10_sasrec_baseline/<RUN_TAG_RPAD>/ + meta.json update\n",
    "\n",
    "REPORT_DIR_RPAD = REPO_ROOT / \"reports\" / \"10_sasrec_baseline\" / RUN_TAG_RPAD\n",
    "REPORT_DIR_RPAD.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj: dict, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "run_meta_rp = {\n",
    "    \"run_tag\": RUN_TAG_RPAD,\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"inputs\": {\n",
    "        \"target_run_tag\": TARGET_TAG,\n",
    "        \"source_run_tag\": SOURCE_TAG,\n",
    "        \"target_train_pt\": str(target_train_pt),\n",
    "        \"target_val_pt\": str(target_val_pt),\n",
    "        \"target_test_pt\": str(target_test_pt),\n",
    "        \"target_vocab_json\": str(target_vocab_json),\n",
    "        \"dataloader_config\": str(cfg_path_repo),\n",
    "        \"sanity_metrics\": str(sanity_path_repo),\n",
    "        \"session_gap_thresholds\": str(gaps_path_repo),\n",
    "    },\n",
    "    \"protocol_reused_from_06\": {\n",
    "        \"K_LIST\": K_LIST,\n",
    "        \"MAX_PREFIX_LEN\": MAX_PREFIX_LEN,\n",
    "        \"PAD_ID_TARGET\": PAD_ID_TARGET,\n",
    "        \"pad_excluded_from_ranking\": True,\n",
    "        \"causal_mask\": True,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"SASRecRightPad\",\n",
    "        \"vocab_size\": int(vocab_size_target),\n",
    "        \"d_model\": int(SAS_CFG_RPAD[\"d_model\"]),\n",
    "        \"n_heads\": int(SAS_CFG_RPAD[\"n_heads\"]),\n",
    "        \"n_layers\": int(SAS_CFG_RPAD[\"n_layers\"]),\n",
    "        \"d_ff\": int(SAS_CFG_RPAD[\"d_ff\"]),\n",
    "        \"dropout\": float(SAS_CFG_RPAD[\"dropout\"]),\n",
    "    },\n",
    "    \"train_cfg\": SAS_CFG_RPAD,\n",
    "    \"early_stopping\": {\n",
    "        \"metric\": SAS_CFG_RPAD[\"early_stop_metric\"],\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_metric\": float(best_metric),\n",
    "        \"patience\": int(SAS_CFG_RPAD[\"patience\"]),\n",
    "        \"min_delta\": float(SAS_CFG_RPAD[\"min_delta\"]),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"CRITICAL FIX: target tensors are left-padded; SASRec is evaluated/trained with on-the-fly right-padding to align absolute position embeddings.\",\n",
    "        \"This run supersedes earlier SASRec attempts that were effectively broken under left padding.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "results_rp = {\n",
    "    \"target\": {\n",
    "        \"val\": t_val_rp,\n",
    "        \"test\": t_test_rp,\n",
    "        \"train_losses\": train_losses_rp,\n",
    "        \"val_history\": val_history_rp,\n",
    "    },\n",
    "    \"source\": None,\n",
    "}\n",
    "\n",
    "save_json(run_meta_rp, REPORT_DIR_RPAD / \"run_meta.json\")\n",
    "save_json(results_rp, REPORT_DIR_RPAD / \"results.json\")\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": model_rp.state_dict(),\n",
    "        \"sas_cfg\": SAS_CFG_RPAD,\n",
    "        \"vocab_size_target\": vocab_size_target,\n",
    "        \"pad_id\": PAD_ID_TARGET,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_metric\": best_metric,\n",
    "        \"note\": \"SASRecRightPad (left->right pad inside forward)\",\n",
    "    },\n",
    "    REPORT_DIR_RPAD / \"model.pt\",\n",
    ")\n",
    "\n",
    "meta_path = REPO_ROOT / \"meta.json\"\n",
    "meta = load_json(meta_path) if meta_path.exists() else {\"artifacts\": {}}\n",
    "meta.setdefault(\"artifacts\", {})\n",
    "meta[\"artifacts\"].setdefault(\"sasrec_baseline\", {})\n",
    "meta[\"artifacts\"][\"sasrec_baseline\"][RUN_TAG_RPAD] = {\n",
    "    \"target_run_tag\": TARGET_TAG,\n",
    "    \"source_run_tag\": SOURCE_TAG,\n",
    "    \"report_dir\": str(REPORT_DIR_RPAD),\n",
    "    \"results_json\": str(REPORT_DIR_RPAD / \"results.json\"),\n",
    "    \"run_meta_json\": str(REPORT_DIR_RPAD / \"run_meta.json\"),\n",
    "    \"model_pt\": str(REPORT_DIR_RPAD / \"model.pt\"),\n",
    "    \"note\": \"Fixed: right-pad-on-the-fly to correct left-padding artifact.\",\n",
    "}\n",
    "meta[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "save_json(meta, meta_path)\n",
    "\n",
    "print(\"[10-19] ✅ Wrote FIXED report files under:\", REPORT_DIR_RPAD)\n",
    "print(\"[10-19] ✅ Updated meta.json:\", meta_path)\n",
    "\n",
    "print(\"\\n[10-19] CHECKPOINT O\")\n",
    "print(\"Paste: REPORT_DIR_RPAD + confirm meta.json updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
