{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4152a563",
   "metadata": {},
   "source": [
    "##### 01 — EDA (Source MOOC Dataset: XuetangX raw)\n",
    "\n",
    "This notebook validates and profiles the **source MOOC dataset** (XuetangX raw user activity) and produces **reproducible EDA artifacts**.\n",
    "\n",
    "Scope (this notebook):\n",
    "- Verify `data/raw/xuetangx` contents (raw JSON)\n",
    "- Stream-parse raw JSON → **Parquet shards** (one-time conversion)\n",
    "- DuckDB EDA: counts, time range, top actions, distributions\n",
    "- Export **filtered course-level interactions** (for later notebooks)\n",
    "- Save plots + tables + dataset metadata into `./reports/01_eda_source_mooc/<RUN_TAG>/`\n",
    "\n",
    "Out of scope (planned notebooks 04/05):\n",
    "- Session-gap analysis, sessionization, prefix/target building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8358103",
   "metadata": {},
   "source": [
    "Imports (single place for shared imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-01] Imports (UPDATED for XuetangX raw JSON -> Parquet -> DuckDB)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "def log(msg: str):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[01] {ts} | {msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866f5d",
   "metadata": {},
   "source": [
    "Bootstrap: force repo root + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-02] Bootstrap: locate repo root reliably (Windows-safe)\n",
    "\n",
    "CWD = Path.cwd().resolve()\n",
    "log(f\"Initial CWD: {CWD}\")\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Search upward for repo root.\n",
    "    Priority: look for PROJECT_STATE.md (most specific).\n",
    "    Fallback: .git folder.\n",
    "    \"\"\"\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find repo root (PROJECT_STATE.md or .git not found upward).\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(CWD)\n",
    "log(f\"REPO_ROOT detected: {REPO_ROOT}\")\n",
    "\n",
    "# Ensure imports work regardless of where Jupyter was launched\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "SRC_DIR = REPO_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "checks = {\n",
    "    \"src/\": SRC_DIR.exists(),\n",
    "    \"notebooks/\": (REPO_ROOT / \"notebooks\").exists(),\n",
    "    \"PROJECT_STATE.md\": (REPO_ROOT / \"PROJECT_STATE.md\").exists(),\n",
    "    \"src/configs/project.yaml\": (REPO_ROOT / \"src\" / \"configs\" / \"project.yaml\").exists(),\n",
    "}\n",
    "\n",
    "log(\"Validation checks:\")\n",
    "for name, exists in checks.items():\n",
    "    status = \"✅\" if exists else \"❌\"\n",
    "    print(f\"  {status} {name}\")\n",
    "\n",
    "if not checks[\"PROJECT_STATE.md\"]:\n",
    "    raise FileNotFoundError(\"PROJECT_STATE.md not found in detected repo root — wrong working directory?\")\n",
    "\n",
    "if not checks[\"src/configs/project.yaml\"]:\n",
    "    raise FileNotFoundError(\"src/configs/project.yaml not found — please ensure the config exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb618b",
   "metadata": {},
   "source": [
    "Load project config (YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-03] Load config (single source of truth): src/configs/project.yaml\n",
    "\n",
    "CFG_PATH = REPO_ROOT / \"src\" / \"configs\" / \"project.yaml\"\n",
    "log(f\"Loading config: {CFG_PATH}\")\n",
    "\n",
    "with open(CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "# Print top-level keys (no assumptions)\n",
    "log(\"Config loaded. Top-level keys:\")\n",
    "print(sorted(list(CFG.keys())))\n",
    "\n",
    "def get_cfg(d, path, default=None):\n",
    "    \"\"\"\n",
    "    Safe nested getter. Example: get_cfg(CFG, \"paths.data_raw\")\n",
    "    \"\"\"\n",
    "    cur = d\n",
    "    for part in path.split(\".\"):\n",
    "        if isinstance(cur, dict) and part in cur:\n",
    "            cur = cur[part]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "# Attempt to identify source dataset fields without guessing schema\n",
    "SOURCE_NAME = (\n",
    "    get_cfg(CFG, \"datasets.source.name\")\n",
    "    or get_cfg(CFG, \"dataset.source.name\")\n",
    "    or get_cfg(CFG, \"source_dataset.name\")\n",
    "    or get_cfg(CFG, \"source.name\")\n",
    "    or None\n",
    ")\n",
    "SOURCE_RAW_SUBDIR = (\n",
    "    get_cfg(CFG, \"datasets.source.raw_subdir\")\n",
    "    or get_cfg(CFG, \"dataset.source.raw_subdir\")\n",
    "    or get_cfg(CFG, \"source_dataset.raw_subdir\")\n",
    "    or get_cfg(CFG, \"source.raw_subdir\")\n",
    "    or None\n",
    ")\n",
    "\n",
    "RAW_DIR_CFG = (\n",
    "    get_cfg(CFG, \"paths.data_raw\")\n",
    "    or get_cfg(CFG, \"paths.raw_dir\")\n",
    "    or get_cfg(CFG, \"data.raw_dir\")\n",
    "    or None\n",
    ")\n",
    "\n",
    "RAW_DIR = Path(RAW_DIR_CFG) if RAW_DIR_CFG else (REPO_ROOT / \"data\" / \"raw\")\n",
    "RAW_DIR = RAW_DIR.expanduser().resolve()\n",
    "\n",
    "log(f\"SOURCE_NAME (from config): {SOURCE_NAME if SOURCE_NAME else 'we don’t know yet'}\")\n",
    "log(f\"RAW_DIR resolved: {RAW_DIR}\")\n",
    "\n",
    "# If a dataset subdir is provided, resolve it (but don't assume it exists)\n",
    "SOURCE_DIR = (RAW_DIR / SOURCE_RAW_SUBDIR).resolve() if SOURCE_RAW_SUBDIR else RAW_DIR\n",
    "log(f\"SOURCE_DIR resolved: {SOURCE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6c4f2",
   "metadata": {},
   "source": [
    "## A) Raw data inventory + selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a992f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-04] Inventory data/raw (recursive) + size summary (REAL files only)\n",
    "\n",
    "if RAW_DIR is None:\n",
    "    raise RuntimeError(\"RAW_DIR is None. Config loader did not resolve RAW_DIR.\")\n",
    "\n",
    "if not RAW_DIR.exists():\n",
    "    raise FileNotFoundError(f\"RAW_DIR does not exist: {RAW_DIR}\")\n",
    "\n",
    "def human_bytes(n: int) -> str:\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    f = float(n)\n",
    "    for u in units:\n",
    "        if f < 1024 or u == units[-1]:\n",
    "            return f\"{f:.2f} {u}\"\n",
    "        f /= 1024\n",
    "\n",
    "log(f\"Scanning RAW_DIR: {RAW_DIR}\")\n",
    "\n",
    "files = []\n",
    "for p in RAW_DIR.rglob(\"*\"):\n",
    "    if p.is_file():\n",
    "        try:\n",
    "            files.append((p, p.stat().st_size))\n",
    "        except OSError:\n",
    "            files.append((p, None))\n",
    "\n",
    "total = sum(s for _, s in files if isinstance(s, int))\n",
    "log(f\"Found files: {len(files)}\")\n",
    "log(f\"Total size: {human_bytes(total)}\")\n",
    "\n",
    "subdirs = sorted([d for d in RAW_DIR.iterdir() if d.is_dir()])\n",
    "log(f\"Top-level subdirs under RAW_DIR ({len(subdirs)}): {[d.name for d in subdirs]}\")\n",
    "\n",
    "# Size by top-level subdir (1 level)\n",
    "size_by_subdir = {}\n",
    "for d in subdirs:\n",
    "    t = 0\n",
    "    for p, s in files:\n",
    "        if s is None:\n",
    "            continue\n",
    "        try:\n",
    "            if d in p.parents:\n",
    "                t += s\n",
    "        except Exception:\n",
    "            pass\n",
    "    size_by_subdir[d.name] = t\n",
    "\n",
    "print(\"\\nSize by top-level subdir:\")\n",
    "for k, v in sorted(size_by_subdir.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    print(f\"  - {k:20s} {human_bytes(v)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-05] Select source raw subdir for this notebook (XuetangX)\n",
    "\n",
    "SOURCE_DATASET_SUBDIR = \"xuetangx\"\n",
    "SOURCE_DIR = (RAW_DIR / SOURCE_DATASET_SUBDIR).resolve()\n",
    "\n",
    "log(f\"SOURCE_DIR: {SOURCE_DIR}\")\n",
    "log(f\"SOURCE_DIR exists: {SOURCE_DIR.exists()}\")\n",
    "\n",
    "if not SOURCE_DIR.exists():\n",
    "    raise FileNotFoundError(\"Expected data/raw/xuetangx folder not found. Place XuetangX raw JSON files there.\")\n",
    "\n",
    "json_files = sorted(SOURCE_DIR.glob(\"*raw_user_activity*.json\"))\n",
    "log(f\"Raw XuetangX JSON files found: {len(json_files)}\")\n",
    "\n",
    "for p in json_files:\n",
    "    print(f\"  - {p.name:45s} {human_bytes(p.stat().st_size)}\")\n",
    "\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(\"No *raw_user_activity*.json files found in data/raw/xuetangx/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae4dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-06] Quick HEAD/TAIL check (confirm JSON array + structure)\n",
    "\n",
    "def head_tail_bytes(path: Path, n=220):\n",
    "    size = path.stat().st_size\n",
    "    with open(path, \"rb\") as f:\n",
    "        head = f.read(n)\n",
    "        f.seek(max(0, size - n))\n",
    "        tail = f.read(n)\n",
    "    return head, tail\n",
    "\n",
    "p = json_files[0]\n",
    "log(f\"Inspecting: {p.name}\")\n",
    "head, tail = head_tail_bytes(p)\n",
    "\n",
    "print(\"HEAD:\", head)\n",
    "print(\"TAIL:\", tail)\n",
    "\n",
    "if not head.lstrip().startswith(b\"[\"):\n",
    "    log(\"⚠️ File does not appear to start with '['; structure might differ.\")\n",
    "if b\"]\" not in tail:\n",
    "    log(\"⚠️ File does not appear to end with ']'; structure might differ.\")\n",
    "\n",
    "log(\"Expected structure per top-level item: [course_id (str), user_map (dict)]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c36371",
   "metadata": {},
   "source": [
    "## B) One-time conversion: raw JSON → Parquet shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-07] Ensure required libs are available: ijson, pyarrow, duckdb\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "def ensure(pkg: str, import_name: str = None):\n",
    "    name = import_name or pkg\n",
    "    try:\n",
    "        return importlib.import_module(name)\n",
    "    except Exception:\n",
    "        log(f\"Installing {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        return importlib.import_module(name)\n",
    "\n",
    "ijson = ensure(\"ijson\")\n",
    "duckdb = ensure(\"duckdb\")\n",
    "pa = ensure(\"pyarrow\", \"pyarrow\")\n",
    "pq = ensure(\"pyarrow\", \"pyarrow.parquet\")\n",
    "\n",
    "log(\"All dependencies ready: ijson, pyarrow, duckdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-08] Stream-sample raw JSON (K course blocks) and summarize actions/time range\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def iter_course_blocks(path: Path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        for item in ijson.items(f, \"item\"):\n",
    "            yield item\n",
    "\n",
    "def sample_summary(path: Path, K=5, max_users_per_course=50, max_events_per_object=500):\n",
    "    action_counts = Counter()\n",
    "    total_events = 0\n",
    "    total_users = 0\n",
    "    total_objects = 0\n",
    "    min_ts = None\n",
    "    max_ts = None\n",
    "\n",
    "    seen_courses = 0\n",
    "    for blk in iter_course_blocks(path):\n",
    "        if not (isinstance(blk, list) and len(blk) == 2):\n",
    "            continue\n",
    "\n",
    "        course_id, user_map = blk[0], blk[1]\n",
    "        if not (isinstance(course_id, str) and isinstance(user_map, dict)):\n",
    "            continue\n",
    "\n",
    "        seen_courses += 1\n",
    "\n",
    "        for ui, (user_id, obj_map) in enumerate(user_map.items()):\n",
    "            if ui >= max_users_per_course:\n",
    "                break\n",
    "            total_users += 1\n",
    "\n",
    "            if not isinstance(obj_map, dict):\n",
    "                continue\n",
    "\n",
    "            for obj_id, evs in obj_map.items():\n",
    "                total_objects += 1\n",
    "                if not isinstance(evs, list):\n",
    "                    continue\n",
    "\n",
    "                for ev in evs[:max_events_per_object]:\n",
    "                    if not (isinstance(ev, list) and len(ev) == 2):\n",
    "                        continue\n",
    "                    action, ts = ev[0], ev[1]\n",
    "                    action_counts[str(action)] += 1\n",
    "                    total_events += 1\n",
    "\n",
    "                    dt = pd.to_datetime(ts, errors=\"coerce\")\n",
    "                    if pd.notna(dt):\n",
    "                        if min_ts is None or dt < min_ts:\n",
    "                            min_ts = dt\n",
    "                        if max_ts is None or dt > max_ts:\n",
    "                            max_ts = dt\n",
    "\n",
    "        if seen_courses >= K:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nSampled file: {path.name}\")\n",
    "    print(\"Sampled courses:\", seen_courses)\n",
    "    print(\"Sampled users:\", total_users)\n",
    "    print(\"Sampled objects:\", total_objects)\n",
    "    print(\"Sampled events:\", total_events)\n",
    "    print(\"Sample time range:\", min_ts, \"->\", max_ts)\n",
    "    print(\"\\nTop actions:\")\n",
    "    for a, c in action_counts.most_common(20):\n",
    "        print(f\"  {a:25s} {c}\")\n",
    "\n",
    "sample_summary(json_files[0], K=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-09] Stream parse -> Parquet shards (one-time conversion)\n",
    "\n",
    "OUT_DIR = (REPO_ROOT / \"data\" / \"processed\" / \"xuetangx_events_parquet\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "log(f\"OUT_DIR: {OUT_DIR}\")\n",
    "\n",
    "def write_shard(rows, shard_path: Path):\n",
    "    df = pd.DataFrame(rows, columns=[\"course_id\", \"user_id\", \"object_id\", \"action\", \"ts\"])\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"course_id\", \"user_id\", \"action\", \"ts\"])\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    pq.write_table(table, shard_path)\n",
    "    return len(df)\n",
    "\n",
    "def convert_file_to_parquet_shards(path: Path, shard_rows=1_000_000, skip_if_exists=True):\n",
    "    existing = sorted(OUT_DIR.glob(f\"{path.stem}_shard_*.parquet\"))\n",
    "    if skip_if_exists and existing:\n",
    "        log(f\"SKIP {path.name} (found {len(existing)} existing shards)\")\n",
    "        return None, len(existing)\n",
    "\n",
    "    shard_idx = 0\n",
    "    buf = []\n",
    "    total_written = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        for blk in ijson.items(f, \"item\"):\n",
    "            if not (isinstance(blk, list) and len(blk) == 2):\n",
    "                continue\n",
    "            course_id, user_map = blk[0], blk[1]\n",
    "            if not (isinstance(course_id, str) and isinstance(user_map, dict)):\n",
    "                continue\n",
    "\n",
    "            for user_id, obj_map in user_map.items():\n",
    "                if not isinstance(obj_map, dict):\n",
    "                    continue\n",
    "                for object_id, evs in obj_map.items():\n",
    "                    if not isinstance(evs, list):\n",
    "                        continue\n",
    "                    for ev in evs:\n",
    "                        if not (isinstance(ev, list) and len(ev) == 2):\n",
    "                            continue\n",
    "                        action, ts = ev[0], ev[1]\n",
    "                        buf.append([course_id, str(user_id), str(object_id), str(action), ts])\n",
    "\n",
    "                        if len(buf) >= shard_rows:\n",
    "                            shard_path = OUT_DIR / f\"{path.stem}_shard_{shard_idx:04d}.parquet\"\n",
    "                            n = write_shard(buf, shard_path)\n",
    "                            total_written += n\n",
    "                            buf = []\n",
    "                            shard_idx += 1\n",
    "\n",
    "                            elapsed = time.time() - t0\n",
    "                            log(f\"{path.name} | shards={shard_idx} | written={total_written:,} | elapsed={elapsed/60:.1f}m\")\n",
    "\n",
    "    if buf:\n",
    "        shard_path = OUT_DIR / f\"{path.stem}_shard_{shard_idx:04d}.parquet\"\n",
    "        n = write_shard(buf, shard_path)\n",
    "        total_written += n\n",
    "        shard_idx += 1\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    log(f\"DONE {path.name} | shards={shard_idx} | total_written={total_written:,} | elapsed={elapsed/60:.1f}m\")\n",
    "    return total_written, shard_idx\n",
    "\n",
    "totals = []\n",
    "for p in json_files:\n",
    "    rows_written, shards = convert_file_to_parquet_shards(p, shard_rows=1_000_000, skip_if_exists=True)\n",
    "    totals.append((p.name, rows_written, shards))\n",
    "\n",
    "print(\"\\nConversion summary:\")\n",
    "for name, rows_written, shards in totals:\n",
    "    rows_txt = f\"{rows_written:,}\" if isinstance(rows_written, int) else \"(skipped)\"\n",
    "    print(f\"- {name}: rows={rows_txt} shards={shards}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020b298",
   "metadata": {},
   "source": [
    "## C) DuckDB EDA + filtered interactions export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-10] DuckDB EDA on Parquet shards (fast)\n",
    "\n",
    "PARQUET_GLOB = str(OUT_DIR / \"*.parquet\")\n",
    "log(f\"Reading Parquet via DuckDB: {PARQUET_GLOB}\")\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW events AS\n",
    "SELECT * FROM read_parquet('{PARQUET_GLOB}');\n",
    "\"\"\")\n",
    "\n",
    "df_counts = con.execute(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS n_events,\n",
    "  COUNT(DISTINCT user_id) AS n_users,\n",
    "  COUNT(DISTINCT course_id) AS n_courses,\n",
    "  COUNT(DISTINCT object_id) AS n_objects,\n",
    "  MIN(ts) AS min_ts,\n",
    "  MAX(ts) AS max_ts\n",
    "FROM events;\n",
    "\"\"\").df()\n",
    "\n",
    "df_top_actions = con.execute(\"\"\"\n",
    "SELECT action, COUNT(*) AS n\n",
    "FROM events\n",
    "GROUP BY action\n",
    "ORDER BY n DESC\n",
    "LIMIT 30;\n",
    "\"\"\").df()\n",
    "\n",
    "df_user_quant = con.execute(\"\"\"\n",
    "SELECT approx_quantile(cnt, [0.5, 0.9, 0.99]) AS q_events_per_user\n",
    "FROM (\n",
    "  SELECT user_id, COUNT(*) cnt\n",
    "  FROM events\n",
    "  GROUP BY user_id\n",
    ");\n",
    "\"\"\").df()\n",
    "\n",
    "display(df_counts)\n",
    "display(df_top_actions)\n",
    "display(df_user_quant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca73ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-11] Export filtered course-level interactions (for later notebooks)\n",
    "\n",
    "KEEP_ACTIONS = {\n",
    "    \"click_courseware\",\n",
    "    \"load_video\",\n",
    "    \"play_video\",\n",
    "    \"problem_get\",\n",
    "    \"problem_check\",\n",
    "    \"click_info\",\n",
    "    \"click_about\",\n",
    "    \"click_progress\",\n",
    "    \"click_forum\",\n",
    "}\n",
    "\n",
    "actions_list = \",\".join([f\"'{a}'\" for a in sorted(KEEP_ACTIONS)])\n",
    "filtered_path = (REPO_ROOT / \"data\" / \"processed\" / \"xuetangx_interactions_course_filtered.parquet\").resolve()\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    course_id AS item_id,\n",
    "    ts,\n",
    "    action\n",
    "  FROM events\n",
    "  WHERE action IN ({actions_list})\n",
    ") TO '{str(filtered_path)}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "log(f\"Wrote filtered interactions parquet: {filtered_path}\")\n",
    "\n",
    "con.execute(f\"CREATE OR REPLACE VIEW inter AS SELECT * FROM read_parquet('{str(filtered_path)}');\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e870b",
   "metadata": {},
   "source": [
    "## D) Reports + reproducibility artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-12] Reports output folder (per-run)\n",
    "\n",
    "REPORT_DIR = (REPO_ROOT / \"reports\" / \"01_eda_source_mooc\").resolve()\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT = REPORT_DIR / RUN_TAG\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log(f\"REPORT OUT: {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-13] Save dataset metadata for reproducibility\n",
    "\n",
    "# Raw files inventory\n",
    "raw_files = []\n",
    "for p in sorted(SOURCE_DIR.glob(\"*.json\")):\n",
    "    st = p.stat()\n",
    "    raw_files.append({\n",
    "        \"name\": p.name,\n",
    "        \"path\": str(p),\n",
    "        \"size_bytes\": int(st.st_size),\n",
    "        \"mtime\": datetime.fromtimestamp(st.st_mtime).isoformat(timespec=\"seconds\"),\n",
    "    })\n",
    "\n",
    "# Parquet shards inventory\n",
    "parquet_files = sorted(OUT_DIR.glob(\"*.parquet\"))\n",
    "parquet_total_bytes = sum(p.stat().st_size for p in parquet_files)\n",
    "\n",
    "counts_row = con.execute(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS n_events,\n",
    "  COUNT(DISTINCT user_id) AS n_users,\n",
    "  COUNT(DISTINCT course_id) AS n_courses,\n",
    "  COUNT(DISTINCT object_id) AS n_objects,\n",
    "  MIN(ts) AS min_ts,\n",
    "  MAX(ts) AS max_ts\n",
    "FROM events;\n",
    "\"\"\").fetchone()\n",
    "\n",
    "top_actions = con.execute(\"\"\"\n",
    "SELECT action, COUNT(*) AS n\n",
    "FROM events\n",
    "GROUP BY action\n",
    "ORDER BY n DESC\n",
    "LIMIT 30;\n",
    "\"\"\").df().to_dict(orient=\"records\")\n",
    "\n",
    "user_q = con.execute(\"\"\"\n",
    "SELECT approx_quantile(cnt, [0.5, 0.9, 0.99]) AS q_events_per_user\n",
    "FROM (SELECT user_id, COUNT(*) cnt FROM events GROUP BY user_id);\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "meta = {\n",
    "    \"generated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"source_dir\": str(SOURCE_DIR),\n",
    "    \"raw_files\": raw_files,\n",
    "    \"events_parquet_dir\": str(OUT_DIR),\n",
    "    \"events_parquet_shards\": len(parquet_files),\n",
    "    \"events_parquet_total_bytes\": int(parquet_total_bytes),\n",
    "    \"duckdb_events_view\": str(OUT_DIR / \"*.parquet\"),\n",
    "    \"counts\": {\n",
    "        \"n_events\": int(counts_row[0]),\n",
    "        \"n_users\": int(counts_row[1]),\n",
    "        \"n_courses\": int(counts_row[2]),\n",
    "        \"n_objects\": int(counts_row[3]),\n",
    "        \"min_ts\": str(counts_row[4]),\n",
    "        \"max_ts\": str(counts_row[5]),\n",
    "    },\n",
    "    \"top_actions\": top_actions,\n",
    "    \"events_per_user_quantiles\": user_q,\n",
    "    \"filtered_interactions_parquet\": str(filtered_path),\n",
    "    \"filtered_actions\": sorted(list(KEEP_ACTIONS)),\n",
    "    \"env\": {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"platform\": platform.platform(),\n",
    "        \"pandas\": getattr(pd, \"__version__\", \"unknown\"),\n",
    "        \"duckdb\": getattr(duckdb, \"__version__\", \"unknown\"),\n",
    "        \"pyarrow\": getattr(pa, \"__version__\", \"unknown\"),\n",
    "        \"ijson\": getattr(ijson, \"__version__\", \"unknown\"),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"Notebook 01 covers EDA + JSON→Parquet conversion + DuckDB summaries only.\",\n",
    "        \"Session-gap analysis and sessionization are planned for notebooks 04/05.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "meta_proc_path = (OUT_DIR / \"dataset_metadata.json\")\n",
    "meta_rep_path = (OUT / \"dataset_metadata.json\")\n",
    "\n",
    "meta_proc_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "meta_rep_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "log(f\"Saved metadata: {meta_proc_path}\")\n",
    "log(f\"Saved metadata copy: {meta_rep_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffe7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-14] Plot: Top actions (save PNG + CSV)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_top20 = con.execute(\"\"\"\n",
    "SELECT action, COUNT(*) AS n\n",
    "FROM events\n",
    "GROUP BY action\n",
    "ORDER BY n DESC\n",
    "LIMIT 20;\n",
    "\"\"\").df()\n",
    "\n",
    "df_top20.to_csv(OUT / \"top_actions_top20.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_top20[\"action\"][::-1], df_top20[\"n\"][::-1])\n",
    "plt.xlabel(\"Event count\")\n",
    "plt.title(\"Top 20 actions (XuetangX raw events)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "png = OUT / \"plot_top20_actions.png\"\n",
    "plt.savefig(png, dpi=200)\n",
    "plt.show()\n",
    "\n",
    "log(f\"Saved: {png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-15] Plot: Daily event volume (save PNG + CSV)\n",
    "\n",
    "df_daily = con.execute(\"\"\"\n",
    "SELECT DATE_TRUNC('day', ts) AS day, COUNT(*) AS n\n",
    "FROM events\n",
    "GROUP BY day\n",
    "ORDER BY day;\n",
    "\"\"\").df()\n",
    "\n",
    "df_daily.to_csv(OUT / \"daily_event_volume.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(df_daily[\"day\"], df_daily[\"n\"])\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.title(\"Daily event volume (XuetangX raw events)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "png = OUT / \"plot_daily_event_volume.png\"\n",
    "plt.savefig(png, dpi=200)\n",
    "plt.show()\n",
    "\n",
    "log(f\"Saved: {png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-16] Plot: User activity distribution (log10 bins) (save PNG + CSV)\n",
    "\n",
    "df_bins = con.execute(\"\"\"\n",
    "WITH u AS (\n",
    "  SELECT user_id, COUNT(*) AS cnt\n",
    "  FROM events\n",
    "  GROUP BY user_id\n",
    "),\n",
    "b AS (\n",
    "  SELECT\n",
    "    CAST(FLOOR(LOG10(cnt)) AS INTEGER) AS log10_bin,\n",
    "    COUNT(*) AS n_users\n",
    "  FROM u\n",
    "  GROUP BY log10_bin\n",
    ")\n",
    "SELECT * FROM b\n",
    "ORDER BY log10_bin;\n",
    "\"\"\").df()\n",
    "\n",
    "df_bins.to_csv(OUT / \"user_activity_log10_bins.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(df_bins[\"log10_bin\"], df_bins[\"n_users\"])\n",
    "plt.xlabel(\"log10(events per user) bin\")\n",
    "plt.ylabel(\"Number of users\")\n",
    "plt.title(\"User activity distribution (binned by log10)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "png = OUT / \"plot_user_activity_log10_bins.png\"\n",
    "plt.savefig(png, dpi=200)\n",
    "plt.show()\n",
    "\n",
    "log(f\"Saved: {png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea212d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-17] Plot: Top courses by event count (save PNG + CSV)\n",
    "\n",
    "df_courses = con.execute(\"\"\"\n",
    "SELECT course_id, COUNT(*) AS n\n",
    "FROM events\n",
    "GROUP BY course_id\n",
    "ORDER BY n DESC\n",
    "LIMIT 20;\n",
    "\"\"\").df()\n",
    "\n",
    "df_courses.to_csv(OUT / \"top_courses_top20.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(df_courses[\"course_id\"][::-1], df_courses[\"n\"][::-1])\n",
    "plt.xlabel(\"Event count\")\n",
    "plt.title(\"Top 20 courses by event count\")\n",
    "plt.tight_layout()\n",
    "\n",
    "png = OUT / \"plot_top20_courses.png\"\n",
    "plt.savefig(png, dpi=200)\n",
    "plt.show()\n",
    "\n",
    "log(f\"Saved: {png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 01-18] Write Markdown report (artifact)\n",
    "\n",
    "md = []\n",
    "md.append(f\"# 01 — EDA Source MOOC (XuetangX raw) — {RUN_TAG}\\n\")\n",
    "md.append(\"## Overview\\n\")\n",
    "md.append(df_counts.to_markdown(index=False))\n",
    "md.append(\"\\n\\n## Top actions (top 20)\\n\")\n",
    "md.append(df_top20.to_markdown(index=False))\n",
    "md.append(\"\\n\\n## User event quantiles\\n\")\n",
    "md.append(df_user_quant.to_markdown(index=False))\n",
    "md.append(\"\\n\\n## Artifacts\\n\")\n",
    "md.append(\"- dataset_metadata.json (also saved under data/processed/xuetangx_events_parquet/)\\n\")\n",
    "md.append(\"- plot_top20_actions.png\\n\")\n",
    "md.append(\"- plot_daily_event_volume.png\\n\")\n",
    "md.append(\"- plot_user_activity_log10_bins.png\\n\")\n",
    "md.append(\"- plot_top20_courses.png\\n\")\n",
    "\n",
    "report_path = OUT / \"report_01_eda_source_mooc.md\"\n",
    "report_path.write_text(\"\\n\".join(md), encoding=\"utf-8\")\n",
    "log(f\"Wrote: {report_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
