{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9284ed5b",
   "metadata": {},
   "source": [
    "Bootstrap: locate repo root (Windows-safe) + env info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3fd576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\mooc-coldstart-session-meta\\notebooks\n",
      "Python: 3.11.14\n",
      "Platform: win32\n",
      "torch: 2.9.1+cpu\n",
      "numpy: 2.4.0\n",
      "pandas: 2.3.3\n",
      "REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "PROC_DIR: C:\\mooc-coldstart-session-meta\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-00] Bootstrap + env info (Windows-safe)\n",
    "\n",
    "import os, sys, json, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "CWD = Path.cwd().resolve()\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", sys.platform)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists() or (p / \".git\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find repo root (PROJECT_STATE.md or .git)\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(CWD)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "print(\"PROC_DIR:\", PROC_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2fc33",
   "metadata": {},
   "source": [
    "Config: RUN_TAG + load tensor packs + metadata (PyTorch 2.6+ safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd11f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_TAG_TARGET: 20251229_163357\n",
      "RUN_TAG_SOURCE: 20251229_232834\n",
      "\n",
      "Target PT exists? True True True True\n",
      "Source sequences dir exists? True\n",
      "Source train glob example: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\train\\sessions_b*.parquet\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-01] Config (REAL run tags + paths)\n",
    "\n",
    "# Target tensors (from 05B)\n",
    "RUN_TAG_TARGET = \"20251229_163357\"\n",
    "TENSOR_TGT_DIR = PROC_DIR / \"tensor_target\"\n",
    "\n",
    "TGT_TRAIN_PT = TENSOR_TGT_DIR / f\"target_tensor_train_{RUN_TAG_TARGET}.pt\"\n",
    "TGT_VAL_PT   = TENSOR_TGT_DIR / f\"target_tensor_val_{RUN_TAG_TARGET}.pt\"\n",
    "TGT_TEST_PT  = TENSOR_TGT_DIR / f\"target_tensor_test_{RUN_TAG_TARGET}.pt\"\n",
    "TGT_META_JSON= TENSOR_TGT_DIR / f\"target_tensor_metadata_{RUN_TAG_TARGET}.json\"\n",
    "\n",
    "# Source session sequences (from 05C)\n",
    "RUN_TAG_SOURCE = \"20251229_232834\"\n",
    "SEQ_SRC_DIR = PROC_DIR / \"session_sequences\" / f\"source_sessions_{RUN_TAG_SOURCE}\"\n",
    "\n",
    "SRC_SEQ_TRAIN_GLOB = SEQ_SRC_DIR / \"train\" / \"sessions_b*.parquet\"\n",
    "SRC_SEQ_VAL_GLOB   = SEQ_SRC_DIR / \"val\"   / \"sessions_b*.parquet\"\n",
    "SRC_SEQ_TEST_GLOB  = SEQ_SRC_DIR / \"test\"  / \"sessions_b*.parquet\"\n",
    "\n",
    "print(\"RUN_TAG_TARGET:\", RUN_TAG_TARGET)\n",
    "print(\"RUN_TAG_SOURCE:\", RUN_TAG_SOURCE)\n",
    "print(\"\\nTarget PT exists?\",\n",
    "      TGT_TRAIN_PT.exists(), TGT_VAL_PT.exists(), TGT_TEST_PT.exists(), TGT_META_JSON.exists())\n",
    "print(\"Source sequences dir exists?\", SEQ_SRC_DIR.exists())\n",
    "print(\"Source train glob example:\", SRC_SEQ_TRAIN_GLOB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459daefd",
   "metadata": {},
   "source": [
    "Protocol config + deterministic seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45b0023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set: {'python': 20251229, 'numpy': 20251229, 'torch': 20251229}\n",
      "PROTO: {\n",
      "  \"max_prefix_len\": 20,\n",
      "  \"source_vocab_mode\": \"source_local\",\n",
      "  \"source_pair_rule\": \"for t=1..L-1: input=last max_len of items[:t], label=items[t]\",\n",
      "  \"source_long_session_policy\": {\n",
      "    \"enabled\": true,\n",
      "    \"cap_session_len\": 200,\n",
      "    \"cap_strategy\": \"take_last\"\n",
      "  },\n",
      "  \"dataloader\": {\n",
      "    \"batch_size_train\": 256,\n",
      "    \"batch_size_eval\": 512,\n",
      "    \"num_workers\": 0,\n",
      "    \"pin_memory\": false,\n",
      "    \"shuffle_train\": true,\n",
      "    \"drop_last_train\": false\n",
      "  },\n",
      "  \"seeds\": {\n",
      "    \"python\": 20251229,\n",
      "    \"num ...\n",
      "\n",
      "CHECKPOINT 1 ✅ If max_prefix_len != 20 or you want different caps, stop here and adjust.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-02] \n",
    "\n",
    "PROTO = {\n",
    "    \"max_prefix_len\": 20,          # must match your target tensor max_len\n",
    "    \"source_vocab_mode\": \"source_local\",  # keep source vocab separate from target\n",
    "    \"source_pair_rule\": \"for t=1..L-1: input=last max_len of items[:t], label=items[t]\",\n",
    "    \"source_long_session_policy\": {\n",
    "        \"enabled\": True,\n",
    "        \"cap_session_len\": 200,     # cap very long sessions for speed/stability\n",
    "        \"cap_strategy\": \"take_last\"\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"batch_size_train\": 256,\n",
    "        \"batch_size_eval\": 512,\n",
    "        \"num_workers\": 0,           # Windows-safe; increase later if stable\n",
    "        \"pin_memory\": False,\n",
    "        \"shuffle_train\": True,\n",
    "        \"drop_last_train\": False\n",
    "    },\n",
    "    \"seeds\": {\n",
    "        \"python\": 20251229,\n",
    "        \"numpy\": 20251229,\n",
    "        \"torch\": 20251229\n",
    "    }\n",
    "}\n",
    "\n",
    "def set_all_seeds(seed_py: int, seed_np: int, seed_torch: int):\n",
    "    random.seed(seed_py)\n",
    "    np.random.seed(seed_np)\n",
    "    torch.manual_seed(seed_torch)\n",
    "    # If later you use CUDA:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_torch)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(PROTO[\"seeds\"][\"python\"], PROTO[\"seeds\"][\"numpy\"], PROTO[\"seeds\"][\"torch\"])\n",
    "print(\"Seeds set:\", PROTO[\"seeds\"])\n",
    "print(\"PROTO:\", json.dumps(PROTO, indent=2)[:500], \"...\")\n",
    "print(\"\\nCHECKPOINT 1 ✅ If max_prefix_len != 20 or you want different caps, stop here and adjust.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a8cb2",
   "metadata": {},
   "source": [
    "Load target tensors + safe torch.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850841a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target train keys: ['input_ids', 'attn_mask', 'labels', 'session_id', 'user_id', 't', 'split']\n",
      "train input_ids: (1944, 20) labels: (1944,)\n",
      "val   input_ids: (189, 20) labels: (189,)\n",
      "test  input_ids: (200, 20) labels: (200,)\n",
      "\n",
      "Target meta: {'run_tag': '20251229_163357', 'max_len': 20, 'vocab_size': 747, 'pad_id': 0, 'unk_id': 1}\n",
      "\n",
      "CHECKPOINT 2 ✅ Paste shapes + meta snippet if anything looks off.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-03] Load target tensors (safe torch.load for torch>=2.6)\n",
    "\n",
    "from torch.serialization import safe_globals\n",
    "import numpy as _np\n",
    "\n",
    "def torch_load_trusted(path: Path):\n",
    "    # These files are produced by YOU, so safe to load with weights_only=False.\n",
    "    return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "tgt_train = torch_load_trusted(TGT_TRAIN_PT)\n",
    "tgt_val   = torch_load_trusted(TGT_VAL_PT)\n",
    "tgt_test  = torch_load_trusted(TGT_TEST_PT)\n",
    "\n",
    "print(\"Target train keys:\", list(tgt_train.keys()))\n",
    "print(\"train input_ids:\", tuple(tgt_train[\"input_ids\"].shape), \"labels:\", tuple(tgt_train[\"labels\"].shape))\n",
    "print(\"val   input_ids:\", tuple(tgt_val[\"input_ids\"].shape),   \"labels:\", tuple(tgt_val[\"labels\"].shape))\n",
    "print(\"test  input_ids:\", tuple(tgt_test[\"input_ids\"].shape),  \"labels:\", tuple(tgt_test[\"labels\"].shape))\n",
    "\n",
    "with open(TGT_META_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    tgt_meta = json.load(f)\n",
    "\n",
    "print(\"\\nTarget meta:\", {k: tgt_meta.get(k) for k in [\"run_tag\",\"max_len\",\"vocab_size\",\"pad_id\",\"unk_id\"]})\n",
    "PAD_ID_TGT = int(tgt_meta[\"pad_id\"])\n",
    "UNK_ID_TGT = int(tgt_meta[\"unk_id\"])\n",
    "VOCAB_SIZE_TGT = int(tgt_meta[\"vocab_size\"])\n",
    "\n",
    "print(\"\\nCHECKPOINT 2 ✅ Paste shapes + meta snippet if anything looks off.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0962e",
   "metadata": {},
   "source": [
    "Target TensorDataset + loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153a80d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target batch keys: ['input_ids', 'attn_mask', 'labels', 'session_id', 'user_id', 't', 'split']\n",
      "input_ids: torch.Size([256, 20]) attn_mask: torch.Size([256, 20]) labels: torch.Size([256])\n",
      "nonpad lens (first 5): [1, 1, 9, 3, 3]\n",
      "\n",
      "CHECKPOINT 3 ✅ Paste this batch summary if anything looks wrong.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-04] Target TensorDataset + loaders\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TargetTensorDataset(Dataset):\n",
    "    def __init__(self, blob: dict):\n",
    "        self.blob = blob\n",
    "        self.n = int(blob[\"input_ids\"].shape[0])\n",
    "\n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.blob[\"input_ids\"][idx],\n",
    "            \"attn_mask\": self.blob[\"attn_mask\"][idx],\n",
    "            \"labels\": self.blob[\"labels\"][idx],\n",
    "            \"session_id\": self.blob[\"session_id\"][idx],\n",
    "            \"user_id\": self.blob[\"user_id\"][idx],\n",
    "            \"t\": self.blob[\"t\"][idx],\n",
    "            \"split\": self.blob[\"split\"][idx],\n",
    "        }\n",
    "\n",
    "tgt_train_ds = TargetTensorDataset(tgt_train)\n",
    "tgt_val_ds   = TargetTensorDataset(tgt_val)\n",
    "tgt_test_ds  = TargetTensorDataset(tgt_test)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(PROTO[\"seeds\"][\"torch\"])\n",
    "\n",
    "tgt_train_loader = DataLoader(\n",
    "    tgt_train_ds,\n",
    "    batch_size=PROTO[\"dataloader\"][\"batch_size_train\"],\n",
    "    shuffle=PROTO[\"dataloader\"][\"shuffle_train\"],\n",
    "    num_workers=PROTO[\"dataloader\"][\"num_workers\"],\n",
    "    pin_memory=PROTO[\"dataloader\"][\"pin_memory\"],\n",
    "    generator=g\n",
    ")\n",
    "tgt_val_loader = DataLoader(\n",
    "    tgt_val_ds,\n",
    "    batch_size=PROTO[\"dataloader\"][\"batch_size_eval\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "tgt_test_loader = DataLoader(\n",
    "    tgt_test_ds,\n",
    "    batch_size=PROTO[\"dataloader\"][\"batch_size_eval\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "b = next(iter(tgt_train_loader))\n",
    "print(\"Target batch keys:\", list(b.keys()))\n",
    "print(\"input_ids:\", b[\"input_ids\"].shape, \"attn_mask:\", b[\"attn_mask\"].shape, \"labels:\", b[\"labels\"].shape)\n",
    "print(\"nonpad lens (first 5):\", [int(x) for x in b[\"attn_mask\"].sum(dim=1)[:5].tolist()])\n",
    "print(\"\\nCHECKPOINT 3 ✅ Paste this batch summary if anything looks wrong.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcbd5e",
   "metadata": {},
   "source": [
    "Source: build vocab from TRAIN sequences only (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5935e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size: 1620\n",
      "Top items (first 10): [('course-v1:TsinghuaX+30640014+2015_T2', 4902340), ('course-v1:TsinghuaX+80512073X+2016_T1', 2435199), ('course-v1:TsinghuaX+80512073X_2015_2+2015_T2', 2119141), ('course-v1:TsinghuaX+30640014X+2016_T1', 2095212), ('course-v1:TsinghuaX+10610224X+2016_T1', 2055980), ('course-v1:TsinghuaX+30640014X+2016_T2', 1774362), ('course-v1:TsinghuaX+10610204X_2015_2+2015_T2', 1740811), ('course-v1:TsinghuaX+10610183_2X+2016_T2', 1687189), ('course-v1:TsinghuaX+10610183X_2015_T2+2015_T2', 1662289), ('course-v1:MITx+6_00_1x+sp', 1332019)]\n",
      "Saved source vocab: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\source_vocab_items_20251229_232834.json\n",
      "\n",
      "CHECKPOINT 4 ✅ Paste vocab size. If this takes too long, we will add a faster path.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-05] Build SOURCE vocab from TRAIN sequences (only) to avoid leakage\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "MAX_LEN = int(PROTO[\"max_prefix_len\"])\n",
    "CAP_ENABLED = bool(PROTO[\"source_long_session_policy\"][\"enabled\"])\n",
    "CAP_LEN = int(PROTO[\"source_long_session_policy\"][\"cap_session_len\"])\n",
    "CAP_STRAT = PROTO[\"source_long_session_policy\"][\"cap_strategy\"]\n",
    "\n",
    "src_train_path = str(SRC_SEQ_TRAIN_GLOB)\n",
    "dataset_train = ds.dataset(str(SEQ_SRC_DIR / \"train\"), format=\"parquet\")\n",
    "\n",
    "# We read only the 'items' column to build vocab.\n",
    "# items is a list column.\n",
    "item_counts = {}\n",
    "\n",
    "# Read in fragments (safe)\n",
    "for frag in dataset_train.get_fragments():\n",
    "    tbl = frag.to_table(columns=[\"items\"])\n",
    "    col = tbl[\"items\"].to_pylist()  # list of lists\n",
    "    for items in col:\n",
    "        if items is None:\n",
    "            continue\n",
    "        if CAP_ENABLED and len(items) > CAP_LEN and CAP_STRAT == \"take_last\":\n",
    "            items = items[-CAP_LEN:]\n",
    "        for it in items:\n",
    "            item_counts[it] = item_counts.get(it, 0) + 1\n",
    "\n",
    "# Sort by frequency then item_id for determinism\n",
    "sorted_items = sorted(item_counts.items(), key=lambda x: (-x[1], str(x[0])))\n",
    "\n",
    "# IDs: 0 pad, 1 unk, then items\n",
    "src_item2id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for it, _ in sorted_items:\n",
    "    if it not in src_item2id:\n",
    "        src_item2id[str(it)] = len(src_item2id)\n",
    "\n",
    "src_id2item = {v:k for k,v in src_item2id.items()}\n",
    "SRC_PAD_ID = 0\n",
    "SRC_UNK_ID = 1\n",
    "SRC_VOCAB_SIZE = len(src_item2id)\n",
    "\n",
    "print(\"Source vocab size:\", SRC_VOCAB_SIZE)\n",
    "print(\"Top items (first 10):\", list(sorted_items[:10]))\n",
    "\n",
    "OUT_SRC_VOCAB = SEQ_SRC_DIR / f\"source_vocab_items_{RUN_TAG_SOURCE}.json\"\n",
    "OUT_SRC_VOCAB.write_text(json.dumps({\n",
    "    \"run_tag_source\": RUN_TAG_SOURCE,\n",
    "    \"built_from\": \"train only\",\n",
    "    \"vocab_size\": SRC_VOCAB_SIZE,\n",
    "    \"pad_id\": SRC_PAD_ID,\n",
    "    \"unk_id\": SRC_UNK_ID,\n",
    "    \"item2id\": src_item2id\n",
    "}, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved source vocab:\", OUT_SRC_VOCAB.resolve())\n",
    "\n",
    "print(\"\\nCHECKPOINT 4 ✅ Paste vocab size. If this takes too long, we will add a faster path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2720a3b1",
   "metadata": {},
   "source": [
    "Source Dataset: session sequences → supervised pairs ON THE FLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97fdba",
   "metadata": {},
   "source": [
    "This dataset enumerates sessions and picks a t position. To avoid building a 146M index, we do a streaming sampler approach for training and a deterministic fixed subset for eval.\n",
    "\n",
    "We’ll implement:\n",
    "\n",
    "SourceEvalPairsDataset: deterministic pairs from the first N_SESSIONS_EVAL sessions, all t positions (capped)\n",
    "\n",
    "SourceTrainIterable: iterable dataset that samples pairs deterministically with seed (no huge index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fa5ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SourceEvalPairsDataset] sessions=20,000 examples=331,645\n",
      "[SourceEvalPairsDataset] sessions=20,000 examples=327,311\n",
      "\n",
      "CHECKPOINT 5 ✅ Paste src_val_ds examples count + src_test_ds examples count.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-06] Source datasets (train iterable + eval fixed)\n",
    "\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "def encode_prefix(prefix_items, item2id, max_len, pad_id, unk_id):\n",
    "    ids = [item2id.get(str(x), unk_id) for x in prefix_items]\n",
    "    if len(ids) >= max_len:\n",
    "        ids = ids[-max_len:]\n",
    "    attn = [1] * len(ids)\n",
    "    # pad left\n",
    "    pad_n = max_len - len(ids)\n",
    "    if pad_n > 0:\n",
    "        ids = [pad_id]*pad_n + ids\n",
    "        attn = [0]*pad_n + attn\n",
    "    return torch.tensor(ids, dtype=torch.long), torch.tensor(attn, dtype=torch.long)\n",
    "\n",
    "class SourceEvalPairsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Deterministic evaluation dataset:\n",
    "    - reads a limited number of sessions (N_SESSIONS_EVAL)\n",
    "    - emits all prefix->next pairs for each session (with optional cap)\n",
    "    \"\"\"\n",
    "    def __init__(self, split_dir: Path, item2id: dict, max_len: int, pad_id: int, unk_id: int,\n",
    "                 cap_enabled: bool, cap_len: int, cap_strategy: str,\n",
    "                 n_sessions_eval: int = 20000):\n",
    "        self.item2id = item2id\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "        self.unk_id = unk_id\n",
    "        self.cap_enabled = cap_enabled\n",
    "        self.cap_len = cap_len\n",
    "        self.cap_strategy = cap_strategy\n",
    "\n",
    "        dset = ds.dataset(str(split_dir), format=\"parquet\")\n",
    "        sessions = []\n",
    "        for frag in dset.get_fragments():\n",
    "            tbl = frag.to_table(columns=[\"session_id\",\"user_id\",\"items\"])\n",
    "            df = tbl.to_pandas()\n",
    "            for _, r in df.iterrows():\n",
    "                items = r[\"items\"]\n",
    "                if items is None or len(items) < 2:\n",
    "                    continue\n",
    "                if self.cap_enabled and len(items) > self.cap_len and self.cap_strategy == \"take_last\":\n",
    "                    items = items[-self.cap_len:]\n",
    "                sessions.append((str(r[\"session_id\"]), str(r[\"user_id\"]), [str(x) for x in items]))\n",
    "                if len(sessions) >= n_sessions_eval:\n",
    "                    break\n",
    "            if len(sessions) >= n_sessions_eval:\n",
    "                break\n",
    "\n",
    "        # Build flat list of examples (session_idx, t)\n",
    "        ex = []\n",
    "        for si, (_, _, items) in enumerate(sessions):\n",
    "            L = len(items)\n",
    "            for t in range(1, L):  # t=1..L-1\n",
    "                ex.append((si, t))\n",
    "        self.sessions = sessions\n",
    "        self.examples = ex\n",
    "\n",
    "        print(f\"[SourceEvalPairsDataset] sessions={len(self.sessions):,} examples={len(self.examples):,}\")\n",
    "\n",
    "    def __len__(self): return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        si, t = self.examples[idx]\n",
    "        session_id, user_id, items = self.sessions[si]\n",
    "        prefix = items[:t]\n",
    "        label = items[t]\n",
    "        x_ids, attn = encode_prefix(prefix, self.item2id, self.max_len, self.pad_id, self.unk_id)\n",
    "        y = self.item2id.get(str(label), self.unk_id)\n",
    "        return {\n",
    "            \"input_ids\": x_ids,\n",
    "            \"attn_mask\": attn,\n",
    "            \"labels\": torch.tensor(y, dtype=torch.long),\n",
    "            \"session_id\": session_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"t\": torch.tensor(t, dtype=torch.long),\n",
    "            \"split\": \"eval\"\n",
    "        }\n",
    "\n",
    "class SourceTrainIterable(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable training stream:\n",
    "    - reads session rows sequentially\n",
    "    - yields pairs for each session\n",
    "    - deterministic order if shuffle_sessions=False\n",
    "    - optional shuffle using hash(session_id) with seed\n",
    "    \"\"\"\n",
    "    def __init__(self, split_dir: Path, item2id: dict, max_len: int, pad_id: int, unk_id: int,\n",
    "                 cap_enabled: bool, cap_len: int, cap_strategy: str,\n",
    "                 seed: int = 20251229, shuffle_sessions: bool = True, max_pairs_per_session: int = 0):\n",
    "        self.split_dir = split_dir\n",
    "        self.item2id = item2id\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "        self.unk_id = unk_id\n",
    "        self.cap_enabled = cap_enabled\n",
    "        self.cap_len = cap_len\n",
    "        self.cap_strategy = cap_strategy\n",
    "        self.seed = seed\n",
    "        self.shuffle_sessions = shuffle_sessions\n",
    "        self.max_pairs_per_session = int(max_pairs_per_session)\n",
    "\n",
    "    def __iter__(self):\n",
    "        dset = ds.dataset(str(self.split_dir), format=\"parquet\")\n",
    "        frags = list(dset.get_fragments())\n",
    "\n",
    "        # deterministic \"shuffle\": sort by hash(fragment path + seed)\n",
    "        if self.shuffle_sessions:\n",
    "            frags.sort(key=lambda f: hash((str(f.path), self.seed)))\n",
    "\n",
    "        for frag in frags:\n",
    "            tbl = frag.to_table(columns=[\"session_id\",\"user_id\",\"items\"])\n",
    "            df = tbl.to_pandas()\n",
    "\n",
    "            if self.shuffle_sessions:\n",
    "                # deterministic per-fragment shuffle\n",
    "                df[\"_k\"] = df[\"session_id\"].map(lambda s: hash((str(s), self.seed)))\n",
    "                df = df.sort_values(\"_k\").drop(columns=[\"_k\"])\n",
    "\n",
    "            for _, r in df.iterrows():\n",
    "                items = r[\"items\"]\n",
    "                if items is None or len(items) < 2:\n",
    "                    continue\n",
    "                if self.cap_enabled and len(items) > self.cap_len and self.cap_strategy == \"take_last\":\n",
    "                    items = items[-self.cap_len:]\n",
    "                items = [str(x) for x in items]\n",
    "\n",
    "                L = len(items)\n",
    "                t_positions = list(range(1, L))\n",
    "\n",
    "                # Optional cap: sample subset of t positions deterministically\n",
    "                if self.max_pairs_per_session > 0 and len(t_positions) > self.max_pairs_per_session:\n",
    "                    # deterministic pick based on session_id hash\n",
    "                    sid = str(r[\"session_id\"])\n",
    "                    k = abs(hash((sid, self.seed))) % len(t_positions)\n",
    "                    # take a window of size max_pairs_per_session\n",
    "                    t_positions = (t_positions[k:] + t_positions[:k])[:self.max_pairs_per_session]\n",
    "\n",
    "                for t in t_positions:\n",
    "                    prefix = items[:t]\n",
    "                    label = items[t]\n",
    "                    x_ids, attn = encode_prefix(prefix, self.item2id, self.max_len, self.pad_id, self.unk_id)\n",
    "                    y = self.item2id.get(str(label), self.unk_id)\n",
    "                    yield {\n",
    "                        \"input_ids\": x_ids,\n",
    "                        \"attn_mask\": attn,\n",
    "                        \"labels\": torch.tensor(y, dtype=torch.long),\n",
    "                        \"session_id\": str(r[\"session_id\"]),\n",
    "                        \"user_id\": str(r[\"user_id\"]),\n",
    "                        \"t\": torch.tensor(t, dtype=torch.long),\n",
    "                        \"split\": \"train\"\n",
    "                    }\n",
    "\n",
    "# Build datasets\n",
    "src_train_iter = SourceTrainIterable(\n",
    "    split_dir=SEQ_SRC_DIR / \"train\",\n",
    "    item2id=src_item2id,\n",
    "    max_len=MAX_LEN, pad_id=SRC_PAD_ID, unk_id=SRC_UNK_ID,\n",
    "    cap_enabled=CAP_ENABLED, cap_len=CAP_LEN, cap_strategy=CAP_STRAT,\n",
    "    seed=PROTO[\"seeds\"][\"torch\"], shuffle_sessions=True,\n",
    "    max_pairs_per_session=0  # set >0 to cap compute\n",
    ")\n",
    "\n",
    "src_val_ds = SourceEvalPairsDataset(\n",
    "    split_dir=SEQ_SRC_DIR / \"val\",\n",
    "    item2id=src_item2id,\n",
    "    max_len=MAX_LEN, pad_id=SRC_PAD_ID, unk_id=SRC_UNK_ID,\n",
    "    cap_enabled=CAP_ENABLED, cap_len=CAP_LEN, cap_strategy=CAP_STRAT,\n",
    "    n_sessions_eval=20000\n",
    ")\n",
    "\n",
    "src_test_ds = SourceEvalPairsDataset(\n",
    "    split_dir=SEQ_SRC_DIR / \"test\",\n",
    "    item2id=src_item2id,\n",
    "    max_len=MAX_LEN, pad_id=SRC_PAD_ID, unk_id=SRC_UNK_ID,\n",
    "    cap_enabled=CAP_ENABLED, cap_len=CAP_LEN, cap_strategy=CAP_STRAT,\n",
    "    n_sessions_eval=20000\n",
    ")\n",
    "\n",
    "print(\"\\nCHECKPOINT 5 ✅ Paste src_val_ds examples count + src_test_ds examples count.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311969c",
   "metadata": {},
   "source": [
    "Source DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f75d6acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch keys: ['input_ids', 'attn_mask', 'labels', 'session_id', 'user_id', 't', 'split']\n",
      "input_ids: torch.Size([256, 20]) attn_mask: torch.Size([256, 20]) labels: torch.Size([256])\n",
      "nonpad lens (first 5): [1, 2, 3, 4, 5]\n",
      "labels (first 5): [93, 93, 93, 93, 93]\n",
      "\n",
      "CHECKPOINT 6 ✅ Paste this source batch summary if anything looks wrong.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-07] Source loaders\n",
    "\n",
    "def collate_dict(batch):\n",
    "    # batch is list of dicts with tensors already padded\n",
    "    out = {}\n",
    "    keys = batch[0].keys()\n",
    "    for k in keys:\n",
    "        v0 = batch[0][k]\n",
    "        if torch.is_tensor(v0):\n",
    "            out[k] = torch.stack([b[k] for b in batch], dim=0)\n",
    "        else:\n",
    "            out[k] = [b[k] for b in batch]\n",
    "    return out\n",
    "\n",
    "src_train_loader = DataLoader(\n",
    "    src_train_iter,\n",
    "    batch_size=PROTO[\"dataloader\"][\"batch_size_train\"],\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_dict\n",
    ")\n",
    "\n",
    "src_val_loader = DataLoader(\n",
    "    src_val_ds,\n",
    "    batch_size=PROTO[\"dataloader\"][\"batch_size_eval\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_dict\n",
    ")\n",
    "\n",
    "src_test_loader = DataLoader(\n",
    "    src_test_ds,\n",
    "    batch_size=PROTO[\"dataloader\"][\"batch_size_eval\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_dict\n",
    ")\n",
    "\n",
    "b = next(iter(src_train_loader))\n",
    "print(\"Source batch keys:\", list(b.keys()))\n",
    "print(\"input_ids:\", b[\"input_ids\"].shape, \"attn_mask:\", b[\"attn_mask\"].shape, \"labels:\", b[\"labels\"].shape)\n",
    "print(\"nonpad lens (first 5):\", [int(x) for x in b[\"attn_mask\"].sum(dim=1)[:5].tolist()])\n",
    "print(\"labels (first 5):\", b[\"labels\"][:5].tolist())\n",
    "print(\"\\nCHECKPOINT 6 ✅ Paste this source batch summary if anything looks wrong.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce71712",
   "metadata": {},
   "source": [
    "Metrics (HR/MRR/NDCG) — shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bd66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-08] Metrics: HR@K / MRR@K / NDCG@K (batch)\n",
    "\n",
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_metrics_from_scores(scores: torch.Tensor, labels: torch.Tensor, ks=(5,10,20)):\n",
    "    \"\"\"\n",
    "    scores: [B, V] higher is better\n",
    "    labels: [B] int64\n",
    "    \"\"\"\n",
    "    B, V = scores.shape\n",
    "    max_k = max(ks)\n",
    "    topk = torch.topk(scores, k=max_k, dim=1).indices  # [B, max_k]\n",
    "    out = {}\n",
    "    for K in ks:\n",
    "        preds = topk[:, :K]  # [B, K]\n",
    "        hit = (preds == labels.unsqueeze(1)).any(dim=1).float()\n",
    "        out[f\"HR@{K}\"] = float(hit.mean().item())\n",
    "\n",
    "        # rank (1..K) if hit else inf\n",
    "        ranks = torch.full((B,), fill_value=0, dtype=torch.long)\n",
    "        for i in range(B):\n",
    "            m = (preds[i] == labels[i]).nonzero(as_tuple=False)\n",
    "            ranks[i] = (m[0,0] + 1) if len(m) else 0\n",
    "\n",
    "        rr = torch.where(ranks > 0, 1.0 / ranks.float(), torch.zeros_like(ranks, dtype=torch.float))\n",
    "        out[f\"MRR@{K}\"] = float(rr.mean().item())\n",
    "\n",
    "        ndcg = torch.where(ranks > 0, 1.0 / torch.log2(ranks.float() + 1.0), torch.zeros_like(ranks, dtype=torch.float))\n",
    "        out[f\"NDCG@{K}\"] = float(ndcg.mean().item())\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader(model_fn, loader, vocab_size: int, ks=(5,10,20), device=\"cpu\", max_batches=None):\n",
    "    \"\"\"\n",
    "    model_fn: function(batch) -> scores [B, V]\n",
    "    \"\"\"\n",
    "    totals = {f\"HR@{k}\": 0.0 for k in ks}\n",
    "    totals.update({f\"MRR@{k}\": 0.0 for k in ks})\n",
    "    totals.update({f\"NDCG@{k}\": 0.0 for k in ks})\n",
    "\n",
    "    n = 0\n",
    "    bcount = 0\n",
    "    for batch in loader:\n",
    "        bcount += 1\n",
    "        if max_batches and bcount > max_batches:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn_mask = batch[\"attn_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        scores = model_fn(input_ids, attn_mask)  # [B,V]\n",
    "        m = batch_metrics_from_scores(scores, labels, ks=ks)\n",
    "        bs = int(labels.shape[0])\n",
    "        for k,v in m.items():\n",
    "            totals[k] += v * bs\n",
    "        n += bs\n",
    "\n",
    "    for k in totals:\n",
    "        totals[k] = totals[k] / max(n, 1)\n",
    "    return totals, {\"rows\": n, \"batches\": bcount}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978e594",
   "metadata": {},
   "source": [
    "Sanity models: Random + MostPop on TARGET and SOURCE eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4157c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET] Random on VAL: {'HR@5': 0.005291005130857229, 'HR@10': 0.01587301678955555, 'HR@20': 0.026455026119947433, 'MRR@5': 0.0026455025654286146, 'MRR@10': 0.0038947677239775658, 'MRR@20': 0.004629629664123058, 'NDCG@5': 0.0033382526598870754, 'NDCG@10': 0.006600130349397659, 'NDCG@20': 0.009275511838495731} | {'rows': 189, 'batches': 1}\n",
      "[TARGET] MostPop on VAL: {'HR@5': 0.07407407462596893, 'HR@10': 0.13227513432502747, 'HR@20': 0.1746031790971756, 'MRR@5': 0.050264548510313034, 'MRR@10': 0.05870497226715088, 'MRR@20': 0.06130174547433853, 'NDCG@5': 0.05620747059583664, 'NDCG@10': 0.07570020109415054, 'NDCG@20': 0.08597812801599503} | {'rows': 189, 'batches': 1}\n",
      "\n",
      "[SOURCE] Random on VAL: {'HR@5': 0.001953125, 'HR@10': 0.00390625, 'HR@20': 0.005859375, 'MRR@5': 0.001953125, 'MRR@10': 0.0021484375465661287, 'MRR@20': 0.0022879464086145163, 'NDCG@5': 0.001953125, 'NDCG@10': 0.002517704851925373, 'NDCG@20': 0.00301762274466455} | {'rows': 512, 'batches': 2}\n",
      "[SOURCE] MostPop(approx) on VAL: {'HR@5': 0.03515625, 'HR@10': 0.103515625, 'HR@20': 0.234375, 'MRR@5': 0.01155599020421505, 'MRR@10': 0.02073567546904087, 'MRR@20': 0.030035395175218582, 'NDCG@5': 0.017367366701364517, 'NDCG@10': 0.0395485945045948, 'NDCG@20': 0.07295819371938705} | {'rows': 512, 'batches': 2}\n",
      "\n",
      "CHECKPOINT 7 ✅ Paste the four metric dicts (target random/mostpop + source random/mostpop).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-09] Sanity: Random + MostPop\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "ks = (5,10,20)\n",
    "\n",
    "# --- TARGET MostPop: compute popularity from target train tensors ---\n",
    "tgt_train_labels = tgt_train[\"labels\"].numpy().tolist()\n",
    "pop_counts = {}\n",
    "for y in tgt_train_labels:\n",
    "    pop_counts[int(y)] = pop_counts.get(int(y), 0) + 1\n",
    "tgt_pop_rank = [k for k,_ in sorted(pop_counts.items(), key=lambda x: (-x[1], x[0]))]\n",
    "tgt_pop_rank = tgt_pop_rank[:VOCAB_SIZE_TGT]\n",
    "\n",
    "tgt_pop_tensor = torch.tensor(tgt_pop_rank, dtype=torch.long)\n",
    "\n",
    "def random_model_fn(vocab_size):\n",
    "    def fn(input_ids, attn_mask):\n",
    "        B = input_ids.shape[0]\n",
    "        return torch.rand((B, vocab_size), device=input_ids.device)\n",
    "    return fn\n",
    "\n",
    "def mostpop_model_fn(pop_rank_tensor, vocab_size):\n",
    "    # returns scores with higher for most popular\n",
    "    def fn(input_ids, attn_mask):\n",
    "        B = input_ids.shape[0]\n",
    "        scores = torch.zeros((B, vocab_size), device=input_ids.device)\n",
    "        # descending scores for ranked items\n",
    "        # score = -(rank)\n",
    "        scores[:, pop_rank_tensor.to(scores.device)] = torch.linspace(1.0, 0.0, steps=len(pop_rank_tensor), device=scores.device)\n",
    "        return scores\n",
    "    return fn\n",
    "\n",
    "# Target sanity on VAL\n",
    "tgt_random_metrics, tgt_random_aux = eval_loader(random_model_fn(VOCAB_SIZE_TGT), tgt_val_loader, VOCAB_SIZE_TGT, ks=ks, device=DEVICE, max_batches=1)\n",
    "tgt_mostpop_metrics, tgt_mostpop_aux = eval_loader(mostpop_model_fn(tgt_pop_tensor, VOCAB_SIZE_TGT), tgt_val_loader, VOCAB_SIZE_TGT, ks=ks, device=DEVICE, max_batches=1)\n",
    "\n",
    "print(\"[TARGET] Random on VAL:\", tgt_random_metrics, \"|\", tgt_random_aux)\n",
    "print(\"[TARGET] MostPop on VAL:\", tgt_mostpop_metrics, \"|\", tgt_mostpop_aux)\n",
    "\n",
    "# --- SOURCE MostPop: compute popularity from source TRAIN sequences (fast approximate: first N fragments) ---\n",
    "# To keep this notebook responsive, we do an approximate MostPop from first ~N fragments.\n",
    "# Later, in 07/08 we can compute full pop offline if needed.\n",
    "SRC_V = SRC_VOCAB_SIZE\n",
    "\n",
    "src_pop_counts = {}\n",
    "dataset_train = ds.dataset(str(SEQ_SRC_DIR / \"train\"), format=\"parquet\")\n",
    "frag_limit = 50\n",
    "frag_seen = 0\n",
    "for frag in dataset_train.get_fragments():\n",
    "    tbl = frag.to_table(columns=[\"items\"])\n",
    "    for items in tbl[\"items\"].to_pylist():\n",
    "        if items is None:\n",
    "            continue\n",
    "        if CAP_ENABLED and len(items) > CAP_LEN and CAP_STRAT == \"take_last\":\n",
    "            items = items[-CAP_LEN:]\n",
    "        for it in items:\n",
    "            sid = src_item2id.get(str(it), SRC_UNK_ID)\n",
    "            src_pop_counts[sid] = src_pop_counts.get(sid, 0) + 1\n",
    "    frag_seen += 1\n",
    "    if frag_seen >= frag_limit:\n",
    "        break\n",
    "\n",
    "src_pop_rank = [k for k,_ in sorted(src_pop_counts.items(), key=lambda x: (-x[1], x[0]))]\n",
    "# ensure includes at least something\n",
    "if len(src_pop_rank) < 10:\n",
    "    src_pop_rank = list(range(2, min(SRC_V, 1000)))\n",
    "\n",
    "src_pop_tensor = torch.tensor(src_pop_rank[:min(len(src_pop_rank), SRC_V)], dtype=torch.long)\n",
    "\n",
    "src_random_metrics, src_random_aux = eval_loader(random_model_fn(SRC_V), src_val_loader, SRC_V, ks=ks, device=DEVICE, max_batches=1)\n",
    "src_mostpop_metrics, src_mostpop_aux = eval_loader(mostpop_model_fn(src_pop_tensor, SRC_V), src_val_loader, SRC_V, ks=ks, device=DEVICE, max_batches=1)\n",
    "\n",
    "print(\"\\n[SOURCE] Random on VAL:\", src_random_metrics, \"|\", src_random_aux)\n",
    "print(\"[SOURCE] MostPop(approx) on VAL:\", src_mostpop_metrics, \"|\", src_mostpop_aux)\n",
    "\n",
    "print(\"\\nCHECKPOINT 7 ✅ Paste the four metric dicts (target random/mostpop + source random/mostpop).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80601c3c",
   "metadata": {},
   "source": [
    "Save loader protocol config (recreate later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3707fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "\n",
      "Done ✅ 06_data_loader_and_eval_protocol is ready for baselines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-10] Save dataloader protocol config (reproducibility)\n",
    "\n",
    "OUT_CFG = PROC_DIR / \"supervised\" / f\"dataloader_config_{RUN_TAG_TARGET}_{RUN_TAG_SOURCE}.json\"\n",
    "OUT_CFG.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg = {\n",
    "    \"target\": {\n",
    "        \"run_tag\": RUN_TAG_TARGET,\n",
    "        \"tensor_dir\": str(TENSOR_TGT_DIR.resolve()),\n",
    "        \"train_pt\": str(TGT_TRAIN_PT.resolve()),\n",
    "        \"val_pt\": str(TGT_VAL_PT.resolve()),\n",
    "        \"test_pt\": str(TGT_TEST_PT.resolve()),\n",
    "        \"meta_json\": str(TGT_META_JSON.resolve()),\n",
    "    },\n",
    "    \"source\": {\n",
    "        \"run_tag\": RUN_TAG_SOURCE,\n",
    "        \"seq_dir\": str(SEQ_SRC_DIR.resolve()),\n",
    "        \"train_glob\": str(SRC_SEQ_TRAIN_GLOB),\n",
    "        \"val_glob\": str(SRC_SEQ_VAL_GLOB),\n",
    "        \"test_glob\": str(SRC_SEQ_TEST_GLOB),\n",
    "        \"vocab_json\": str(OUT_SRC_VOCAB.resolve()),\n",
    "    },\n",
    "    \"protocol\": PROTO,\n",
    "}\n",
    "\n",
    "OUT_CFG.write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", OUT_CFG.resolve())\n",
    "print(\"\\nDone ✅ 06_data_loader_and_eval_protocol is ready for baselines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a399964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOURCE] Random (20 batches): {'HR@5': 0.00244140625, 'HR@10': 0.00625, 'HR@20': 0.0115234375, 'MRR@5': 0.001062825536064338, 'MRR@10': 0.0015507192772929556, 'MRR@20': 0.00190973978897091, 'NDCG@5': 0.0014018987130839378, 'NDCG@10': 0.0026125961798243225, 'NDCG@20': 0.003937902301549912}\n",
      "[SOURCE] MostPop approx (20 batches): {'HR@5': 0.05693359375, 'HR@10': 0.13525390625, 'HR@20': 0.21279296875, 'MRR@5': 0.027268881801865062, 'MRR@10': 0.037007572944276035, 'MRR@20': 0.04246135508292355, 'NDCG@5': 0.03453330923803151, 'NDCG@10': 0.059153526765294374, 'NDCG@20': 0.07883587318938226}\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-11] Stronger sanity (still fast)\n",
    "\n",
    "ks = (5,10,20)\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "src_random_metrics_20, _ = eval_loader(random_model_fn(SRC_VOCAB_SIZE), src_val_loader, SRC_VOCAB_SIZE, ks=ks, device=DEVICE, max_batches=20)\n",
    "src_mostpop_metrics_20, _ = eval_loader(mostpop_model_fn(src_pop_tensor, SRC_VOCAB_SIZE), src_val_loader, SRC_VOCAB_SIZE, ks=ks, device=DEVICE, max_batches=20)\n",
    "\n",
    "print(\"[SOURCE] Random (20 batches):\", src_random_metrics_20)\n",
    "print(\"[SOURCE] MostPop approx (20 batches):\", src_mostpop_metrics_20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
