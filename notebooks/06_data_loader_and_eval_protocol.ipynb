{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9284ed5b",
   "metadata": {},
   "source": [
    "Bootstrap: locate repo root (Windows-safe) + env info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-00] Bootstrap: locate repo root (Windows-safe) + env info\n",
    "\n",
    "import sys, platform\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"CWD:\", Path.cwd().resolve())\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "TENSOR_DIR = PROC_DIR / \"tensor_target\"\n",
    "print(\"TENSOR_DIR:\", TENSOR_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2fc33",
   "metadata": {},
   "source": [
    "Config: RUN_TAG + load tensor packs + metadata (PyTorch 2.6+ safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd11f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-01] Config: RUN_TAG + load tensor packs + metadata (PyTorch 2.6+ safe)\n",
    "\n",
    "import json\n",
    "\n",
    "RUN_TAG = \"20251229_163357\"\n",
    "\n",
    "TRAIN_PT = TENSOR_DIR / f\"target_tensor_train_{RUN_TAG}.pt\"\n",
    "VAL_PT   = TENSOR_DIR / f\"target_tensor_val_{RUN_TAG}.pt\"\n",
    "TEST_PT  = TENSOR_DIR / f\"target_tensor_test_{RUN_TAG}.pt\"\n",
    "META_JSON = TENSOR_DIR / f\"target_tensor_metadata_{RUN_TAG}.json\"\n",
    "VOCAB_JSON = TENSOR_DIR / f\"target_vocab_items_{RUN_TAG}.json\"\n",
    "\n",
    "for p in [TRAIN_PT, VAL_PT, TEST_PT, META_JSON, VOCAB_JSON]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p.resolve()}\")\n",
    "\n",
    "# PyTorch 2.6+ default weights_only=True breaks dict checkpoints; we must use weights_only=False\n",
    "train_pack = torch.load(TRAIN_PT, map_location=\"cpu\", weights_only=False)\n",
    "val_pack   = torch.load(VAL_PT, map_location=\"cpu\", weights_only=False)\n",
    "test_pack  = torch.load(TEST_PT, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "with open(META_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_blob = json.load(f)\n",
    "\n",
    "vocab = vocab_blob[\"vocab\"]\n",
    "pad_id = int(meta[\"pad_id\"])\n",
    "unk_id = int(meta[\"unk_id\"])\n",
    "max_len = int(meta[\"max_len\"])\n",
    "vocab_size = int(meta[\"vocab_size\"])\n",
    "\n",
    "print(\"RUN_TAG:\", RUN_TAG)\n",
    "print(\"max_len:\", max_len, \"| vocab_size:\", vocab_size, \"| pad_id:\", pad_id, \"| unk_id:\", unk_id)\n",
    "print(\"train keys:\", list(train_pack.keys()))\n",
    "print(\"train input_ids:\", tuple(train_pack[\"input_ids\"].shape), \"labels:\", tuple(train_pack[\"labels\"].shape))\n",
    "print(\"val   input_ids:\", tuple(val_pack[\"input_ids\"].shape),   \"labels:\", tuple(val_pack[\"labels\"].shape))\n",
    "print(\"test  input_ids:\", tuple(test_pack[\"input_ids\"].shape),  \"labels:\", tuple(test_pack[\"labels\"].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f25e0",
   "metadata": {},
   "source": [
    "Define Dataset + DataLoader helpers (CHECKPOINT 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-02] Define Dataset + DataLoader helpers (CHECKPOINT 1)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NextItemTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps our saved dict packs:\n",
    "      input_ids: [N, L]\n",
    "      attn_mask: [N, L]\n",
    "      labels:    [N]\n",
    "    Also keeps debug ids: session_id, user_id, t, split\n",
    "    \"\"\"\n",
    "    def __init__(self, pack: dict):\n",
    "        self.pack = pack\n",
    "        self.N = int(pack[\"input_ids\"].shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        out = {\n",
    "            \"input_ids\": self.pack[\"input_ids\"][idx],\n",
    "            \"attn_mask\": self.pack[\"attn_mask\"][idx],\n",
    "            \"labels\": self.pack[\"labels\"][idx],\n",
    "            # debug fields (numpy arrays)\n",
    "            \"session_id\": self.pack[\"session_id\"][idx],\n",
    "            \"user_id\": self.pack[\"user_id\"][idx],\n",
    "            \"t\": int(self.pack[\"t\"][idx]),\n",
    "            \"split\": self.pack[\"split\"][idx],\n",
    "        }\n",
    "        return out\n",
    "\n",
    "def make_loader(pack: dict, batch_size: int, shuffle: bool):\n",
    "    ds = NextItemTensorDataset(pack)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = make_loader(train_pack, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = make_loader(val_pack,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = make_loader(test_pack,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Quick batch sanity\n",
    "b = next(iter(train_loader))\n",
    "print(\"Batch keys:\", list(b.keys()))\n",
    "print(\"input_ids:\", b[\"input_ids\"].shape, \"attn_mask:\", b[\"attn_mask\"].shape, \"labels:\", b[\"labels\"].shape)\n",
    "print(\"nonpad lens (first 5):\", b[\"attn_mask\"].sum(dim=1)[:5].tolist())\n",
    "print(\"labels (first 5):\", b[\"labels\"][:5].tolist())\n",
    "\n",
    "print(\"\\nCHECKPOINT 1 ✅ Paste this cell output if anything looks off.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e10ba8",
   "metadata": {},
   "source": [
    "Metrics: HR@K, MRR@K, NDCG@K (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-03] Metrics: HR@K, MRR@K, NDCG@K (vectorized)\n",
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics_at_k(scores: torch.Tensor, labels: torch.Tensor, k_list=(5, 10, 20)):\n",
    "    \"\"\"\n",
    "    scores: [B, V] higher is better\n",
    "    labels: [B] item ids in [0..V-1]\n",
    "    returns dict like {\"HR@10\":..., \"MRR@10\":..., \"NDCG@10\":...}\n",
    "    \"\"\"\n",
    "    assert scores.ndim == 2\n",
    "    B, V = scores.shape\n",
    "    assert labels.shape == (B,)\n",
    "\n",
    "    # topk indices for max K\n",
    "    max_k = max(k_list)\n",
    "    topk = torch.topk(scores, k=max_k, dim=1).indices  # [B, max_k]\n",
    "    lab = labels.view(-1, 1)                           # [B,1]\n",
    "    hit_matrix = (topk == lab)                         # [B, max_k] bool\n",
    "\n",
    "    out = {}\n",
    "    for k in k_list:\n",
    "        hits_k = hit_matrix[:, :k].any(dim=1).float()  # [B]\n",
    "        hr = hits_k.mean().item()\n",
    "\n",
    "        # rank position (1-based) if hit, else 0\n",
    "        # find first True index\n",
    "        pos = hit_matrix[:, :k].float().argmax(dim=1) + 1  # [B], but meaningless if no hit\n",
    "        has_hit = hit_matrix[:, :k].any(dim=1)\n",
    "        rr = torch.zeros(B, device=scores.device)\n",
    "        rr[has_hit] = 1.0 / pos[has_hit].float()\n",
    "        mrr = rr.mean().item()\n",
    "\n",
    "        ndcg = torch.zeros(B, device=scores.device)\n",
    "        ndcg[has_hit] = 1.0 / torch.log2(pos[has_hit].float() + 1.0)\n",
    "        ndcg = ndcg.mean().item()\n",
    "\n",
    "        out[f\"HR@{k}\"] = hr\n",
    "        out[f\"MRR@{k}\"] = mrr\n",
    "        out[f\"NDCG@{k}\"] = ndcg\n",
    "\n",
    "    return out\n",
    "\n",
    "print(\"Metrics function ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a792790",
   "metadata": {},
   "source": [
    "Evaluation runner (full-vocab) + padding safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-04] Evaluation runner (full-vocab) + padding safety\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_full_vocab(model_fn, loader, vocab_size: int, k_list=(5,10,20), desc=\"eval\"):\n",
    "    \"\"\"\n",
    "    model_fn(batch) -> scores [B, V] on DEVICE\n",
    "    loader yields CPU tensors\n",
    "    \"\"\"\n",
    "    agg = defaultdict(float)\n",
    "    n_batches = 0\n",
    "    n_rows = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        # move tensors\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attn_mask = batch[\"attn_mask\"].to(DEVICE)\n",
    "        labels    = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        scores = model_fn(input_ids, attn_mask)  # [B,V]\n",
    "        if scores.shape[1] != vocab_size:\n",
    "            raise ValueError(f\"{desc}: scores V={scores.shape[1]} != vocab_size={vocab_size}\")\n",
    "\n",
    "        # Optional: prevent recommending PAD\n",
    "        scores[:, pad_id] = -1e9\n",
    "\n",
    "        m = metrics_at_k(scores, labels, k_list=k_list)\n",
    "        B = int(labels.shape[0])\n",
    "        for key, val in m.items():\n",
    "            agg[key] += val * B\n",
    "\n",
    "        n_rows += B\n",
    "        n_batches += 1\n",
    "\n",
    "    for key in list(agg.keys()):\n",
    "        agg[key] /= max(1, n_rows)\n",
    "\n",
    "    return dict(agg), {\"rows\": n_rows, \"batches\": n_batches}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eca1d4",
   "metadata": {},
   "source": [
    "Protocol sanity check with two dummy baselines\n",
    "(A) Random scores\n",
    "(B) MostPop scores from TRAIN labels (very fast baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-05] Protocol sanity check with two dummy baselines:\n",
    "#  (A) Random scores\n",
    "#  (B) MostPop scores from TRAIN labels (very fast baseline)\n",
    "\n",
    "import torch\n",
    "\n",
    "K_LIST = (5, 10, 20)\n",
    "\n",
    "# (A) Random scorer\n",
    "def random_model_fn(input_ids, attn_mask):\n",
    "    B = input_ids.shape[0]\n",
    "    return torch.rand((B, vocab_size), device=input_ids.device)\n",
    "\n",
    "m_rand, info_rand = eval_full_vocab(random_model_fn, val_loader, vocab_size, k_list=K_LIST, desc=\"random\")\n",
    "print(\"[SANITY] Random on VAL:\", m_rand, \"|\", info_rand)\n",
    "\n",
    "# (B) MostPop from TRAIN labels: score = item_count\n",
    "train_labels = train_pack[\"labels\"].numpy()\n",
    "counts = np.bincount(train_labels, minlength=vocab_size).astype(np.float32)\n",
    "pop_scores = torch.from_numpy(counts).to(DEVICE)  # [V]\n",
    "\n",
    "def mostpop_model_fn(input_ids, attn_mask):\n",
    "    B = input_ids.shape[0]\n",
    "    return pop_scores.unsqueeze(0).repeat(B, 1)\n",
    "\n",
    "m_pop, info_pop = eval_full_vocab(mostpop_model_fn, val_loader, vocab_size, k_list=K_LIST, desc=\"mostpop\")\n",
    "print(\"[SANITY] MostPop on VAL:\", m_pop, \"|\", info_pop)\n",
    "\n",
    "print(\"\\nCHECKPOINT 2 ✅ Paste these two metric dicts (Random and MostPop).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef35c9d7",
   "metadata": {},
   "source": [
    "Plots (matplotlib): non-pad length distribution + label frequency head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 06-06] Plots (matplotlib): non-pad length distribution + label frequency head\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# non-pad lengths (train)\n",
    "lens = train_pack[\"attn_mask\"].sum(dim=1).numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(lens, bins=20)\n",
    "plt.title(\"Non-pad prefix length distribution (train tensors)\")\n",
    "plt.xlabel(\"nonpad_len\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "# label frequency (top 30)\n",
    "topn = 30\n",
    "top_idx = np.argsort(-counts)[:topn]\n",
    "top_vals = counts[top_idx]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([str(i) for i in top_idx], top_vals)\n",
    "plt.title(\"MostPop label frequency (top 30 item_ids)\")\n",
    "plt.xlabel(\"item_id\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e31580",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
