{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b363a20d",
   "metadata": {},
   "source": [
    "Bootstrap (root, run dir, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e00132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-00] Starting... 2026-01-05 01:33:27\n",
      "[12C-00] REPO_ROOT  : C:\\mooc-coldstart-session-meta\n",
      "[12C-00] REPORT_DIR : C:\\mooc-coldstart-session-meta\\reports\\12C_meta_adapt_and_eval_on_target\\20260105_013327\n",
      "[12C-00] DEVICE     : cpu | torch=2.9.1+cpu\n",
      "[12C-00] SEED       : 20260105\n",
      "[12C-00] Done in 0.01s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-00] Bootstrap\n",
    "import os, json, time, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "CELL = \"12C-00\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"meta.json\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = str(find_repo_root(Path.cwd().resolve()))\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "REPORT_DIR = os.path.join(REPO_ROOT, \"reports\", \"12C_meta_adapt_and_eval_on_target\", RUN_TAG)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SEED = 20260105\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"[{CELL}] REPO_ROOT  : {REPO_ROOT}\")\n",
    "print(f\"[{CELL}] REPORT_DIR : {REPORT_DIR}\")\n",
    "print(f\"[{CELL}] DEVICE     : {DEVICE} | torch={torch.__version__}\")\n",
    "print(f\"[{CELL}] SEED       : {SEED}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979688b8",
   "metadata": {},
   "source": [
    "Load latest 12B run (report + ckpt + meta_cfg/proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9672dfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-01] Starting... 2026-01-05 01:33:49\n",
      "[12C-01] 12B run_tag: 20260104_165117\n",
      "[12C-01] 12B report : C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\\report.json\n",
      "[12C-01] 12B ckpt   : C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\\meta_model_source.pt\n",
      "[12C-01] PROTO keys   : ['K_LIST', 'MAX_PREFIX_LEN', 'CAP_ENABLED', 'CAP_SESSION_LEN', 'CAP_STRATEGY']\n",
      "[12C-01] META_CFG keys: ['emb_dim', 'hidden_dim', 'dropout', 'meta_lr', 'inner_lr', 'inner_steps', 'meta_steps', 'meta_batch_tasks', 'grad_clip', 'seed', 'log_every', 'eval_every', 'val_episodes']\n",
      "[12C-01] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-01] Load latest 12B meta-train run from meta.json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-01\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "META_JSON_PATH = os.path.join(REPO_ROOT, \"meta.json\")\n",
    "with open(META_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    META = json.load(f)\n",
    "\n",
    "runs_12b = [r for r in META.get(\"runs\", []) if r.get(\"kind\") == \"12B_meta_train_on_source\"]\n",
    "if not runs_12b:\n",
    "    raise RuntimeError(f\"[{CELL}] No 12B run found in meta.json\")\n",
    "\n",
    "# pick last (created_at sort if present)\n",
    "runs_12b = sorted(runs_12b, key=lambda r: r.get(\"created_at\") or \"\")\n",
    "RUN_12B = runs_12b[-1]\n",
    "\n",
    "RUN_TAG_12B = RUN_12B[\"run_tag\"]\n",
    "REPORT_DIR_12B = RUN_12B[\"report_dir\"]\n",
    "REPORT_JSON_12B = RUN_12B[\"artifacts\"][\"report_json\"]\n",
    "CKPT_META_INIT = RUN_12B[\"artifacts\"][\"meta_model_source_pt\"]\n",
    "\n",
    "print(f\"[{CELL}] 12B run_tag: {RUN_TAG_12B}\")\n",
    "print(f\"[{CELL}] 12B report : {REPORT_JSON_12B}\")\n",
    "print(f\"[{CELL}] 12B ckpt   : {CKPT_META_INIT}\")\n",
    "\n",
    "with open(REPORT_JSON_12B, \"r\", encoding=\"utf-8\") as f:\n",
    "    REPORT_12B = json.load(f)\n",
    "\n",
    "PROTO = REPORT_12B[\"proto\"]\n",
    "META_CFG = REPORT_12B[\"meta_cfg\"]  # your report uses meta_cfg\n",
    "print(f\"[{CELL}] PROTO keys   : {list(PROTO.keys())}\")\n",
    "print(f\"[{CELL}] META_CFG keys: {list(META_CFG.keys())}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c299d6f",
   "metadata": {},
   "source": [
    "Load dataloader_config + target tensor metadata (PAD/VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9c3b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-02] Starting... 2026-01-05 01:34:46\n",
      "[12C-02] DL_CFG_PATH: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[12C-02] train_pt : C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[12C-02] val_pt   : C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[12C-02] test_pt  : C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[12C-02] meta_json: C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_metadata_20251229_163357.json\n",
      "[12C-02] PAD_ID=0 UNK_ID=1 VOCAB_SIZE_TARGET=747\n",
      "[12C-02] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-02] Load tensor target config + target_tensor_metadata_*.json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-02\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "DL_CFG_PATH = os.path.join(REPO_ROOT, \"data\", \"processed\", \"supervised\", \"dataloader_config_20251229_163357_20251229_232834.json\")\n",
    "print(f\"[{CELL}] DL_CFG_PATH: {DL_CFG_PATH}\")\n",
    "\n",
    "with open(DL_CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    DL_CFG = json.load(f)\n",
    "\n",
    "TARGET_CFG = DL_CFG[\"target\"]\n",
    "TRAIN_PT = TARGET_CFG[\"train_pt\"]\n",
    "VAL_PT   = TARGET_CFG[\"val_pt\"]\n",
    "TEST_PT  = TARGET_CFG[\"test_pt\"]\n",
    "TARGET_META_JSON = TARGET_CFG[\"meta_json\"]\n",
    "\n",
    "print(f\"[{CELL}] train_pt : {TRAIN_PT}\")\n",
    "print(f\"[{CELL}] val_pt   : {VAL_PT}\")\n",
    "print(f\"[{CELL}] test_pt  : {TEST_PT}\")\n",
    "print(f\"[{CELL}] meta_json: {TARGET_META_JSON}\")\n",
    "\n",
    "with open(TARGET_META_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    TARGET_META = json.load(f)\n",
    "\n",
    "PAD_ID = int(TARGET_META[\"pad_id\"])\n",
    "UNK_ID = int(TARGET_META[\"unk_id\"])\n",
    "VOCAB_SIZE_TARGET = int(TARGET_META[\"vocab_size\"])\n",
    "\n",
    "print(f\"[{CELL}] PAD_ID={PAD_ID} UNK_ID={UNK_ID} VOCAB_SIZE_TARGET={VOCAB_SIZE_TARGET}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119b82d",
   "metadata": {},
   "source": [
    "Load latest 12B run (report + ckpt + meta_cfg/proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918c9c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-01] Starting... 2026-01-05 01:35:11\n",
      "[12C-01] 12B run_tag: 20260104_165117\n",
      "[12C-01] 12B report : C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\\report.json\n",
      "[12C-01] 12B ckpt   : C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\\meta_model_source.pt\n",
      "[12C-01] PROTO keys   : ['K_LIST', 'MAX_PREFIX_LEN', 'CAP_ENABLED', 'CAP_SESSION_LEN', 'CAP_STRATEGY']\n",
      "[12C-01] META_CFG keys: ['emb_dim', 'hidden_dim', 'dropout', 'meta_lr', 'inner_lr', 'inner_steps', 'meta_steps', 'meta_batch_tasks', 'grad_clip', 'seed', 'log_every', 'eval_every', 'val_episodes']\n",
      "[12C-01] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-01] Load latest 12B meta-train run from meta.json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-01\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "META_JSON_PATH = os.path.join(REPO_ROOT, \"meta.json\")\n",
    "with open(META_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    META = json.load(f)\n",
    "\n",
    "runs_12b = [r for r in META.get(\"runs\", []) if r.get(\"kind\") == \"12B_meta_train_on_source\"]\n",
    "if not runs_12b:\n",
    "    raise RuntimeError(f\"[{CELL}] No 12B run found in meta.json\")\n",
    "\n",
    "# pick last (created_at sort if present)\n",
    "runs_12b = sorted(runs_12b, key=lambda r: r.get(\"created_at\") or \"\")\n",
    "RUN_12B = runs_12b[-1]\n",
    "\n",
    "RUN_TAG_12B = RUN_12B[\"run_tag\"]\n",
    "REPORT_DIR_12B = RUN_12B[\"report_dir\"]\n",
    "REPORT_JSON_12B = RUN_12B[\"artifacts\"][\"report_json\"]\n",
    "CKPT_META_INIT = RUN_12B[\"artifacts\"][\"meta_model_source_pt\"]\n",
    "\n",
    "print(f\"[{CELL}] 12B run_tag: {RUN_TAG_12B}\")\n",
    "print(f\"[{CELL}] 12B report : {REPORT_JSON_12B}\")\n",
    "print(f\"[{CELL}] 12B ckpt   : {CKPT_META_INIT}\")\n",
    "\n",
    "with open(REPORT_JSON_12B, \"r\", encoding=\"utf-8\") as f:\n",
    "    REPORT_12B = json.load(f)\n",
    "\n",
    "PROTO = REPORT_12B[\"proto\"]\n",
    "META_CFG = REPORT_12B[\"meta_cfg\"]  # your report uses meta_cfg\n",
    "print(f\"[{CELL}] PROTO keys   : {list(PROTO.keys())}\")\n",
    "print(f\"[{CELL}] META_CFG keys: {list(META_CFG.keys())}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96f623",
   "metadata": {},
   "source": [
    "Load dataloader_config + target tensor metadata (PAD/VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b66e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-02] Starting... 2026-01-05 01:35:34\n",
      "[12C-02] DL_CFG_PATH: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[12C-02] train_pt : C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[12C-02] val_pt   : C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[12C-02] test_pt  : C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[12C-02] meta_json: C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_metadata_20251229_163357.json\n",
      "[12C-02] PAD_ID=0 UNK_ID=1 VOCAB_SIZE_TARGET=747\n",
      "[12C-02] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-02] Load tensor target config + target_tensor_metadata_*.json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-02\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "DL_CFG_PATH = os.path.join(REPO_ROOT, \"data\", \"processed\", \"supervised\", \"dataloader_config_20251229_163357_20251229_232834.json\")\n",
    "print(f\"[{CELL}] DL_CFG_PATH: {DL_CFG_PATH}\")\n",
    "\n",
    "with open(DL_CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    DL_CFG = json.load(f)\n",
    "\n",
    "TARGET_CFG = DL_CFG[\"target\"]\n",
    "TRAIN_PT = TARGET_CFG[\"train_pt\"]\n",
    "VAL_PT   = TARGET_CFG[\"val_pt\"]\n",
    "TEST_PT  = TARGET_CFG[\"test_pt\"]\n",
    "TARGET_META_JSON = TARGET_CFG[\"meta_json\"]\n",
    "\n",
    "print(f\"[{CELL}] train_pt : {TRAIN_PT}\")\n",
    "print(f\"[{CELL}] val_pt   : {VAL_PT}\")\n",
    "print(f\"[{CELL}] test_pt  : {TEST_PT}\")\n",
    "print(f\"[{CELL}] meta_json: {TARGET_META_JSON}\")\n",
    "\n",
    "with open(TARGET_META_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    TARGET_META = json.load(f)\n",
    "\n",
    "PAD_ID = int(TARGET_META[\"pad_id\"])\n",
    "UNK_ID = int(TARGET_META[\"unk_id\"])\n",
    "VOCAB_SIZE_TARGET = int(TARGET_META[\"vocab_size\"])\n",
    "\n",
    "print(f\"[{CELL}] PAD_ID={PAD_ID} UNK_ID={UNK_ID} VOCAB_SIZE_TARGET={VOCAB_SIZE_TARGET}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef27214",
   "metadata": {},
   "source": [
    "Load .pt tensors (PyTorch 2.6+ weights_only fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f032a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-03] Starting... 2026-01-05 01:36:56\n",
      "[12C-03] TARGET_TRAIN: input_ids=(1944, 20) labels=(1944,)\n",
      "[12C-03] TARGET_TRAIN: lengths min=1 p50=4 max=20\n",
      "[12C-03] TARGET_TRAIN: sample0 len=1 y=423 x_last5=[0, 0, 0, 0, 416]\n",
      "[12C-03] TARGET_VAL: input_ids=(189, 20) labels=(189,)\n",
      "[12C-03] TARGET_VAL: lengths min=1 p50=3 max=20\n",
      "[12C-03] TARGET_VAL: sample0 len=1 y=380 x_last5=[0, 0, 0, 0, 383]\n",
      "[12C-03] TARGET_TEST: input_ids=(200, 20) labels=(200,)\n",
      "[12C-03] TARGET_TEST: lengths min=1 p50=3 max=20\n",
      "[12C-03] TARGET_TEST: sample0 len=1 y=151 x_last5=[0, 0, 0, 0, 150]\n",
      "[12C-03] TENSOR_SEQ_LEN=20\n",
      "[12C-03] max_token_id_seen=746 | VOCAB_SIZE_TARGET=747\n",
      "[12C-03] Done in 0.01s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-03] Load TARGET tensors (trusted) + lengths from attn_mask\n",
    "import time, torch\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-03\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def torch_load_trusted(pt_path: str):\n",
    "    try:\n",
    "        return torch.load(pt_path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        return torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "def make_lengths(attn_mask: torch.Tensor) -> torch.Tensor:\n",
    "    return attn_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "def load_split(pt_path, name):\n",
    "    raw = torch_load_trusted(pt_path)\n",
    "    if not isinstance(raw, dict):\n",
    "        raise RuntimeError(f\"[{CELL}] {name}: expected dict, got {type(raw)}\")\n",
    "    for k in [\"input_ids\", \"attn_mask\", \"labels\"]:\n",
    "        if k not in raw:\n",
    "            raise RuntimeError(f\"[{CELL}] {name}: missing key '{k}' in pt dict. keys={list(raw.keys())}\")\n",
    "\n",
    "    out = {\n",
    "        \"input_ids\": raw[\"input_ids\"].long(),\n",
    "        \"attn_mask\": raw[\"attn_mask\"].long(),\n",
    "        \"labels\": raw[\"labels\"].long(),\n",
    "        # keep metadata if present (optional)\n",
    "        \"session_id\": raw.get(\"session_id\"),\n",
    "        \"user_id\": raw.get(\"user_id\"),\n",
    "        \"t\": raw.get(\"t\"),\n",
    "        \"split\": raw.get(\"split\"),\n",
    "    }\n",
    "    out[\"lengths\"] = make_lengths(out[\"attn_mask\"])\n",
    "\n",
    "    N, L = out[\"input_ids\"].shape\n",
    "    lens = out[\"lengths\"]\n",
    "    print(f\"[{CELL}] {name}: input_ids={tuple(out['input_ids'].shape)} labels={tuple(out['labels'].shape)}\")\n",
    "    print(f\"[{CELL}] {name}: lengths min={int(lens.min())} p50={int(lens.median())} max={int(lens.max())}\")\n",
    "\n",
    "    # left-pad sanity: last token should be non-pad for len>=1\n",
    "    i0 = 0\n",
    "    Li = int(lens[i0])\n",
    "    tail = out[\"input_ids\"][i0].tolist()[-5:]\n",
    "    print(f\"[{CELL}] {name}: sample0 len={Li} y={int(out['labels'][i0])} x_last5={tail}\")\n",
    "    return out\n",
    "\n",
    "TARGET_TRAIN = load_split(TRAIN_PT, \"TARGET_TRAIN\")\n",
    "TARGET_VAL   = load_split(VAL_PT,   \"TARGET_VAL\")\n",
    "TARGET_TEST  = load_split(TEST_PT,  \"TARGET_TEST\")\n",
    "\n",
    "TENSOR_SEQ_LEN = int(TARGET_TRAIN[\"input_ids\"].shape[1])\n",
    "print(f\"[{CELL}] TENSOR_SEQ_LEN={TENSOR_SEQ_LEN}\")\n",
    "\n",
    "# verify vocab bound\n",
    "mx = int(torch.max(torch.stack([\n",
    "    TARGET_TRAIN[\"input_ids\"].max(),\n",
    "    TARGET_VAL[\"input_ids\"].max(),\n",
    "    TARGET_TEST[\"input_ids\"].max(),\n",
    "    TARGET_TRAIN[\"labels\"].max(),\n",
    "    TARGET_VAL[\"labels\"].max(),\n",
    "    TARGET_TEST[\"labels\"].max(),\n",
    "])).item())\n",
    "print(f\"[{CELL}] max_token_id_seen={mx} | VOCAB_SIZE_TARGET={VOCAB_SIZE_TARGET}\")\n",
    "if mx >= VOCAB_SIZE_TARGET:\n",
    "    raise RuntimeError(f\"[{CELL}] VOCAB_SIZE_TARGET too small: max_id={mx} >= {VOCAB_SIZE_TARGET}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571eb7f",
   "metadata": {},
   "source": [
    "Metrics + exclude-seen masking (FIXED for left-padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb71d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-04] Starting... 2026-01-05 01:37:20\n",
      "[12C-04] K_LIST=[5, 10, 20] PAD_ID=0 VOCAB_SIZE_TARGET=747 DEVICE=cpu\n",
      "[12C-04] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-04] Metrics + exclude-seen mask (left-padded => seen are last 'length' tokens)\n",
    "import time, torch\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-04\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "K_LIST = PROTO.get(\"K_LIST\", [5,10,20])\n",
    "\n",
    "def get_batch(split_dict, idxs):\n",
    "    x = split_dict[\"input_ids\"][idxs].to(DEVICE).long()   # [B,L]\n",
    "    l = split_dict[\"lengths\"][idxs].to(DEVICE).long()     # [B]\n",
    "    y = split_dict[\"labels\"][idxs].to(DEVICE).long()      # [B]\n",
    "    return x, l, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def mask_seen_logits_leftpad(logits, input_ids, lengths, pad_id: int):\n",
    "    # left-padded: actual tokens are at the tail => use input_ids[i, -Li:]\n",
    "    masked = logits.clone()\n",
    "    B = input_ids.size(0)\n",
    "    for i in range(B):\n",
    "        Li = int(lengths[i].item())\n",
    "        if Li <= 0:\n",
    "            continue\n",
    "        seen = input_ids[i, -Li:]\n",
    "        seen = seen[seen != int(pad_id)]\n",
    "        if seen.numel() > 0:\n",
    "            masked[i, seen] = float(\"-inf\")\n",
    "    return masked\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics_from_logits(logits, y_true, k_list):\n",
    "    res = {}\n",
    "    max_k = max(k_list)\n",
    "    topk = torch.topk(logits, k=max_k, dim=-1).indices  # [B,max_k]\n",
    "    y = y_true.view(-1,1)\n",
    "\n",
    "    for k in k_list:\n",
    "        match = (topk[:, :k] == y)\n",
    "        hits = match.any(dim=1).float()\n",
    "        hr = hits.mean().item()\n",
    "\n",
    "        rank = torch.where(match.any(dim=1), match.float().argmax(dim=1)+1, torch.zeros_like(hits, dtype=torch.long))\n",
    "        mrr = torch.where(rank>0, 1.0/rank.float(), torch.zeros_like(hits)).mean().item()\n",
    "        ndcg = torch.where(rank>0, 1.0/torch.log2(rank.float()+1.0), torch.zeros_like(hits)).mean().item()\n",
    "\n",
    "        res[f\"HR@{k}\"] = hr\n",
    "        res[f\"MRR@{k}\"] = mrr\n",
    "        res[f\"NDCG@{k}\"] = ndcg\n",
    "    return res\n",
    "\n",
    "print(f\"[{CELL}] K_LIST={K_LIST} PAD_ID={PAD_ID} VOCAB_SIZE_TARGET={VOCAB_SIZE_TARGET} DEVICE={DEVICE}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7f63f",
   "metadata": {},
   "source": [
    "Model definition (copy from 12B unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f8f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-05] ✅ GRU4RecDropout ready\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-05] Model: copied from 12B unchanged\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "class GRU4RecDropout(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, pad_id: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=self.pad_id)\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        # input_ids: [B, T] left-padded. lengths: [B] counts of non-pad\n",
    "        emb = self.drop(self.emb(input_ids))  # [B,T,E]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h = self.gru(packed)  # h: [1,B,H]\n",
    "        logits = self.out(h.squeeze(0))  # [B,V]\n",
    "        return logits\n",
    "\n",
    "print(\"[12C-05] ✅ GRU4RecDropout ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab3e28",
   "metadata": {},
   "source": [
    "Load 12B checkpoint and build TARGET model with partial load (gru only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d9a61dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-06] Starting... 2026-01-05 01:38:01\n",
      "[12C-06] ckpt keys: ['run_tag', 'task_run_dir', 'proto', 'task_cfg', 'meta_cfg', 'vocab_size_source', 'pad_id_source', 'unk_id_source', 'state_dict', 'best_step', 'best_val_hr20']\n",
      "[12C-06] VOCAB_SIZE_SOURCE(from ckpt)=1620 | VOCAB_SIZE_TARGET=747\n",
      "[12C-06] Will load 4 GRU keys. Example keys: ['gru.weight_ih_l0', 'gru.weight_hh_l0', 'gru.bias_ih_l0', 'gru.bias_hh_l0']\n",
      "[12C-06] load_state_dict(gru-only strict=False): missing=3 unexpected=0\n",
      "[12C-06] missing sample: ['emb.weight', 'out.weight', 'out.bias']\n",
      "[12C-06] Copied emb rows for PAD_ID=0 and UNK_ID=1 from source checkpoint\n",
      "[12C-06] ✅ Target model ready (gru transferred, emb/out target-sized).\n",
      "[12C-06] Done in 0.01s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-06] Load 12B ckpt; build target model; load only GRU weights (plus optional PAD/UNK rows)\n",
    "import time, torch\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-06\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "ckpt = torch.load(CKPT_META_INIT, map_location=\"cpu\", weights_only=False)\n",
    "print(f\"[{CELL}] ckpt keys: {list(ckpt.keys())}\")\n",
    "\n",
    "state_dict_src = ckpt[\"state_dict\"]\n",
    "VOCAB_SIZE_SOURCE = int(ckpt.get(\"vocab_size_source\", -1))\n",
    "print(f\"[{CELL}] VOCAB_SIZE_SOURCE(from ckpt)={VOCAB_SIZE_SOURCE} | VOCAB_SIZE_TARGET={VOCAB_SIZE_TARGET}\")\n",
    "\n",
    "# Instantiate TARGET-sized model\n",
    "emb_dim = int(META_CFG[\"emb_dim\"])\n",
    "hidden_dim = int(META_CFG[\"hidden_dim\"])\n",
    "dropout = float(META_CFG[\"dropout\"])\n",
    "\n",
    "model = GRU4RecDropout(\n",
    "    vocab_size=VOCAB_SIZE_TARGET,\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    pad_id=PAD_ID,\n",
    "    dropout=dropout,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Build a filtered state_dict: keep only GRU weights (transferable across vocab)\n",
    "filtered = {k: v for k, v in state_dict_src.items() if k.startswith(\"gru.\")}\n",
    "print(f\"[{CELL}] Will load {len(filtered)} GRU keys. Example keys: {list(filtered.keys())[:6]}\")\n",
    "\n",
    "missing, unexpected = model.load_state_dict(filtered, strict=False)\n",
    "print(f\"[{CELL}] load_state_dict(gru-only strict=False): missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "if missing[:10]:\n",
    "    print(f\"[{CELL}] missing sample: {missing[:10]}\")\n",
    "if unexpected[:10]:\n",
    "    print(f\"[{CELL}] unexpected sample: {unexpected[:10]}\")\n",
    "\n",
    "# Optional: copy PAD/UNK embedding rows if they exist in source (safe and tiny)\n",
    "if \"emb.weight\" in state_dict_src:\n",
    "    with torch.no_grad():\n",
    "        src_emb = state_dict_src[\"emb.weight\"]\n",
    "        if src_emb.size(0) > max(PAD_ID, UNK_ID):\n",
    "            model.emb.weight[PAD_ID].copy_(src_emb[PAD_ID])\n",
    "            model.emb.weight[UNK_ID].copy_(src_emb[UNK_ID])\n",
    "            print(f\"[{CELL}] Copied emb rows for PAD_ID={PAD_ID} and UNK_ID={UNK_ID} from source checkpoint\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"[{CELL}] ✅ Target model ready (gru transferred, emb/out target-sized).\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb569ef1",
   "metadata": {},
   "source": [
    "Evaluate META-INIT on TARGET (VAL/TEST), raw + exclude-seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d28c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-07] Starting... 2026-01-05 01:38:24\n",
      "[12C-07] batch0: x=torch.Size([189, 20]) l=torch.Size([189]) y=torch.Size([189]) logits=torch.Size([189, 747])\n",
      "[12C-07] batch0: x=torch.Size([200, 20]) l=torch.Size([200]) y=torch.Size([200]) logits=torch.Size([200, 747])\n",
      "[12C-07] META-INIT VAL  exclude_seen HR@20=0.05820105969905853 | raw HR@20=0.05820105969905853\n",
      "[12C-07] META-INIT TEST exclude_seen HR@20=0.05000000074505806 | raw HR@20=0.05000000074505806\n",
      "[12C-07] Done in 0.07s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-07] Meta-init eval on TARGET (no adaptation)\n",
    "import time, torch\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-07\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_no_adapt(split_dict, batch_size=256):\n",
    "    model.eval()\n",
    "    N = int(split_dict[\"input_ids\"].shape[0])\n",
    "\n",
    "    logits_raw_list = []\n",
    "    logits_mask_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = min(N, start+batch_size)\n",
    "        idxs = list(range(start, end))\n",
    "        x, l, y = get_batch(split_dict, idxs)\n",
    "\n",
    "        logits = model(x, l)  # [B,V]\n",
    "        if start == 0:\n",
    "            print(f\"[{CELL}] batch0: x={x.shape} l={l.shape} y={y.shape} logits={logits.shape}\")\n",
    "\n",
    "        logits_masked = mask_seen_logits_leftpad(logits, x, l, pad_id=PAD_ID)\n",
    "\n",
    "        logits_raw_list.append(logits.cpu())\n",
    "        logits_mask_list.append(logits_masked.cpu())\n",
    "        y_list.append(y.cpu())\n",
    "\n",
    "    logits_raw = torch.cat(logits_raw_list, dim=0)\n",
    "    logits_masked = torch.cat(logits_mask_list, dim=0)\n",
    "    y_cat = torch.cat(y_list, dim=0)\n",
    "\n",
    "    res_raw = metrics_from_logits(logits_raw, y_cat, K_LIST)\n",
    "    res_mask = metrics_from_logits(logits_masked, y_cat, K_LIST)\n",
    "\n",
    "    return {\n",
    "        \"raw\": {**res_raw, \"_n_examples\": N},\n",
    "        \"exclude_seen\": {**res_mask, \"_n_examples\": N},\n",
    "    }\n",
    "\n",
    "VAL_META_INIT = eval_no_adapt(TARGET_VAL)\n",
    "TEST_META_INIT = eval_no_adapt(TARGET_TEST)\n",
    "\n",
    "print(f\"[{CELL}] META-INIT VAL  exclude_seen HR@20={VAL_META_INIT['exclude_seen'].get('HR@20')} | raw HR@20={VAL_META_INIT['raw'].get('HR@20')}\")\n",
    "print(f\"[{CELL}] META-INIT TEST exclude_seen HR@20={TEST_META_INIT['exclude_seen'].get('HR@20')} | raw HR@20={TEST_META_INIT['raw'].get('HR@20')}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ca26b",
   "metadata": {},
   "source": [
    "Meta-adapt on TARGET (deepcopy + SGD inner loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad256481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-08] Starting... 2026-01-05 01:38:49\n",
      "[12C-08] inner_steps=1 inner_lr=0.01 support=32 query=64 episodes(val/test)=50/200\n",
      "[12C-08]   adapt step0 support_loss=6.610803 B=32\n",
      "[12C-08]   ep=1/50 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.626634 B=32\n",
      "[12C-08]   ep=2/50 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.621930 B=32\n",
      "[12C-08]   ep=3/50 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.600657 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.594532 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.625181 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.605661 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.601851 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.615778 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.590286 B=32\n",
      "[12C-08]   ep=10/50 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.602042 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.598908 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.619802 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.593378 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.618748 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.637110 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.599706 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.579751 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.651757 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609675 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621051 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.637434 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.630651 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.642345 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621953 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.590976 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.622135 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.606484 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.592682 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.649232 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.636955 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.612489 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.615812 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.625339 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.604176 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.645838 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.624340 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.607981 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.574657 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.586583 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.626779 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609664 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.632533 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613068 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621763 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609540 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.627503 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.593835 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.611677 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.612473 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.605182 B=32\n",
      "[12C-08]   ep=1/200 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.612953 B=32\n",
      "[12C-08]   ep=2/200 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.614780 B=32\n",
      "[12C-08]   ep=3/200 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.600945 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.636032 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.637277 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.626453 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609152 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.617922 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.630844 B=32\n",
      "[12C-08]   ep=10/200 query_logits=(64, 747)\n",
      "[12C-08]   adapt step0 support_loss=6.600334 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.608347 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.629787 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.608471 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.603517 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.628126 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616889 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621328 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614518 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613320 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621404 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.630487 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609251 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.635776 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.625201 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.640784 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.627907 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.627268 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.615264 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.584218 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609503 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.639862 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.607064 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620345 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.612599 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.617907 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631039 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.636257 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.623627 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.611839 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609106 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620618 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613055 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.627269 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616149 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.610870 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616651 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631208 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614970 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621279 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.605194 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.638888 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.630740 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616197 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.635510 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.603902 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.630405 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.611044 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613980 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616996 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.607250 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.628118 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.641670 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.593777 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.617557 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.622044 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.626921 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613218 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616534 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.592851 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613853 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.635464 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.610281 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631099 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614743 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.618521 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631072 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.637684 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614237 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.618099 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621708 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.640523 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.601534 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.624691 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.619680 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.589547 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620076 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.622577 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.618697 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.618618 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.629894 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609653 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.648764 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631832 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614879 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613298 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.644382 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.586012 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.602951 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616168 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.628213 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.640308 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.605241 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.588901 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.580233 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.591739 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.588878 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.602917 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.633507 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.629395 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.624711 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609122 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.596962 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.626896 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616206 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.618703 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.635950 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.605230 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.634611 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.647453 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.605314 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.612025 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.615578 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.582063 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.638088 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.639310 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620079 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.588936 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620365 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.591378 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.601422 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620684 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.594731 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609698 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.619919 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.606871 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.596227 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.603712 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631191 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.627165 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.588639 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.622589 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.616603 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.623879 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614780 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.606467 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.625233 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.629005 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.651392 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.640697 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.601145 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.632644 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613445 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.627934 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.623064 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.627367 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621934 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.629752 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.581346 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.654797 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.612734 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631443 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.648087 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.632874 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.582302 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.619154 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631535 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.608616 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.612909 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.611336 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614950 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631320 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.646688 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620896 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.617147 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.624450 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609985 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.632977 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.620997 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.614984 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.639803 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.636497 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.625479 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.631304 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621601 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.604437 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.632872 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.591438 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.609543 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.651258 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.644094 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.632006 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.625873 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.646782 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.612217 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.628065 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.619173 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.621639 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.613597 B=32\n",
      "[12C-08]   adapt step0 support_loss=6.619956 B=32\n",
      "[12C-08] META-ADAPT VAL  exclude_seen HR@20=0.054375000298023224 | raw HR@20=0.054375000298023224\n",
      "[12C-08] META-ADAPT TEST exclude_seen HR@20=0.049531251192092896 | raw HR@20=0.049531251192092896\n",
      "[12C-08] Done in 8.64s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-08] Meta-adapt on TARGET: adapt on TRAIN support, eval on VAL/TEST query\n",
    "import time, copy\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-08\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "INNER_STEPS = int(META_CFG.get(\"inner_steps\", 1))\n",
    "INNER_LR = float(META_CFG.get(\"inner_lr\", 1e-3))\n",
    "SUPPORT_SIZE = int(META_CFG.get(\"n_support\", 32)) if \"n_support\" in META_CFG else 32\n",
    "QUERY_SIZE = int(META_CFG.get(\"n_query\", 64)) if \"n_query\" in META_CFG else 64\n",
    "EPISODES_VAL = int(META_CFG.get(\"val_episodes\", 100))\n",
    "EPISODES_TEST = int(META_CFG.get(\"test_episodes\", 200)) if \"test_episodes\" in META_CFG else 200\n",
    "\n",
    "print(f\"[{CELL}] inner_steps={INNER_STEPS} inner_lr={INNER_LR} support={SUPPORT_SIZE} query={QUERY_SIZE} episodes(val/test)={EPISODES_VAL}/{EPISODES_TEST}\")\n",
    "\n",
    "def sample_indices(N, k, rng):\n",
    "    replace = N < k\n",
    "    return rng.choice(N, size=k, replace=replace)\n",
    "\n",
    "def adapt_on_support(base_model, support_idxs):\n",
    "    m = copy.deepcopy(base_model)\n",
    "    m.train()\n",
    "    opt = torch.optim.SGD(m.parameters(), lr=INNER_LR)\n",
    "\n",
    "    x, l, y = get_batch(TARGET_TRAIN, support_idxs)\n",
    "\n",
    "    for s in range(INNER_STEPS):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.enable_grad():\n",
    "            logits = m(x, l)\n",
    "            loss = F.cross_entropy(logits, y)  # labels are next-item ids (not PAD)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if s == 0:\n",
    "            print(f\"[{CELL}]   adapt step0 support_loss={loss.item():.6f} B={x.size(0)}\")\n",
    "    return m\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_meta_adapt(query_split, n_episodes, seed):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N_train = int(TARGET_TRAIN[\"input_ids\"].shape[0])\n",
    "    N_q = int(query_split[\"input_ids\"].shape[0])\n",
    "\n",
    "    logits_raw_list = []\n",
    "    logits_mask_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        sup = sample_indices(N_train, SUPPORT_SIZE, rng)\n",
    "        qry = sample_indices(N_q, QUERY_SIZE, rng)\n",
    "\n",
    "        adapted = adapt_on_support(model, sup)\n",
    "\n",
    "        xq, lq, yq = get_batch(query_split, qry)\n",
    "        adapted.eval()\n",
    "        logits = adapted(xq, lq)\n",
    "        logits_masked = mask_seen_logits_leftpad(logits, xq, lq, pad_id=PAD_ID)\n",
    "\n",
    "        logits_raw_list.append(logits.cpu())\n",
    "        logits_mask_list.append(logits_masked.cpu())\n",
    "        y_list.append(yq.cpu())\n",
    "\n",
    "        if ep in [0, 1, 2, 9]:\n",
    "            print(f\"[{CELL}]   ep={ep+1}/{n_episodes} query_logits={tuple(logits.shape)}\")\n",
    "\n",
    "    logits_raw = torch.cat(logits_raw_list, dim=0)\n",
    "    logits_masked = torch.cat(logits_mask_list, dim=0)\n",
    "    y_cat = torch.cat(y_list, dim=0)\n",
    "\n",
    "    res_raw = metrics_from_logits(logits_raw, y_cat, K_LIST)\n",
    "    res_mask = metrics_from_logits(logits_masked, y_cat, K_LIST)\n",
    "\n",
    "    return {\n",
    "        \"raw\": {**res_raw, \"_n_episodes\": n_episodes, \"_support\": SUPPORT_SIZE, \"_query\": QUERY_SIZE, \"_inner_steps\": INNER_STEPS, \"_inner_lr\": INNER_LR},\n",
    "        \"exclude_seen\": {**res_mask, \"_n_episodes\": n_episodes, \"_support\": SUPPORT_SIZE, \"_query\": QUERY_SIZE, \"_inner_steps\": INNER_STEPS, \"_inner_lr\": INNER_LR},\n",
    "    }\n",
    "\n",
    "VAL_META_ADAPT = eval_meta_adapt(TARGET_VAL, n_episodes=EPISODES_VAL, seed=SEED+10)\n",
    "TEST_META_ADAPT = eval_meta_adapt(TARGET_TEST, n_episodes=EPISODES_TEST, seed=SEED+20)\n",
    "\n",
    "print(f\"[{CELL}] META-ADAPT VAL  exclude_seen HR@20={VAL_META_ADAPT['exclude_seen'].get('HR@20')} | raw HR@20={VAL_META_ADAPT['raw'].get('HR@20')}\")\n",
    "print(f\"[{CELL}] META-ADAPT TEST exclude_seen HR@20={TEST_META_ADAPT['exclude_seen'].get('HR@20')} | raw HR@20={TEST_META_ADAPT['raw'].get('HR@20')}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b22197e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-08B] Starting... 2026-01-05 01:47:45\n",
      "[12C-08B] VAL META-INIT  exclude_seen HR@20=0.058201\n",
      "[12C-08B] VAL META-ADAPT exclude_seen HR@20=0.054375\n",
      "[12C-08B] VAL ΔHR@20 = -0.003826\n",
      "------------------------------------------------------------\n",
      "[12C-08B] TEST META-INIT  exclude_seen HR@20=0.050000\n",
      "[12C-08B] TEST META-ADAPT exclude_seen HR@20=0.049531\n",
      "[12C-08B] TEST ΔHR@20 = -0.000469\n",
      "------------------------------------------------------------\n",
      "[12C-08B] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-08B] Compact results summary (META-INIT vs META-ADAPT) + deltas\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-08B\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def _delta(a, b, key):\n",
    "    return float(b.get(key, 0.0) - a.get(key, 0.0))\n",
    "\n",
    "for split_name, init_res, adapt_res in [\n",
    "    (\"VAL\",  VAL_META_INIT[\"exclude_seen\"],  VAL_META_ADAPT[\"exclude_seen\"]),\n",
    "    (\"TEST\", TEST_META_INIT[\"exclude_seen\"], TEST_META_ADAPT[\"exclude_seen\"]),\n",
    "]:\n",
    "    print(f\"[{CELL}] {split_name} META-INIT  exclude_seen HR@20={init_res.get('HR@20'):.6f}\")\n",
    "    print(f\"[{CELL}] {split_name} META-ADAPT exclude_seen HR@20={adapt_res.get('HR@20'):.6f}\")\n",
    "    print(f\"[{CELL}] {split_name} ΔHR@20 = {_delta(init_res, adapt_res, 'HR@20'):.6f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8877ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-08C] Starting... 2026-01-05 01:48:07\n",
      "[12C-08C] VAL  audit: {'N_q': 189, 'n_episodes': 50, 'query_size': 64, 'total_query_draws': 3200, 'unique_query_indices': 189, 'unique_coverage_ratio': 1.0, 'sampling_with_replacement': False}\n",
      "[12C-08C] TEST audit: {'N_q': 200, 'n_episodes': 200, 'query_size': 64, 'total_query_draws': 12800, 'unique_query_indices': 200, 'unique_coverage_ratio': 1.0, 'sampling_with_replacement': False}\n",
      "[12C-08C] NOTE: VAL total draws exceed dataset size (expected for episodic eval).\n",
      "[12C-08C] NOTE: TEST total draws exceed dataset size (expected for episodic eval).\n",
      "[12C-08C] Done in 0.02s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-08C] Audit episodic sampling: replacement + unique coverage stats (VAL/TEST)\n",
    "import numpy as np, time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-08C\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def audit_sampling(N_q, n_episodes, query_size, seed):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    all_q = []\n",
    "    for _ in range(n_episodes):\n",
    "        replace = N_q < query_size\n",
    "        q = rng.choice(N_q, size=query_size, replace=replace)\n",
    "        all_q.append(q)\n",
    "    all_q = np.concatenate(all_q, axis=0)\n",
    "\n",
    "    unique = np.unique(all_q)\n",
    "    return {\n",
    "        \"N_q\": int(N_q),\n",
    "        \"n_episodes\": int(n_episodes),\n",
    "        \"query_size\": int(query_size),\n",
    "        \"total_query_draws\": int(n_episodes * query_size),\n",
    "        \"unique_query_indices\": int(unique.size),\n",
    "        \"unique_coverage_ratio\": float(unique.size / max(1, N_q)),\n",
    "        \"sampling_with_replacement\": bool(N_q < query_size),\n",
    "    }\n",
    "\n",
    "N_val = int(TARGET_VAL[\"input_ids\"].shape[0])\n",
    "N_test = int(TARGET_TEST[\"input_ids\"].shape[0])\n",
    "\n",
    "audit_val  = audit_sampling(N_val,  EPISODES_VAL,  QUERY_SIZE, seed=SEED+10)\n",
    "audit_test = audit_sampling(N_test, EPISODES_TEST, QUERY_SIZE, seed=SEED+20)\n",
    "\n",
    "print(f\"[{CELL}] VAL  audit: {audit_val}\")\n",
    "print(f\"[{CELL}] TEST audit: {audit_test}\")\n",
    "\n",
    "if audit_val[\"total_query_draws\"] > audit_val[\"N_q\"]:\n",
    "    print(f\"[{CELL}] NOTE: VAL total draws exceed dataset size (expected for episodic eval).\")\n",
    "if audit_test[\"total_query_draws\"] > audit_test[\"N_q\"]:\n",
    "    print(f\"[{CELL}] NOTE: TEST total draws exceed dataset size (expected for episodic eval).\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4020ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-08D] Starting... 2026-01-05 01:48:45\n",
      "[12C-08D] EXHAUSTIVE VAL  exclude_seen HR@20=0.058201\n",
      "[12C-08D] EXHAUSTIVE TEST exclude_seen HR@20=0.050000\n",
      "[12C-08D] Done in 0.07s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-08D] Exhaustive evaluation: single pass over full VAL/TEST (no resampling)\n",
    "import time, torch\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-08D\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_exhaustive(split_dict, batch_size=256):\n",
    "    model.eval()\n",
    "    N = int(split_dict[\"input_ids\"].shape[0])\n",
    "\n",
    "    logits_raw_list = []\n",
    "    logits_mask_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = min(N, start+batch_size)\n",
    "        idxs = list(range(start, end))\n",
    "        x, l, y = get_batch(split_dict, idxs)\n",
    "\n",
    "        logits = model(x, l)\n",
    "        logits_masked = mask_seen_logits_leftpad(logits, x, l, pad_id=PAD_ID)\n",
    "\n",
    "        logits_raw_list.append(logits.cpu())\n",
    "        logits_mask_list.append(logits_masked.cpu())\n",
    "        y_list.append(y.cpu())\n",
    "\n",
    "    logits_raw = torch.cat(logits_raw_list, dim=0)\n",
    "    logits_masked = torch.cat(logits_mask_list, dim=0)\n",
    "    y_cat = torch.cat(y_list, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"raw\": {**metrics_from_logits(logits_raw, y_cat, K_LIST), \"_n_examples\": N},\n",
    "        \"exclude_seen\": {**metrics_from_logits(logits_masked, y_cat, K_LIST), \"_n_examples\": N},\n",
    "    }\n",
    "\n",
    "EXH_VAL  = eval_exhaustive(TARGET_VAL)\n",
    "EXH_TEST = eval_exhaustive(TARGET_TEST)\n",
    "\n",
    "print(f\"[{CELL}] EXHAUSTIVE VAL  exclude_seen HR@20={EXH_VAL['exclude_seen'].get('HR@20'):.6f}\")\n",
    "print(f\"[{CELL}] EXHAUSTIVE TEST exclude_seen HR@20={EXH_TEST['exclude_seen'].get('HR@20'):.6f}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252d368",
   "metadata": {},
   "source": [
    "Save report + manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abda7bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-09] Starting... 2026-01-05 01:39:35\n",
      "[12C-09] ✅ report.json   : C:\\mooc-coldstart-session-meta\\reports\\12C_meta_adapt_and_eval_on_target\\20260105_013327\\report.json\n",
      "[12C-09] ✅ manifest.json : C:\\mooc-coldstart-session-meta\\reports\\12C_meta_adapt_and_eval_on_target\\20260105_013327\\manifest.json\n",
      "[12C-09] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-09] Save report.json + manifest.json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-09\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "report = {\n",
    "    \"kind\": \"12C_meta_adapt_and_eval_on_target\",\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"source_12b_run_tag\": RUN_TAG_12B,\n",
    "    \"proto\": PROTO,\n",
    "    \"meta_cfg_from_12b\": META_CFG,\n",
    "    \"target_meta_json\": TARGET_META_JSON,\n",
    "    \"target_pts\": {\"train_pt\": TRAIN_PT, \"val_pt\": VAL_PT, \"test_pt\": TEST_PT},\n",
    "    \"transfer_loading\": {\n",
    "        \"note\": \"Loaded GRU weights only (vocab mismatch between source and target). emb/out are target-sized init; copied PAD/UNK emb rows if possible.\"\n",
    "    },\n",
    "    \"target_eval\": {\n",
    "        \"meta_init\": {\"val\": VAL_META_INIT, \"test\": TEST_META_INIT},\n",
    "        \"meta_adapt\": {\"val\": VAL_META_ADAPT, \"test\": TEST_META_ADAPT},\n",
    "    },\n",
    "}\n",
    "\n",
    "report_path = os.path.join(REPORT_DIR, \"report.json\")\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "manifest = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"report_dir\": REPORT_DIR,\n",
    "    \"files\": {\"report.json\": report_path},\n",
    "    \"sizes_bytes\": {\"report.json\": os.path.getsize(report_path)},\n",
    "    \"updated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "manifest_path = os.path.join(REPORT_DIR, \"manifest.json\")\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"[{CELL}] ✅ report.json   : {report_path}\")\n",
    "print(f\"[{CELL}] ✅ manifest.json : {manifest_path}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb684c",
   "metadata": {},
   "source": [
    "Update root meta.json (idempotent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac152d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-10] Starting... 2026-01-05 01:40:01\n",
      "[12C-10] ✅ Added 12C entry run_tag=20260105_013327\n",
      "[12C-10] ✅ Saved meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "[12C-10] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-10] Update meta.json with 12C run entry\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-10\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "meta_path = os.path.join(REPO_ROOT, \"meta.json\")\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "entry = {\n",
    "    \"kind\": \"12C_meta_adapt_and_eval_on_target\",\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"inputs\": {\n",
    "        \"source_12b_run_tag\": RUN_TAG_12B,\n",
    "        \"ckpt_meta_init\": CKPT_META_INIT,\n",
    "        \"dataloader_config\": DL_CFG_PATH,\n",
    "        \"target_meta_json\": TARGET_META_JSON,\n",
    "    },\n",
    "    \"report_dir\": REPORT_DIR,\n",
    "    \"artifacts\": {\n",
    "        \"report_json\": os.path.join(REPORT_DIR, \"report.json\"),\n",
    "        \"manifest_json\": os.path.join(REPORT_DIR, \"manifest.json\"),\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"target_val_meta_init\": VAL_META_INIT,\n",
    "        \"target_test_meta_init\": TEST_META_INIT,\n",
    "        \"target_val_meta_adapt\": VAL_META_ADAPT,\n",
    "        \"target_test_meta_adapt\": TEST_META_ADAPT,\n",
    "    },\n",
    "    \"updated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "\n",
    "meta.setdefault(\"runs\", [])\n",
    "idx = None\n",
    "for i, r in enumerate(meta[\"runs\"]):\n",
    "    if r.get(\"kind\") == entry[\"kind\"] and r.get(\"run_tag\") == entry[\"run_tag\"]:\n",
    "        idx = i\n",
    "        break\n",
    "\n",
    "if idx is None:\n",
    "    meta[\"runs\"].append(entry)\n",
    "    print(f\"[{CELL}] ✅ Added 12C entry run_tag={RUN_TAG}\")\n",
    "else:\n",
    "    meta[\"runs\"][idx] = entry\n",
    "    print(f\"[{CELL}] ✅ Updated 12C entry run_tag={RUN_TAG}\")\n",
    "\n",
    "meta[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"[{CELL}] ✅ Saved meta.json: {meta_path}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0bef2",
   "metadata": {},
   "source": [
    "Sanity: confirm exclude-seen masking changes logits for at least one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97982394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-11] Starting... 2026-01-05 01:41:43\n",
      "[12C-11] num_changed_elements=176 (should be > 0)\n",
      "[12C-11] num_-inf_elements=176 (should be > 0 if any seen tokens exist)\n",
      "[12C-11] Done in 0.02s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-11] Sanity: confirm exclude-seen masking changes logits for at least one batch\n",
    "import torch, time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-11\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "x, l, y = get_batch(TARGET_VAL, list(range(min(32, TARGET_VAL[\"input_ids\"].shape[0]))))\n",
    "with torch.no_grad():\n",
    "    logits = model(x, l)\n",
    "    logits_m = mask_seen_logits_leftpad(logits, x, l, pad_id=PAD_ID)\n",
    "\n",
    "diff = (logits - logits_m).abs()\n",
    "num_changed = int((diff > 0).sum().item())\n",
    "print(f\"[{CELL}] num_changed_elements={num_changed} (should be > 0)\")\n",
    "\n",
    "# check how many positions were set to -inf\n",
    "num_infs = int(torch.isinf(logits_m).sum().item())\n",
    "print(f\"[{CELL}] num_-inf_elements={num_infs} (should be > 0 if any seen tokens exist)\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9264a6",
   "metadata": {},
   "source": [
    "Config: head-only adaptation settings (freeze GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "370df4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-12] Starting... 2026-01-05 01:53:58\n",
      "[12C-12] ADAPT_SCOPE=out+emb\n",
      "[12C-12] inner_steps=3 inner_lr=0.05 support=64 query=64 episodes(val/test)=100/200\n",
      "[12C-12] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-12] Config: head-only adaptation settings (freeze GRU)\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-12\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Keep your previous episode settings for apples-to-apples comparison\n",
    "INNER_STEPS_H = 3          # try 3 steps (more signal than 1)\n",
    "INNER_LR_H = 0.05          # head can take higher LR\n",
    "SUPPORT_SIZE_H = 64        # more support since target is small\n",
    "QUERY_SIZE_H = 64\n",
    "EPISODES_VAL_H = 100\n",
    "EPISODES_TEST_H = 200\n",
    "\n",
    "# Choose adaptation scope:\n",
    "ADAPT_SCOPE = \"out+emb\"        # \"out\" or \"out+emb\"\n",
    "\n",
    "print(f\"[{CELL}] ADAPT_SCOPE={ADAPT_SCOPE}\")\n",
    "print(f\"[{CELL}] inner_steps={INNER_STEPS_H} inner_lr={INNER_LR_H} support={SUPPORT_SIZE_H} query={QUERY_SIZE_H} episodes(val/test)={EPISODES_VAL_H}/{EPISODES_TEST_H}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aebd0e",
   "metadata": {},
   "source": [
    "Meta-adapt eval (HEAD-ONLY): freeze GRU, adapt out (or out+emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3760607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-13] Starting... 2026-01-05 01:54:04\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.624877\n",
      "[12C-13]   ep=1/100 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.634795\n",
      "[12C-13]   ep=2/100 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.628953\n",
      "[12C-13]   ep=3/100 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.602334\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.652944\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.638491\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615194\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612211\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613170\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619499\n",
      "[12C-13]   ep=10/100 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618895\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.604784\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615229\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616261\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.632188\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.614970\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.592694\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609834\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625379\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619841\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.588287\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.604740\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619538\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626599\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617179\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.614746\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619980\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613686\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607014\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.629328\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613526\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621594\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615575\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620877\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.604493\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609644\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.601622\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620440\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618728\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620358\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.628028\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617638\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612869\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620683\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609121\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613217\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626221\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.601526\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625748\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.586567\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.624968\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626117\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616325\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.629005\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.600621\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613708\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611099\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.624024\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617454\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.610058\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.624751\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608184\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611361\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608373\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609108\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608891\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.605074\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.644616\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.597340\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612560\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.601332\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.614311\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.624813\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611310\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616524\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623873\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616870\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.591982\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625917\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607504\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620401\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621998\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620090\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626268\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.633864\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608933\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616612\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613350\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618743\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608132\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.610143\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.600396\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.624743\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626925\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623862\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.639865\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623439\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612272\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619027\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.622695\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.599942\n",
      "[12C-13]   ep=1/200 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.628880\n",
      "[12C-13]   ep=2/200 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621850\n",
      "[12C-13]   ep=3/200 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621521\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.642204\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621909\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611540\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.630371\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.598992\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.604401\n",
      "[12C-13]   ep=10/200 query_logits=(64, 747)\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609296\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607070\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623735\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617984\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623400\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615677\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607910\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.604554\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608106\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615338\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620163\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.603095\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623063\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.637653\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.606385\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615093\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.606055\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615470\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616998\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.586283\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.628616\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.629414\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.597755\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.598286\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626031\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621213\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616970\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.601107\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611630\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.622005\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626520\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618948\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613043\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626617\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609265\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611190\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.601845\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.602415\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617419\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.599504\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620387\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626739\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.605606\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615772\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618094\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613690\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619996\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620311\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618455\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609153\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607742\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.606464\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.614254\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.648106\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.641738\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.638519\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.628170\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621145\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617311\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613979\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.634830\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.641333\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617854\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616349\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.629444\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.598935\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611386\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626841\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.627021\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.606240\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.635448\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618337\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.639019\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608911\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612716\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616477\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619426\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.587468\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.610637\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607088\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621224\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.630922\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621848\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609467\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.622385\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.629022\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625375\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623966\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625816\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.590163\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616241\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613884\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607193\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626255\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.628930\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609375\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617110\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607868\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621627\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616922\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.602148\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621948\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.598125\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619123\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621165\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613965\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619759\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625229\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620224\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618763\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.608922\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616104\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613831\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.606334\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621554\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.596760\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611790\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617861\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616038\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.610879\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.605285\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625001\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612649\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612963\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.599228\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.627919\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.610357\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611861\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.600075\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.629432\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.622949\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.584816\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620930\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.630849\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.609132\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620710\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.617022\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.627582\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612115\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619735\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607173\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.615719\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.611962\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.628234\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.613291\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612914\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.607320\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621250\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612453\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626228\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626487\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620943\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.621236\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.612700\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.600021\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625587\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623946\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618831\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625411\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619178\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.601993\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.624174\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626519\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.603049\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.616108\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.622599\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620776\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.593006\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.610207\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.604695\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.603809\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620100\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.618028\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.620117\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.643511\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.634123\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.626403\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.638029\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.622615\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.627582\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.598055\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.594479\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.625143\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.633358\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.604873\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.619951\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.623209\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.638494\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.601961\n",
      "[12C-13] clone_for_adapt: trainable=96,363/218,667 (out+emb)\n",
      "[12C-13]   adapt step0 support_loss=6.629771\n",
      "[12C-13] HEAD-ADAPT VAL  exclude_seen HR@20=0.06593749672174454 | raw HR@20=0.06578125059604645\n",
      "[12C-13] HEAD-ADAPT TEST exclude_seen HR@20=0.054218750447034836 | raw HR@20=0.054218750447034836\n",
      "[12C-13] Done in 10.19s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-13] Meta-adapt eval (HEAD-ONLY): freeze GRU, adapt out (or out+emb)\n",
    "import time, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-13\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def sample_indices(N, k, rng):\n",
    "    replace = N < k\n",
    "    return rng.choice(N, size=k, replace=replace)\n",
    "\n",
    "def clone_for_adapt(base_model):\n",
    "    # deepcopy keeps this simple & reproducible (slow but OK for small target)\n",
    "    m = copy.deepcopy(base_model)\n",
    "\n",
    "    # Freeze GRU always\n",
    "    for p in m.gru.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Set scope\n",
    "    if ADAPT_SCOPE == \"out+emb\":\n",
    "        for p in m.emb.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in m.out.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        params = list(m.out.parameters())\n",
    "\n",
    "    elif ADAPT_SCOPE == \"out+emb\":\n",
    "        for p in m.emb.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in m.out.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        params = list(m.emb.parameters()) + list(m.out.parameters())\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(f\"[{CELL}] Unknown ADAPT_SCOPE={ADAPT_SCOPE}\")\n",
    "\n",
    "    # Debug: count trainable params\n",
    "    n_trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    n_total = sum(p.numel() for p in m.parameters())\n",
    "    print(f\"[{CELL}] clone_for_adapt: trainable={n_trainable:,}/{n_total:,} ({ADAPT_SCOPE})\")\n",
    "\n",
    "    return m, params\n",
    "\n",
    "def adapt_head_on_support(base_model, support_idxs):\n",
    "    m, params = clone_for_adapt(base_model)\n",
    "    m.train()\n",
    "\n",
    "    opt = torch.optim.SGD(params, lr=INNER_LR_H)\n",
    "\n",
    "    x, l, y = get_batch(TARGET_TRAIN, support_idxs)\n",
    "    for s in range(INNER_STEPS_H):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.enable_grad():\n",
    "            logits = m(x, l)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if s == 0:\n",
    "            print(f\"[{CELL}]   adapt step0 support_loss={loss.item():.6f}\")\n",
    "    return m\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_meta_adapt_head(query_split, n_episodes, seed):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N_train = int(TARGET_TRAIN[\"input_ids\"].shape[0])\n",
    "    N_q = int(query_split[\"input_ids\"].shape[0])\n",
    "\n",
    "    logits_raw_list = []\n",
    "    logits_mask_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        sup = sample_indices(N_train, SUPPORT_SIZE_H, rng)\n",
    "        qry = sample_indices(N_q, QUERY_SIZE_H, rng)\n",
    "\n",
    "        adapted = adapt_head_on_support(model, sup)\n",
    "\n",
    "        xq, lq, yq = get_batch(query_split, qry)\n",
    "        adapted.eval()\n",
    "        logits = adapted(xq, lq)\n",
    "        logits_masked = mask_seen_logits_leftpad(logits, xq, lq, pad_id=PAD_ID)\n",
    "\n",
    "        logits_raw_list.append(logits.cpu())\n",
    "        logits_mask_list.append(logits_masked.cpu())\n",
    "        y_list.append(yq.cpu())\n",
    "\n",
    "        if ep in [0, 1, 2, 9]:\n",
    "            print(f\"[{CELL}]   ep={ep+1}/{n_episodes} query_logits={tuple(logits.shape)}\")\n",
    "\n",
    "    logits_raw = torch.cat(logits_raw_list, dim=0)\n",
    "    logits_masked = torch.cat(logits_mask_list, dim=0)\n",
    "    y_cat = torch.cat(y_list, dim=0)\n",
    "\n",
    "    res_raw = metrics_from_logits(logits_raw, y_cat, K_LIST)\n",
    "    res_mask = metrics_from_logits(logits_masked, y_cat, K_LIST)\n",
    "\n",
    "    return {\n",
    "        \"raw\": {**res_raw, \"_n_episodes\": n_episodes, \"_support\": SUPPORT_SIZE_H, \"_query\": QUERY_SIZE_H, \"_inner_steps\": INNER_STEPS_H, \"_inner_lr\": INNER_LR_H, \"_scope\": ADAPT_SCOPE},\n",
    "        \"exclude_seen\": {**res_mask, \"_n_episodes\": n_episodes, \"_support\": SUPPORT_SIZE_H, \"_query\": QUERY_SIZE_H, \"_inner_steps\": INNER_STEPS_H, \"_inner_lr\": INNER_LR_H, \"_scope\": ADAPT_SCOPE},\n",
    "    }\n",
    "\n",
    "VAL_META_ADAPT_HEAD = eval_meta_adapt_head(TARGET_VAL, n_episodes=EPISODES_VAL_H, seed=SEED+110)\n",
    "TEST_META_ADAPT_HEAD = eval_meta_adapt_head(TARGET_TEST, n_episodes=EPISODES_TEST_H, seed=SEED+120)\n",
    "\n",
    "print(f\"[{CELL}] HEAD-ADAPT VAL  exclude_seen HR@20={VAL_META_ADAPT_HEAD['exclude_seen'].get('HR@20')} | raw HR@20={VAL_META_ADAPT_HEAD['raw'].get('HR@20')}\")\n",
    "print(f\"[{CELL}] HEAD-ADAPT TEST exclude_seen HR@20={TEST_META_ADAPT_HEAD['exclude_seen'].get('HR@20')} | raw HR@20={TEST_META_ADAPT_HEAD['raw'].get('HR@20')}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09798d7",
   "metadata": {},
   "source": [
    "Save head-adapt addendum (does not overwrite prior report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10a37fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12C-14] Starting... 2026-01-05 01:54:39\n",
      "[12C-14] ✅ Wrote addendum: C:\\mooc-coldstart-session-meta\\reports\\12C_meta_adapt_and_eval_on_target\\20260105_013327\\head_adapt_addendum_out_emb.json\n",
      "[12C-14] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12C-14] Save head-adapt addendum (does not overwrite prior report)\n",
    "import os, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"12C-14\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "addendum = {\n",
    "    \"kind\": \"12C_head_only_meta_adapt_addendum\",\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"base_run_tag\": RUN_TAG,\n",
    "    \"scope\": ADAPT_SCOPE,\n",
    "    \"cfg\": {\n",
    "        \"inner_steps\": INNER_STEPS_H,\n",
    "        \"inner_lr\": INNER_LR_H,\n",
    "        \"support\": SUPPORT_SIZE_H,\n",
    "        \"query\": QUERY_SIZE_H,\n",
    "        \"episodes_val\": EPISODES_VAL_H,\n",
    "        \"episodes_test\": EPISODES_TEST_H,\n",
    "        \"seed_base\": SEED,\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"val\": VAL_META_ADAPT_HEAD,\n",
    "        \"test\": TEST_META_ADAPT_HEAD,\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"GRU frozen. Adapted head only (out) or head+emb depending on scope.\",\n",
    "        \"This is a targeted test because source->target vocab mismatch prevents loading emb/out from source.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "path = os.path.join(REPORT_DIR, f\"head_adapt_addendum_{ADAPT_SCOPE.replace('+','_')}.json\")\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(addendum, f, indent=2)\n",
    "\n",
    "print(f\"[{CELL}] ✅ Wrote addendum: {path}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
