{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598b07ce",
   "metadata": {},
   "source": [
    "Bootstrap: locate repo root (Windows-safe) + env info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2cf0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\mooc-coldstart-session-meta\\notebooks\n",
      "REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "DATA_DIR: C:\\mooc-coldstart-session-meta\\data\n",
      "PROC_DIR: C:\\mooc-coldstart-session-meta\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05C-00] Bootstrap (Windows-safe) + locate repo root\n",
    "\n",
    "import os, sys, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "CWD = Path.cwd().resolve()\n",
    "print(\"CWD:\", CWD)\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists() or (p / \".git\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find repo root (PROJECT_STATE.md or .git)\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(CWD)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"PROC_DIR:\", PROC_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c85e1f",
   "metadata": {},
   "source": [
    "Config: input sessionized source + outputs + split params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99aef2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_SRC_SESS: C:\\mooc-coldstart-session-meta\\data\\processed\\sessionized\\source_events_sessionized_20251229_232834.parquet\n",
      "OUT_SEQ_DIR: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05C-01] Config: run tag + input sessionized parquet + output session sequences dataset\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "RUN_TAG = \"20251229_232834\"  # <-- your source sessionization tag\n",
    "\n",
    "# Input: sessionized source events (from 05_sessionize_and_prefix_target.ipynb / your sessionize source step)\n",
    "IN_SRC_SESS = (REPO_ROOT / \"data\" / \"processed\" / \"sessionized\" / f\"source_events_sessionized_{RUN_TAG}.parquet\")\n",
    "if not IN_SRC_SESS.exists():\n",
    "    alt = (REPO_ROOT / \"data\" / \"sessionized\" / f\"source_events_sessionized_{RUN_TAG}.parquet\")\n",
    "    if alt.exists():\n",
    "        IN_SRC_SESS = alt\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing: {IN_SRC_SESS.resolve()} (and fallback: {alt.resolve()})\")\n",
    "\n",
    "# Output: session sequences (one row per session)\n",
    "OUT_SEQ_DIR = (REPO_ROOT / \"data\" / \"processed\" / \"session_sequences\" / f\"source_sessions_{RUN_TAG}\")\n",
    "OUT_SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_SEQ_TRAIN = OUT_SEQ_DIR / \"train\"\n",
    "OUT_SEQ_VAL   = OUT_SEQ_DIR / \"val\"\n",
    "OUT_SEQ_TEST  = OUT_SEQ_DIR / \"test\"\n",
    "\n",
    "for d in [OUT_SEQ_TRAIN, OUT_SEQ_VAL, OUT_SEQ_TEST]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"IN_SRC_SESS:\", IN_SRC_SESS.resolve())\n",
    "print(\"OUT_SEQ_DIR:\", OUT_SEQ_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc2862",
   "metadata": {},
   "source": [
    "DuckDB setup (spill to disk) + input sanity (REPO_ROOT-safe) (CHECKPOINT 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab693e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_events  n_users  n_items  n_sessions              min_ts  \\\n",
      "0  154817413   770283     1628     9996057 2015-07-31 23:59:15   \n",
      "\n",
      "               max_ts  \n",
      "0 2017-07-31 23:59:09  \n",
      "duckdb_tmp: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\_duckdb_tmp\n",
      "\n",
      "CHECKPOINT 1 ✅ Paste the stats table.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05C-02] DuckDB setup (spill to disk) + view src\n",
    "\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "duckdb_tmp = OUT_SEQ_DIR / \"_duckdb_tmp\"\n",
    "duckdb_tmp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "# Stability in notebooks\n",
    "con.execute(\"SET enable_progress_bar=false;\")\n",
    "con.execute(\"SET preserve_insertion_order=false;\")\n",
    "con.execute(\"PRAGMA enable_object_cache=false;\")\n",
    "con.execute(\"SET temp_directory = ?;\", [duckdb_tmp.as_posix()])\n",
    "\n",
    "# Speed/Resources: you have 32GB RAM, so give DuckDB more memory\n",
    "con.execute(\"SET threads=4;\")                # try 4; if unstable lower to 2\n",
    "con.execute(\"PRAGMA memory_limit='20GB';\")   # use your RAM, reduce spill\n",
    "\n",
    "src_path = IN_SRC_SESS.as_posix().replace(\"'\", \"''\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW src AS\n",
    "SELECT\n",
    "  CAST(domain AS VARCHAR) AS domain,\n",
    "  CAST(user_id AS VARCHAR) AS user_id,\n",
    "  CAST(item_id AS VARCHAR) AS item_id,\n",
    "  CAST(timestamp AS TIMESTAMP) AS ts,\n",
    "  CAST(session_id AS VARCHAR) AS session_id\n",
    "FROM read_parquet('{src_path}');\n",
    "\"\"\")\n",
    "\n",
    "stats = con.execute(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS n_events,\n",
    "  COUNT(DISTINCT user_id) AS n_users,\n",
    "  COUNT(DISTINCT item_id) AS n_items,\n",
    "  COUNT(DISTINCT session_id) AS n_sessions,\n",
    "  MIN(ts) AS min_ts,\n",
    "  MAX(ts) AS max_ts\n",
    "FROM src;\n",
    "\"\"\").df()\n",
    "\n",
    "print(stats)\n",
    "print(\"duckdb_tmp:\", duckdb_tmp.resolve())\n",
    "print(\"\\nCHECKPOINT 1 ✅ Paste the stats table.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f52dc7",
   "metadata": {},
   "source": [
    "Build session_lengths + deterministic splits (no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93994e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_sessions  singleton_sessions  eligible_sessions             q\n",
      "0         9996057           1655500.0          8340557.0  [7, 36, 127]\n",
      "            split  n_sessions\n",
      "0  drop_singleton     1655500\n",
      "1            test      835233\n",
      "2           train     6672282\n",
      "3             val      833042\n",
      "Seconds: 15.11\n",
      "\n",
      "CHECKPOINT 2 ✅ Paste summary + split counts.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05C-03] Build session_lengths + deterministic splits (no leakage)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS sess_len;\")\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE sess_len AS\n",
    "SELECT\n",
    "  session_id,\n",
    "  ANY_VALUE(domain) AS domain,\n",
    "  ANY_VALUE(user_id) AS user_id,\n",
    "  COUNT(*) AS session_length,\n",
    "  MIN(ts) AS start_ts,\n",
    "  MAX(ts) AS end_ts\n",
    "FROM src\n",
    "GROUP BY session_id;\n",
    "\"\"\")\n",
    "\n",
    "summary = con.execute(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS total_sessions,\n",
    "  SUM(CASE WHEN session_length=1 THEN 1 ELSE 0 END) AS singleton_sessions,\n",
    "  SUM(CASE WHEN session_length>=2 THEN 1 ELSE 0 END) AS eligible_sessions,\n",
    "  approx_quantile(session_length, [0.5,0.9,0.99]) AS q\n",
    "FROM sess_len;\n",
    "\"\"\").df()\n",
    "print(summary)\n",
    "\n",
    "VAL_FRAC = 0.10\n",
    "TEST_FRAC = 0.10\n",
    "val_cut = int(VAL_FRAC * 1000)\n",
    "test_cut = int(TEST_FRAC * 1000)\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS session_splits;\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE session_splits AS\n",
    "SELECT\n",
    "  session_id,\n",
    "  domain,\n",
    "  user_id,\n",
    "  session_length,\n",
    "  start_ts,\n",
    "  end_ts,\n",
    "  CASE\n",
    "    WHEN session_length < 2 THEN 'drop_singleton'\n",
    "    WHEN (abs(hash(session_id)) % 1000) < {test_cut} THEN 'test'\n",
    "    WHEN (abs(hash(session_id)) % 1000) < {test_cut + val_cut} THEN 'val'\n",
    "    ELSE 'train'\n",
    "  END AS split\n",
    "FROM sess_len;\n",
    "\"\"\")\n",
    "\n",
    "counts = con.execute(\"\"\"\n",
    "SELECT split, COUNT(*) AS n_sessions\n",
    "FROM session_splits\n",
    "GROUP BY split\n",
    "ORDER BY split;\n",
    "\"\"\").df()\n",
    "print(counts)\n",
    "\n",
    "print(\"Seconds:\", round(time.time() - t0, 2))\n",
    "print(\"\\nCHECKPOINT 2 ✅ Paste summary + split counts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5e7a5",
   "metadata": {},
   "source": [
    "Write session sequences (bucketed + resume)\n",
    "This writes one row per session with items list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf41ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_BUCKETS: 1024 | bucket range: 0 → 1023\n",
      "\n",
      "======================================================================\n",
      "WRITING SEQUENCES: val\n",
      "======================================================================\n",
      "  val: 128/1024 | wrote=6 | empty=0 | exists=122 | 1.2m\n",
      "  val: 192/1024 | wrote=70 | empty=0 | exists=122 | 13.7m\n",
      "  val: 256/1024 | wrote=134 | empty=0 | exists=122 | 26.0m\n",
      "  val: 320/1024 | wrote=198 | empty=0 | exists=122 | 38.3m\n",
      "  val: 384/1024 | wrote=262 | empty=0 | exists=122 | 50.6m\n",
      "  val: 448/1024 | wrote=326 | empty=0 | exists=122 | 63.0m\n",
      "  val: 512/1024 | wrote=390 | empty=0 | exists=122 | 75.7m\n",
      "  val: 576/1024 | wrote=454 | empty=0 | exists=122 | 89.1m\n",
      "  val: 640/1024 | wrote=518 | empty=0 | exists=122 | 101.5m\n",
      "  val: 704/1024 | wrote=582 | empty=0 | exists=122 | 114.1m\n",
      "  val: 768/1024 | wrote=646 | empty=0 | exists=122 | 126.3m\n",
      "  val: 832/1024 | wrote=710 | empty=0 | exists=122 | 138.2m\n",
      "  val: 896/1024 | wrote=774 | empty=0 | exists=122 | 150.1m\n",
      "  val: 960/1024 | wrote=838 | empty=0 | exists=122 | 161.9m\n",
      "  val: 1024/1024 | wrote=902 | empty=0 | exists=122 | 173.9m\n",
      "✅ Finished val: wrote=902 | empty=0 | exists=122\n",
      "out_dir: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\val\n",
      "\n",
      "======================================================================\n",
      "WRITING SEQUENCES: test\n",
      "======================================================================\n",
      "  test: 64/1024 | wrote=64 | empty=0 | exists=0 | 12.5m\n",
      "  test: 128/1024 | wrote=128 | empty=0 | exists=0 | 23.9m\n",
      "  test: 192/1024 | wrote=192 | empty=0 | exists=0 | 35.0m\n",
      "  test: 256/1024 | wrote=256 | empty=0 | exists=0 | 46.4m\n",
      "  test: 320/1024 | wrote=320 | empty=0 | exists=0 | 59.0m\n",
      "  test: 384/1024 | wrote=384 | empty=0 | exists=0 | 72.4m\n",
      "  test: 448/1024 | wrote=448 | empty=0 | exists=0 | 85.9m\n",
      "  test: 512/1024 | wrote=512 | empty=0 | exists=0 | 99.0m\n",
      "  test: 576/1024 | wrote=576 | empty=0 | exists=0 | 111.3m\n",
      "  test: 640/1024 | wrote=640 | empty=0 | exists=0 | 123.7m\n",
      "  test: 704/1024 | wrote=704 | empty=0 | exists=0 | 135.9m\n",
      "  test: 768/1024 | wrote=768 | empty=0 | exists=0 | 148.2m\n",
      "  test: 832/1024 | wrote=832 | empty=0 | exists=0 | 161.0m\n",
      "  test: 896/1024 | wrote=896 | empty=0 | exists=0 | 173.9m\n",
      "  test: 960/1024 | wrote=960 | empty=0 | exists=0 | 186.7m\n",
      "  test: 1024/1024 | wrote=1024 | empty=0 | exists=0 | 199.6m\n",
      "✅ Finished test: wrote=1024 | empty=0 | exists=0\n",
      "out_dir: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\test\n",
      "\n",
      "======================================================================\n",
      "WRITING SEQUENCES: train\n",
      "======================================================================\n",
      "  train: 64/1024 | wrote=64 | empty=0 | exists=0 | 14.8m\n",
      "  train: 128/1024 | wrote=128 | empty=0 | exists=0 | 29.0m\n",
      "  train: 192/1024 | wrote=192 | empty=0 | exists=0 | 43.4m\n",
      "  train: 256/1024 | wrote=256 | empty=0 | exists=0 | 57.7m\n",
      "  train: 320/1024 | wrote=320 | empty=0 | exists=0 | 71.8m\n",
      "  train: 384/1024 | wrote=384 | empty=0 | exists=0 | 86.1m\n",
      "  train: 448/1024 | wrote=448 | empty=0 | exists=0 | 102.2m\n",
      "  train: 512/1024 | wrote=512 | empty=0 | exists=0 | 117.6m\n",
      "  train: 576/1024 | wrote=576 | empty=0 | exists=0 | 133.4m\n",
      "  train: 640/1024 | wrote=640 | empty=0 | exists=0 | 149.5m\n",
      "  train: 704/1024 | wrote=704 | empty=0 | exists=0 | 164.8m\n",
      "  train: 768/1024 | wrote=768 | empty=0 | exists=0 | 179.8m\n",
      "  train: 832/1024 | wrote=832 | empty=0 | exists=0 | 194.8m\n",
      "  train: 896/1024 | wrote=896 | empty=0 | exists=0 | 209.7m\n",
      "  train: 960/1024 | wrote=960 | empty=0 | exists=0 | 224.4m\n",
      "  train: 1024/1024 | wrote=1024 | empty=0 | exists=0 | 239.6m\n",
      "✅ Finished train: wrote=1024 | empty=0 | exists=0\n",
      "out_dir: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\train\n",
      "\n",
      "All sequences written.\n",
      "Total seconds: 36785.5\n",
      "\n",
      "CHECKPOINT 3 ✅ Paste the Finished val/test/train lines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05C-04] Write SOURCE session sequences (1 row/session) — bucketed + resume\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "N_BUCKETS = 1024          # 512–2048 ok; 1024 is a good balance\n",
    "START_BUCKET = 0          # set to resume if interrupted\n",
    "END_BUCKET = N_BUCKETS-1\n",
    "PROGRESS_EVERY = 64\n",
    "\n",
    "print(\"N_BUCKETS:\", N_BUCKETS, \"| bucket range:\", START_BUCKET, \"→\", END_BUCKET)\n",
    "\n",
    "def write_sequences(split_name: str, out_dir: Path):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WRITING SEQUENCES:\", split_name)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    written = 0\n",
    "    skipped_exists = 0\n",
    "    skipped_empty = 0\n",
    "    t_split = time.time()\n",
    "\n",
    "    for b in range(START_BUCKET, END_BUCKET + 1):\n",
    "        out_file = out_dir / f\"sessions_b{b:04d}.parquet\"\n",
    "        if out_file.exists() and out_file.stat().st_size > 0:\n",
    "            skipped_exists += 1\n",
    "            continue\n",
    "\n",
    "        # quick skip empty bucket\n",
    "        n_sess = con.execute(f\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM session_splits\n",
    "            WHERE split='{split_name}'\n",
    "              AND (abs(hash(session_id)) % {N_BUCKETS}) = {b};\n",
    "        \"\"\").fetchone()[0]\n",
    "\n",
    "        if n_sess == 0:\n",
    "            skipped_empty += 1\n",
    "            continue\n",
    "\n",
    "        out_sql = out_file.as_posix().replace(\"'\", \"''\")\n",
    "\n",
    "        sql = f\"\"\"\n",
    "        COPY (\n",
    "          WITH eligible AS (\n",
    "            SELECT session_id, domain, user_id, start_ts, end_ts, session_length\n",
    "            FROM session_splits\n",
    "            WHERE split='{split_name}'\n",
    "              AND (abs(hash(session_id)) % {N_BUCKETS}) = {b}\n",
    "          ),\n",
    "          ev AS (\n",
    "            SELECT\n",
    "              e.domain,\n",
    "              e.user_id,\n",
    "              s.session_id,\n",
    "              s.ts,\n",
    "              s.item_id,\n",
    "              e.start_ts,\n",
    "              e.end_ts,\n",
    "              e.session_length\n",
    "            FROM src s\n",
    "            JOIN eligible e USING(session_id)\n",
    "          )\n",
    "          SELECT\n",
    "            domain,\n",
    "            user_id,\n",
    "            session_id,\n",
    "            session_length,\n",
    "            start_ts,\n",
    "            end_ts,\n",
    "            list(item_id ORDER BY ts, item_id) AS items,\n",
    "            '{split_name}' AS split\n",
    "          FROM ev\n",
    "          GROUP BY domain, user_id, session_id, session_length, start_ts, end_ts\n",
    "        ) TO '{out_sql}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\n",
    "        \"\"\"\n",
    "\n",
    "        con.execute(sql)\n",
    "        written += 1\n",
    "\n",
    "        if (b + 1) % PROGRESS_EVERY == 0:\n",
    "            elapsed = (time.time() - t_split) / 60\n",
    "            print(f\"  {split_name}: {b+1}/{N_BUCKETS} | wrote={written} | empty={skipped_empty} | exists={skipped_exists} | {elapsed:.1f}m\")\n",
    "\n",
    "    print(f\"✅ Finished {split_name}: wrote={written} | empty={skipped_empty} | exists={skipped_exists}\")\n",
    "    print(\"out_dir:\", out_dir.resolve())\n",
    "\n",
    "# smaller first\n",
    "write_sequences(\"val\",  OUT_SEQ_VAL)\n",
    "write_sequences(\"test\", OUT_SEQ_TEST)\n",
    "write_sequences(\"train\",OUT_SEQ_TRAIN)\n",
    "\n",
    "print(\"\\nAll sequences written.\")\n",
    "print(\"Total seconds:\", round(time.time() - t0, 2))\n",
    "print(\"\\nCHECKPOINT 3 ✅ Paste the Finished val/test/train lines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975dbcc4",
   "metadata": {},
   "source": [
    "Verify output counts (sessions) + quick sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session rows written:\n",
      "  val:   833,042\n",
      "  test:  835,233\n",
      "  train: 6,672,282\n",
      "\n",
      "Sample rows:\n",
      "     session_id  user_id  session_length  items_len\n",
      "0  3071230::792  3071230             113        113\n",
      "1   1510255::23  1510255               9          9\n",
      "2    646033::12   646033              12         12\n",
      "3   2686301::18  2686301               5          5\n",
      "4   3111881::85  3111881              10         10\n",
      "\n",
      "CHECKPOINT 4 ✅ Paste session row counts + sample rows.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05C-05] Verify output counts (sessions) + quick sanity\n",
    "\n",
    "val_glob  = (OUT_SEQ_VAL  / \"sessions_b*.parquet\").as_posix().replace(\"'\", \"''\")\n",
    "test_glob = (OUT_SEQ_TEST / \"sessions_b*.parquet\").as_posix().replace(\"'\", \"''\")\n",
    "train_glob= (OUT_SEQ_TRAIN/ \"sessions_b*.parquet\").as_posix().replace(\"'\", \"''\")\n",
    "\n",
    "val_n  = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{val_glob}');\").fetchone()[0]\n",
    "test_n = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{test_glob}');\").fetchone()[0]\n",
    "train_n= con.execute(f\"SELECT COUNT(*) FROM read_parquet('{train_glob}');\").fetchone()[0]\n",
    "\n",
    "print(\"Session rows written:\")\n",
    "print(\"  val:  \", f\"{val_n:,}\")\n",
    "print(\"  test: \", f\"{test_n:,}\")\n",
    "print(\"  train:\", f\"{train_n:,}\")\n",
    "\n",
    "# optional: inspect one row\n",
    "sample = con.execute(f\"\"\"\n",
    "SELECT session_id, user_id, session_length, list_count(items) AS items_len\n",
    "FROM read_parquet('{val_glob}')\n",
    "LIMIT 5;\n",
    "\"\"\").df()\n",
    "print(\"\\nSample rows:\")\n",
    "print(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13225cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\meta_source_session_sequences.json\n",
      "{\n",
      "  \"train_sessions\": 6672282,\n",
      "  \"val_sessions\": 833042,\n",
      "  \"test_sessions\": 835233,\n",
      "  \"eligible_sessions_total\": 8340557\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05C-META] Write meta entry for source session sequences\n",
    "\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "\n",
    "meta = {\n",
    "  \"run_tag\": RUN_TAG,\n",
    "  \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "  \"artifact_type\": \"session_sequences\",\n",
    "  \"dataset\": \"source\",\n",
    "  \"paths\": {\n",
    "    \"root\": str(OUT_SEQ_DIR.resolve()),\n",
    "    \"train_glob\": str((OUT_SEQ_TRAIN / \"sessions_b*.parquet\").resolve()),\n",
    "    \"val_glob\": str((OUT_SEQ_VAL / \"sessions_b*.parquet\").resolve()),\n",
    "    \"test_glob\": str((OUT_SEQ_TEST / \"sessions_b*.parquet\").resolve()),\n",
    "  },\n",
    "  \"counts\": {\n",
    "    \"train_sessions\": int(train_n),\n",
    "    \"val_sessions\": int(val_n),\n",
    "    \"test_sessions\": int(test_n),\n",
    "    \"eligible_sessions_total\": int(train_n + val_n + test_n),\n",
    "  },\n",
    "  \"notes\": {\n",
    "    \"source_gap_seconds\": 600,\n",
    "    \"split_rule\": \"hash(session_id) modulo 1000; test=10%, val=10%, train=80%; singletons dropped\",\n",
    "    \"items_list_order\": \"ORDER BY ts, item_id\"\n",
    "  }\n",
    "}\n",
    "\n",
    "OUT_META = OUT_SEQ_DIR / \"meta_source_session_sequences.json\"\n",
    "OUT_META.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", OUT_META.resolve())\n",
    "print(json.dumps(meta[\"counts\"], indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
