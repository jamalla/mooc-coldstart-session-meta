{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341ae1bb",
   "metadata": {},
   "source": [
    "Imports + env info (GRU4Rec baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fd6745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-00] Imports OK\n",
      "[09-00] torch: 2.9.1+cpu\n",
      "[09-00] pandas: 2.3.3\n",
      "[09-00] numpy: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-00] Imports + env info (GRU4Rec baseline)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"[09-00] Imports OK\")\n",
    "print(\"[09-00] torch:\", torch.__version__)\n",
    "print(\"[09-00] pandas:\", pd.__version__)\n",
    "print(\"[09-00] numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4753f",
   "metadata": {},
   "source": [
    "Locate repo root + fixed upstream run tags + load protocol/config artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8c84cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[09-01] RUN_TAG: 20260102_231041\n",
      "[09-01] Loaded dataloader_config keys: ['target', 'source', 'protocol']\n",
      "[09-01] Loaded sanity_metrics keys: ['run_tag_target', 'run_tag_source', 'created_at', 'target', 'source', 'notes']\n",
      "[09-01] Loaded session_gap_thresholds keys: ['generated_from_run_tag', 'generated_at', 'target', 'source', 'decision_notes']\n",
      "[09-01] ✅ Session gaps confirmed: target=30m, source=10m\n",
      "[09-01] Protocol from 06:\n",
      "  K_LIST: [5, 10, 20]\n",
      "  MAX_PREFIX_LEN: 20\n",
      "  CAP_ENABLED: True\n",
      "  CAP_SESSION_LEN: 200\n",
      "  CAP_STRATEGY: take_last\n",
      "\n",
      "[09-01] CHECKPOINT A\n",
      "Confirm JSON loads + gap asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-01] Locate repo root + fixed upstream run tags + load protocol/config artifacts\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists() and (p / \"meta.json\").exists():\n",
    "            return p\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not locate repo root (expected PROJECT_STATE.md).\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
    "print(\"[09-01] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[09-01] RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "# Fixed upstream run tags (do NOT change)\n",
    "TARGET_TAG = \"20251229_163357\"\n",
    "SOURCE_TAG = \"20251229_232834\"\n",
    "\n",
    "def load_json(path: Path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "cfg_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"dataloader_config_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "sanity_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"sanity_metrics_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "gaps_path_repo = REPO_ROOT / \"data/processed/normalized_events\" / \"session_gap_thresholds.json\"\n",
    "\n",
    "dataloader_cfg = load_json(cfg_path_repo)\n",
    "sanity_metrics = load_json(sanity_path_repo)\n",
    "session_gaps = load_json(gaps_path_repo)\n",
    "\n",
    "print(\"[09-01] Loaded dataloader_config keys:\", list(dataloader_cfg.keys()))\n",
    "print(\"[09-01] Loaded sanity_metrics keys:\", list(sanity_metrics.keys()))\n",
    "print(\"[09-01] Loaded session_gap_thresholds keys:\", list(session_gaps.keys()))\n",
    "\n",
    "# Enforce fixed decisions\n",
    "assert session_gaps[\"target\"][\"primary_threshold_seconds\"] == 1800, \"Target gap must be 30m (1800s).\"\n",
    "assert session_gaps[\"source\"][\"primary_threshold_seconds\"] == 600, \"Source gap must be 10m (600s).\"\n",
    "print(\"[09-01] ✅ Session gaps confirmed: target=30m, source=10m\")\n",
    "\n",
    "proto = dataloader_cfg[\"protocol\"]\n",
    "K_LIST = [5, 10, 20]\n",
    "MAX_K = max(K_LIST)\n",
    "\n",
    "MAX_PREFIX_LEN = int(proto[\"max_prefix_len\"])\n",
    "CAP_ENABLED = bool(proto[\"source_long_session_policy\"][\"enabled\"])\n",
    "CAP_SESSION_LEN = int(proto[\"source_long_session_policy\"][\"cap_session_len\"])\n",
    "CAP_STRATEGY = str(proto[\"source_long_session_policy\"][\"cap_strategy\"])\n",
    "\n",
    "print(\"[09-01] Protocol from 06:\")\n",
    "print(\"  K_LIST:\", K_LIST)\n",
    "print(\"  MAX_PREFIX_LEN:\", MAX_PREFIX_LEN)\n",
    "print(\"  CAP_ENABLED:\", CAP_ENABLED)\n",
    "print(\"  CAP_SESSION_LEN:\", CAP_SESSION_LEN)\n",
    "print(\"  CAP_STRATEGY:\", CAP_STRATEGY)\n",
    "\n",
    "print(\"\\n[09-01] CHECKPOINT A\")\n",
    "print(\"Confirm JSON loads + gap asserts passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a41c1",
   "metadata": {},
   "source": [
    "Resolve artifact paths (target tensors + source sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7308b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-02] ✅ All required artifacts exist\n",
      "\n",
      "[09-02] CHECKPOINT B\n",
      "If any artifact missing, STOP and paste the error.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-02] Resolve artifact paths (target tensors + source sequences) + existence checks\n",
    "\n",
    "def must_exist(p: Path, label: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "    return p\n",
    "\n",
    "# TARGET tensors (05B output)\n",
    "TARGET_TENSOR_DIR = REPO_ROOT / \"data/processed/tensor_target\"\n",
    "target_train_pt = TARGET_TENSOR_DIR / f\"target_tensor_train_{TARGET_TAG}.pt\"\n",
    "target_val_pt   = TARGET_TENSOR_DIR / f\"target_tensor_val_{TARGET_TAG}.pt\"\n",
    "target_test_pt  = TARGET_TENSOR_DIR / f\"target_tensor_test_{TARGET_TAG}.pt\"\n",
    "target_vocab_json = TARGET_TENSOR_DIR / f\"target_vocab_items_{TARGET_TAG}.json\"\n",
    "\n",
    "# SOURCE sequences (05C output)\n",
    "SOURCE_SEQ_ROOT = REPO_ROOT / \"data/processed/session_sequences\" / f\"source_sessions_{SOURCE_TAG}\"\n",
    "source_train_dir = SOURCE_SEQ_ROOT / \"train\"\n",
    "source_val_dir   = SOURCE_SEQ_ROOT / \"val\"\n",
    "source_test_dir  = SOURCE_SEQ_ROOT / \"test\"\n",
    "source_vocab_json = SOURCE_SEQ_ROOT / f\"source_vocab_items_{SOURCE_TAG}.json\"\n",
    "\n",
    "for p, lbl in [\n",
    "    (target_train_pt, \"target_train_pt\"),\n",
    "    (target_val_pt, \"target_val_pt\"),\n",
    "    (target_test_pt, \"target_test_pt\"),\n",
    "    (target_vocab_json, \"target_vocab_json\"),\n",
    "    (source_train_dir, \"source_train_dir\"),\n",
    "    (source_val_dir, \"source_val_dir\"),\n",
    "    (source_test_dir, \"source_test_dir\"),\n",
    "    (source_vocab_json, \"source_vocab_json\"),\n",
    "]:\n",
    "    must_exist(p, lbl)\n",
    "\n",
    "print(\"[09-02] ✅ All required artifacts exist\")\n",
    "\n",
    "print(\"\\n[09-02] CHECKPOINT B\")\n",
    "print(\"If any artifact missing, STOP and paste the error.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7695e",
   "metadata": {},
   "source": [
    "Torch loader (PyTorch 2.6+) + vocab sizes + PAD/UNK + source token->id mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011b89b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-03] TARGET: vocab_size from max(vocab values)+1 (token->id) = 747\n",
      "[09-03] SOURCE: vocab_size from key 'vocab_size' = 1620\n",
      "[09-03] PAD_ID_TARGET: 0 | UNK_ID_TARGET: 1\n",
      "[09-03] PAD_ID_SOURCE: 0 | UNK_ID_SOURCE: 1\n",
      "[09-03] ✅ Vocab + mapping ready\n",
      "\n",
      "[09-03] CHECKPOINT C\n",
      "Confirm vocab sizes + PAD/UNK before training.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-03] Torch loader (PyTorch 2.6+) + vocab sizes + PAD/UNK + source token->id mapper\n",
    "\n",
    "def torch_load_repo_artifact(path, map_location=\"cpu\"):\n",
    "    path = str(path)\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=map_location, weights_only=False)\n",
    "        print(f\"[09-03] torch.load OK (weights_only=False): {path}\")\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        obj = torch.load(path, map_location=map_location)\n",
    "        print(f\"[09-03] torch.load OK (no weights_only arg): {path}\")\n",
    "        return obj\n",
    "\n",
    "target_vocab = load_json(target_vocab_json)\n",
    "source_vocab = load_json(source_vocab_json)\n",
    "\n",
    "def infer_vocab_size(vocab: dict, name: str) -> int:\n",
    "    for k in [\"vocab_size\", \"n_items\", \"num_items\", \"size\"]:\n",
    "        if k in vocab:\n",
    "            vs = int(vocab[k])\n",
    "            print(f\"[09-03] {name}: vocab_size from key '{k}' = {vs}\")\n",
    "            return vs\n",
    "    if \"vocab\" in vocab and isinstance(vocab[\"vocab\"], dict):\n",
    "        d = vocab[\"vocab\"]\n",
    "        if len(d) == 0:\n",
    "            return 0\n",
    "        sample_k = next(iter(d.keys()))\n",
    "        sample_v = d[sample_k]\n",
    "        if isinstance(sample_v, int):\n",
    "            ids = list(d.values())\n",
    "            vs = max(ids) + 1 if len(ids) else 0\n",
    "            print(f\"[09-03] {name}: vocab_size from max(vocab values)+1 (token->id) = {vs}\")\n",
    "            return vs\n",
    "    if \"item2id\" in vocab and isinstance(vocab[\"item2id\"], dict):\n",
    "        ids = list(vocab[\"item2id\"].values())\n",
    "        vs = max(ids) + 1 if len(ids) else 0\n",
    "        print(f\"[09-03] {name}: vocab_size from max(item2id values)+1 = {vs}\")\n",
    "        return vs\n",
    "    raise KeyError(f\"[09-03] {name}: Could not infer vocab_size. Keys={list(vocab.keys())}\")\n",
    "\n",
    "vocab_size_target = infer_vocab_size(target_vocab, \"TARGET\")\n",
    "vocab_size_source = infer_vocab_size(source_vocab, \"SOURCE\")\n",
    "\n",
    "def get_special_id(vocab_obj: dict, token_key: str, fallback: int, name: str) -> int:\n",
    "    tok = vocab_obj.get(token_key, None)\n",
    "    if tok is None:\n",
    "        return fallback\n",
    "    mapping = vocab_obj.get(\"vocab\", {})\n",
    "    if isinstance(mapping, dict) and tok in mapping and isinstance(mapping[tok], int):\n",
    "        return int(mapping[tok])\n",
    "    return fallback\n",
    "\n",
    "PAD_ID_TARGET = get_special_id(target_vocab, \"pad_token\", 0, \"TARGET\")\n",
    "UNK_ID_TARGET = get_special_id(target_vocab, \"unk_token\", 1, \"TARGET\")\n",
    "\n",
    "PAD_ID_SOURCE = int(source_vocab.get(\"pad_id\", 0))\n",
    "UNK_ID_SOURCE = int(source_vocab.get(\"unk_id\", 1))\n",
    "\n",
    "print(\"[09-03] PAD_ID_TARGET:\", PAD_ID_TARGET, \"| UNK_ID_TARGET:\", UNK_ID_TARGET)\n",
    "print(\"[09-03] PAD_ID_SOURCE:\", PAD_ID_SOURCE, \"| UNK_ID_SOURCE:\", UNK_ID_SOURCE)\n",
    "\n",
    "assert PAD_ID_TARGET == 0\n",
    "assert PAD_ID_SOURCE == 0\n",
    "\n",
    "def build_token_to_id(vocab_obj: dict) -> dict:\n",
    "    if \"item2id\" in vocab_obj and isinstance(vocab_obj[\"item2id\"], dict):\n",
    "        return vocab_obj[\"item2id\"]\n",
    "    if \"vocab\" in vocab_obj and isinstance(vocab_obj[\"vocab\"], dict):\n",
    "        d = vocab_obj[\"vocab\"]\n",
    "        if len(d) > 0 and isinstance(next(iter(d.values())), int):\n",
    "            return d\n",
    "    if \"items\" in vocab_obj and isinstance(vocab_obj[\"items\"], list):\n",
    "        return {tok: i for i, tok in enumerate(vocab_obj[\"items\"])}\n",
    "    raise KeyError(f\"[09-03] Could not build token_to_id. Keys={list(vocab_obj.keys())}\")\n",
    "\n",
    "source_token_to_id = build_token_to_id(source_vocab)\n",
    "\n",
    "def map_source_seq_to_ids(seq) -> np.ndarray:\n",
    "    if seq is None:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    if isinstance(seq, np.ndarray):\n",
    "        seq_list = seq.tolist()\n",
    "    else:\n",
    "        seq_list = list(seq)\n",
    "    if len(seq_list) == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    if isinstance(seq_list[0], (int, np.integer)):\n",
    "        return np.asarray(seq_list, dtype=np.int64)\n",
    "    return np.fromiter((source_token_to_id.get(tok, UNK_ID_SOURCE) for tok in seq_list), dtype=np.int64)\n",
    "\n",
    "print(\"[09-03] ✅ Vocab + mapping ready\")\n",
    "\n",
    "print(\"\\n[09-03] CHECKPOINT C\")\n",
    "print(\"Confirm vocab sizes + PAD/UNK before training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7073d",
   "metadata": {},
   "source": [
    "Metrics (same as 06): HR/MRR/NDCG @ K={5,10,20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2433bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-04] ✅ Metric functions ready\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-04] Metrics (same as 06): HR/MRR/NDCG @ K={5,10,20}\n",
    "\n",
    "def init_metrics():\n",
    "    return {f\"{m}@{k}\": 0.0 for m in [\"HR\", \"MRR\", \"NDCG\"] for k in K_LIST}\n",
    "\n",
    "def update_metrics_from_rank(metrics: dict, rank0: int | None):\n",
    "    if rank0 is None:\n",
    "        return\n",
    "    r = rank0 + 1\n",
    "    for k in K_LIST:\n",
    "        if r <= k:\n",
    "            metrics[f\"HR@{k}\"] += 1.0\n",
    "            metrics[f\"MRR@{k}\"] += 1.0 / r\n",
    "            metrics[f\"NDCG@{k}\"] += 1.0 / math.log2(r + 1.0)\n",
    "\n",
    "def finalize_metrics(metrics: dict, n: int) -> dict:\n",
    "    return {k: (float(v / n) if n > 0 else 0.0) for k, v in metrics.items()}\n",
    "\n",
    "print(\"[09-04] ✅ Metric functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392bee0",
   "metadata": {},
   "source": [
    "GRU4Rec model definition (simple, CPU-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77db1fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-05] ✅ GRU4Rec defined\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-05] GRU4Rec model definition (simple, CPU-friendly)\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int = 64, hidden_dim: int = 128, pad_id: int = 0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        # input_ids: [B, T], lengths: [B]\n",
    "        x = self.emb(input_ids)  # [B, T, D]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, h = self.gru(packed)  # h: [1, B, H]\n",
    "        logits = self.out(h.squeeze(0))   # [B, V]\n",
    "        return logits\n",
    "\n",
    "print(\"[09-05] ✅ GRU4Rec defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e741f2",
   "metadata": {},
   "source": [
    "TARGET data tensors for training (from target_tensor_train_{TAG}.pt)\n",
    "- We train on (input_ids, attn_mask -> lengths, labels).\n",
    "- PAD excluded in loss using ignore_index=PAD_ID_TARGET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d511d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[09-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[09-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[09-06] TARGET train shapes: (1944, 20) (1944, 20) (1944,)\n",
      "[09-06] TARGET val shapes: (189, 20) (189,)\n",
      "[09-06] TARGET test shapes: (200, 20) (200,)\n",
      "\n",
      "[09-06] CHECKPOINT D\n",
      "Confirm shapes match expectations (train rows=1944, seq_len=20).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-06] TARGET data tensors for training (from target_tensor_train_{TAG}.pt)\n",
    "# We train on (input_ids, attn_mask -> lengths, labels).\n",
    "# PAD excluded in loss using ignore_index=PAD_ID_TARGET.\n",
    "\n",
    "train_obj = torch_load_repo_artifact(target_train_pt, map_location=\"cpu\")\n",
    "val_obj   = torch_load_repo_artifact(target_val_pt, map_location=\"cpu\")\n",
    "test_obj  = torch_load_repo_artifact(target_test_pt, map_location=\"cpu\")\n",
    "\n",
    "def as_tensor_dict(obj: dict):\n",
    "    return {\n",
    "        \"input_ids\": torch.as_tensor(obj[\"input_ids\"]).long(),\n",
    "        \"attn_mask\": torch.as_tensor(obj[\"attn_mask\"]).long(),\n",
    "        \"labels\": torch.as_tensor(obj[\"labels\"]).long(),\n",
    "    }\n",
    "\n",
    "target_train = as_tensor_dict(train_obj)\n",
    "target_val   = as_tensor_dict(val_obj)\n",
    "target_test  = as_tensor_dict(test_obj)\n",
    "\n",
    "print(\"[09-06] TARGET train shapes:\",\n",
    "      tuple(target_train[\"input_ids\"].shape),\n",
    "      tuple(target_train[\"attn_mask\"].shape),\n",
    "      tuple(target_train[\"labels\"].shape))\n",
    "\n",
    "print(\"[09-06] TARGET val shapes:\",\n",
    "      tuple(target_val[\"input_ids\"].shape),\n",
    "      tuple(target_val[\"labels\"].shape))\n",
    "\n",
    "print(\"[09-06] TARGET test shapes:\",\n",
    "      tuple(target_test[\"input_ids\"].shape),\n",
    "      tuple(target_test[\"labels\"].shape))\n",
    "\n",
    "print(\"\\n[09-06] CHECKPOINT D\")\n",
    "print(\"Confirm shapes match expectations (train rows=1944, seq_len=20).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa734822",
   "metadata": {},
   "source": [
    "TARGET training loop (small dataset, CPU OK)\n",
    "- Logs training loss and evaluates on VAL each epoch.\n",
    "- NOTE: This is target-only GRU4Rec baseline (Layer-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2ca16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-07] GRU_CFG: {'emb_dim': 64, 'hidden_dim': 128, 'batch_size': 256, 'epochs': 10, 'lr': 0.001, 'weight_decay': 0.0, 'grad_clip': 1.0, 'seed': 42}\n",
      "[09-07] epoch=01/10 loss=6.6174 time=0.8s | VAL: {'HR@5': 0.005291005291005291, 'HR@10': 0.031746031746031744, 'HR@20': 0.037037037037037035, 'MRR@5': 0.0026455026455026454, 'MRR@10': 0.006508776350046192, 'MRR@20': 0.006839464180734022, 'NDCG@5': 0.003338252664399246, 'NDCG@10': 0.01222772904824418, 'NDCG@20': 0.013522176361039025, '_n_examples': 189}\n",
      "[09-07] epoch=02/10 loss=6.5630 time=0.4s | VAL: {'HR@5': 0.031746031746031744, 'HR@10': 0.037037037037037035, 'HR@20': 0.0582010582010582, 'MRR@5': 0.009611992945326277, 'MRR@10': 0.010199882422104643, 'MRR@20': 0.011569319085658954, 'NDCG@5': 0.015001650402719228, 'NDCG@10': 0.01659440170252865, 'NDCG@20': 0.021820438050107506, '_n_examples': 189}\n",
      "[09-07] epoch=03/10 loss=6.5069 time=0.3s | VAL: {'HR@5': 0.031746031746031744, 'HR@10': 0.047619047619047616, 'HR@20': 0.08465608465608465, 'MRR@5': 0.010493827160493826, 'MRR@10': 0.012498950197362897, 'MRR@20': 0.01486779897144043, 'NDCG@5': 0.015694400421615826, 'NDCG@10': 0.02071994648395997, 'NDCG@20': 0.029837035283214165, '_n_examples': 189}\n",
      "[09-07] epoch=04/10 loss=6.4235 time=0.3s | VAL: {'HR@5': 0.031746031746031744, 'HR@10': 0.0582010582010582, 'HR@20': 0.10582010582010581, 'MRR@5': 0.014726631393298061, 'MRR@10': 0.017863441672965486, 'MRR@20': 0.021265031955975, 'NDCG@5': 0.018938565462702908, 'NDCG@10': 0.027099373608420192, 'NDCG@20': 0.03925806482338315, '_n_examples': 189}\n",
      "[09-07] epoch=05/10 loss=6.3088 time=0.4s | VAL: {'HR@5': 0.037037037037037035, 'HR@10': 0.07407407407407407, 'HR@20': 0.12698412698412698, 'MRR@5': 0.01984126984126984, 'MRR@10': 0.024221046443268664, 'MRR@20': 0.02743421280142986, 'NDCG@5': 0.024094651753243886, 'NDCG@10': 0.03549868040560962, 'NDCG@20': 0.048291721507168234, '_n_examples': 189}\n",
      "[09-07] epoch=06/10 loss=6.2033 time=0.5s | VAL: {'HR@5': 0.042328042328042326, 'HR@10': 0.06349206349206349, 'HR@20': 0.13756613756613756, 'MRR@5': 0.0181657848324515, 'MRR@10': 0.02064121945074326, 'MRR@20': 0.025470222940614604, 'NDCG@5': 0.024091786679460304, 'NDCG@10': 0.030583468492977967, 'NDCG@20': 0.048905669665598304, '_n_examples': 189}\n",
      "[09-07] epoch=07/10 loss=6.1079 time=0.6s | VAL: {'HR@5': 0.05291005291005291, 'HR@10': 0.07407407407407407, 'HR@20': 0.12698412698412698, 'MRR@5': 0.01904761904761905, 'MRR@10': 0.022060552616108172, 'MRR@20': 0.025945269645820043, 'NDCG@5': 0.02726371053662364, 'NDCG@10': 0.03429497639199801, 'NDCG@20': 0.04794408501115095, '_n_examples': 189}\n",
      "[09-07] epoch=08/10 loss=6.0247 time=0.3s | VAL: {'HR@5': 0.047619047619047616, 'HR@10': 0.09523809523809523, 'HR@20': 0.12169312169312169, 'MRR@5': 0.017548500881834215, 'MRR@10': 0.024057277231880406, 'MRR@20': 0.02583891656327648, 'NDCG@5': 0.024850079588681056, 'NDCG@10': 0.040416061301324475, 'NDCG@20': 0.04704276910350487, '_n_examples': 189}\n",
      "[09-07] epoch=09/10 loss=5.9456 time=0.3s | VAL: {'HR@5': 0.05291005291005291, 'HR@10': 0.10052910052910052, 'HR@20': 0.15873015873015872, 'MRR@5': 0.019488536155202827, 'MRR@10': 0.025802049214747626, 'MRR@20': 0.029719023589115483, 'NDCG@5': 0.027589669857495867, 'NDCG@10': 0.04296300159700696, 'NDCG@20': 0.05751611006702735, '_n_examples': 189}\n",
      "[09-07] epoch=10/10 loss=5.8632 time=0.3s | VAL: {'HR@5': 0.05291005291005291, 'HR@10': 0.1164021164021164, 'HR@20': 0.15873015873015872, 'MRR@5': 0.020194003527336864, 'MRR@10': 0.029251700680272108, 'MRR@20': 0.03209694209067642, 'NDCG@5': 0.028188332253080303, 'NDCG@10': 0.049299947817881065, 'NDCG@20': 0.05990017040185571, '_n_examples': 189}\n",
      "\n",
      "[09-07] CHECKPOINT E\n",
      "Paste the epoch logs (loss + VAL metrics). If VAL gets worse, we can reduce epochs or adjust hidden_dim.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-07] TARGET training loop (small dataset, CPU OK)\n",
    "# Logs training loss and evaluates on VAL each epoch.\n",
    "# NOTE: This is target-only GRU4Rec baseline (Layer-1).\n",
    "\n",
    "GRU_CFG = {\n",
    "    \"emb_dim\": 64,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "print(\"[09-07] GRU_CFG:\", GRU_CFG)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(GRU_CFG[\"seed\"])\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_t = GRU4Rec(vocab_size=vocab_size_target,\n",
    "                 emb_dim=GRU_CFG[\"emb_dim\"],\n",
    "                 hidden_dim=GRU_CFG[\"hidden_dim\"],\n",
    "                 pad_id=PAD_ID_TARGET).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model_t.parameters(), lr=GRU_CFG[\"lr\"], weight_decay=GRU_CFG[\"weight_decay\"])\n",
    "\n",
    "def make_lengths(attn_mask: torch.Tensor) -> torch.Tensor:\n",
    "    return attn_mask.sum(dim=1).long()\n",
    "\n",
    "def iter_batches(data: dict, batch_size: int, shuffle: bool = True):\n",
    "    n = data[\"input_ids\"].shape[0]\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for s in range(0, n, batch_size):\n",
    "        b = idx[s:s+batch_size]\n",
    "        yield (\n",
    "            data[\"input_ids\"][b].to(device),\n",
    "            make_lengths(data[\"attn_mask\"][b]).to(device),\n",
    "            data[\"labels\"][b].to(device),\n",
    "        )\n",
    "\n",
    "def eval_gru(model: GRU4Rec, data: dict, split_name: str) -> dict:\n",
    "    model.eval()\n",
    "    metrics = init_metrics()\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, lengths, y in iter_batches(data, batch_size=GRU_CFG[\"batch_size\"], shuffle=False):\n",
    "            logits = model(x, lengths)  # [B, V]\n",
    "            # mask PAD from ranking by setting it to -inf\n",
    "            logits[:, PAD_ID_TARGET] = -1e9\n",
    "            topk = torch.topk(logits, k=MAX_K, dim=1).indices.cpu().numpy()  # [B, K]\n",
    "            y_np = y.cpu().numpy()\n",
    "            for i in range(topk.shape[0]):\n",
    "                if int(y_np[i]) == PAD_ID_TARGET:\n",
    "                    continue\n",
    "                row = topk[i]\n",
    "                # rank lookup\n",
    "                pos = np.where(row == int(y_np[i]))[0]\n",
    "                rank0 = int(pos[0]) if pos.size > 0 else None\n",
    "                update_metrics_from_rank(metrics, rank0)\n",
    "                n += 1\n",
    "    out = finalize_metrics(metrics, n)\n",
    "    out[\"_n_examples\"] = int(n)\n",
    "    return out\n",
    "\n",
    "# Train\n",
    "train_losses = []\n",
    "for epoch in range(1, GRU_CFG[\"epochs\"] + 1):\n",
    "    model_t.train()\n",
    "    t0 = time.time()\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for x, lengths, y in iter_batches(target_train, GRU_CFG[\"batch_size\"], shuffle=True):\n",
    "        opt.zero_grad()\n",
    "        logits = model_t(x, lengths)  # [B, V]\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=PAD_ID_TARGET)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_t.parameters(), GRU_CFG[\"grad_clip\"])\n",
    "        opt.step()\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_n += bs\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    avg_loss = total_loss / max(1, total_n)\n",
    "    train_losses.append(avg_loss)\n",
    "    val_metrics = eval_gru(model_t, target_val, \"target_val\")\n",
    "\n",
    "    print(f\"[09-07] epoch={epoch:02d}/{GRU_CFG['epochs']} loss={avg_loss:.4f} time={dt:.1f}s | VAL:\", val_metrics)\n",
    "\n",
    "print(\"\\n[09-07] CHECKPOINT E\")\n",
    "print(\"Paste the epoch logs (loss + VAL metrics). If VAL gets worse, we can reduce epochs or adjust hidden_dim.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304dd4d",
   "metadata": {},
   "source": [
    "TARGET final evaluation on TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d77ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-08] TARGET VAL (GRU4Rec): {'HR@5': 0.05291005291005291, 'HR@10': 0.1164021164021164, 'HR@20': 0.15873015873015872, 'MRR@5': 0.020194003527336864, 'MRR@10': 0.029251700680272108, 'MRR@20': 0.03209694209067642, 'NDCG@5': 0.028188332253080303, 'NDCG@10': 0.049299947817881065, 'NDCG@20': 0.05990017040185571, '_n_examples': 189}\n",
      "[09-08] TARGET TEST (GRU4Rec): {'HR@5': 0.08, 'HR@10': 0.15, 'HR@20': 0.215, 'MRR@5': 0.04699999999999999, 'MRR@10': 0.056803571428571425, 'MRR@20': 0.06119113452337136, 'NDCG@5': 0.05513923995665121, 'NDCG@10': 0.07822673444977948, 'NDCG@20': 0.0945156893266747, '_n_examples': 200}\n",
      "\n",
      "[09-08] CHECKPOINT F\n",
      "Paste TARGET VAL/TEST metrics before we write reports.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-08] TARGET final evaluation on TEST\n",
    "\n",
    "t_val_gru = eval_gru(model_t, target_val, \"target_val\")\n",
    "t_test_gru = eval_gru(model_t, target_test, \"target_test\")\n",
    "\n",
    "print(\"[09-08] TARGET VAL (GRU4Rec):\", t_val_gru)\n",
    "print(\"[09-08] TARGET TEST (GRU4Rec):\", t_test_gru)\n",
    "\n",
    "print(\"\\n[09-08] CHECKPOINT F\")\n",
    "print(\"Paste TARGET VAL/TEST metrics before we write reports.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528e6ea",
   "metadata": {},
   "source": [
    "Write report artifacts to reports/09_gru4rec_baseline/<RUN_TAG>/ + update meta.json\n",
    "- Note: This notebook covers TARGET GRU4Rec baseline first (source training comes later when we decide feasibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f732d280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-09] ✅ Saved model checkpoint: C:\\mooc-coldstart-session-meta\\reports\\09_gru4rec_baseline\\20260102_231041\\model.pt\n",
      "[09-09] ✅ Wrote report files under: C:\\mooc-coldstart-session-meta\\reports\\09_gru4rec_baseline\\20260102_231041\n",
      "[09-09] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[09-09] CHECKPOINT G\n",
      "Paste: report dir + confirm meta.json updated.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-09] Write report artifacts to reports/09_gru4rec_baseline/<RUN_TAG>/ + update meta.json\n",
    "# Note: This notebook covers TARGET GRU4Rec baseline first (source training comes later when we decide feasibility).\n",
    "\n",
    "REPORT_DIR = REPO_ROOT / \"reports\" / \"09_gru4rec_baseline\" / RUN_TAG\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj: dict, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "run_meta = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"inputs\": {\n",
    "        \"target_run_tag\": TARGET_TAG,\n",
    "        \"source_run_tag\": SOURCE_TAG,\n",
    "        \"target_train_pt\": str(target_train_pt),\n",
    "        \"target_val_pt\": str(target_val_pt),\n",
    "        \"target_test_pt\": str(target_test_pt),\n",
    "        \"target_vocab_json\": str(target_vocab_json),\n",
    "        \"dataloader_config\": str(cfg_path_repo),\n",
    "        \"sanity_metrics\": str(sanity_path_repo),\n",
    "        \"session_gap_thresholds\": str(gaps_path_repo),\n",
    "    },\n",
    "    \"protocol_reused_from_06\": {\n",
    "        \"K_LIST\": K_LIST,\n",
    "        \"MAX_PREFIX_LEN\": MAX_PREFIX_LEN,\n",
    "        \"PAD_ID_TARGET\": PAD_ID_TARGET,\n",
    "        \"pad_excluded_from_ranking\": True,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"GRU4Rec\",\n",
    "        \"vocab_size\": int(vocab_size_target),\n",
    "        \"emb_dim\": int(GRU_CFG[\"emb_dim\"]),\n",
    "        \"hidden_dim\": int(GRU_CFG[\"hidden_dim\"]),\n",
    "    },\n",
    "    \"train_cfg\": GRU_CFG,\n",
    "    \"notes\": [\n",
    "        \"This run trains/evaluates GRU4Rec on TARGET only (Layer-1 baseline).\",\n",
    "        \"Source GRU4Rec training will require a streaming sampler over source sessions and is handled separately.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"target\": {\n",
    "        \"val\": t_val_gru,\n",
    "        \"test\": t_test_gru,\n",
    "        \"train_losses\": train_losses,\n",
    "    },\n",
    "    \"source\": None,\n",
    "}\n",
    "\n",
    "save_json(run_meta, REPORT_DIR / \"run_meta.json\")\n",
    "save_json(results, REPORT_DIR / \"results.json\")\n",
    "\n",
    "# Save model checkpoint (optional but useful)\n",
    "ckpt = {\n",
    "    \"state_dict\": model_t.state_dict(),\n",
    "    \"gru_cfg\": GRU_CFG,\n",
    "    \"vocab_size_target\": vocab_size_target,\n",
    "    \"pad_id\": PAD_ID_TARGET,\n",
    "}\n",
    "torch.save(ckpt, REPORT_DIR / \"model.pt\")\n",
    "print(\"[09-09] ✅ Saved model checkpoint:\", REPORT_DIR / \"model.pt\")\n",
    "\n",
    "# Update meta.json\n",
    "meta_path = REPO_ROOT / \"meta.json\"\n",
    "meta = load_json(meta_path) if meta_path.exists() else {\"artifacts\": {}}\n",
    "\n",
    "meta.setdefault(\"artifacts\", {})\n",
    "meta[\"artifacts\"].setdefault(\"gru4rec_baseline\", {})\n",
    "meta[\"artifacts\"][\"gru4rec_baseline\"][RUN_TAG] = {\n",
    "    \"target_run_tag\": TARGET_TAG,\n",
    "    \"source_run_tag\": SOURCE_TAG,\n",
    "    \"report_dir\": str(REPORT_DIR),\n",
    "    \"results_json\": str(REPORT_DIR / \"results.json\"),\n",
    "    \"run_meta_json\": str(REPORT_DIR / \"run_meta.json\"),\n",
    "    \"model_pt\": str(REPORT_DIR / \"model.pt\"),\n",
    "}\n",
    "meta[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "save_json(meta, meta_path)\n",
    "\n",
    "print(\"[09-09] ✅ Wrote report files under:\", REPORT_DIR)\n",
    "print(\"[09-09] ✅ Updated meta.json:\", meta_path)\n",
    "\n",
    "print(\"\\n[09-09] CHECKPOINT G\")\n",
    "print(\"Paste: report dir + confirm meta.json updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856046e6",
   "metadata": {},
   "source": [
    "Footer summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac12c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 09 GRU4Rec Baseline Summary ==========\n",
      "RUN_TAG: 20260102_231041\n",
      "--- TARGET ---\n",
      "VAL : {'HR@5': 0.05291005291005291, 'HR@10': 0.1164021164021164, 'HR@20': 0.15873015873015872, 'MRR@5': 0.020194003527336864, 'MRR@10': 0.029251700680272108, 'MRR@20': 0.03209694209067642, 'NDCG@5': 0.028188332253080303, 'NDCG@10': 0.049299947817881065, 'NDCG@20': 0.05990017040185571, '_n_examples': 189}\n",
      "TEST: {'HR@5': 0.08, 'HR@10': 0.15, 'HR@20': 0.215, 'MRR@5': 0.04699999999999999, 'MRR@10': 0.056803571428571425, 'MRR@20': 0.06119113452337136, 'NDCG@5': 0.05513923995665121, 'NDCG@10': 0.07822673444977948, 'NDCG@20': 0.0945156893266747, '_n_examples': 200}\n",
      "Report dir: C:\\mooc-coldstart-session-meta\\reports\\09_gru4rec_baseline\\20260102_231041\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# [CELL 09-10] Footer summary\n",
    "\n",
    "print(\"========== 09 GRU4Rec Baseline Summary ==========\")\n",
    "print(\"RUN_TAG:\", RUN_TAG)\n",
    "print(\"--- TARGET ---\")\n",
    "print(\"VAL :\", t_val_gru)\n",
    "print(\"TEST:\", t_test_gru)\n",
    "print(\"Report dir:\", REPORT_DIR)\n",
    "print(\"================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
