{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8b94e6",
   "metadata": {},
   "source": [
    "Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdffa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-00] Starting... 2026-01-05 11:43:25\n",
      "[13-00] REPO_ROOT  : C:\\mooc-coldstart-session-meta\n",
      "[13-00] REPORT_DIR : C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\n",
      "[13-00] DEVICE     : cpu | torch=2.9.1+cpu\n",
      "[13-00] SEED       : 20260105\n",
      "[13-00] Done in 0.32s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-00] Bootstrap\n",
    "import os, json, time, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "CELL = \"13-00\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"meta.json\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = str(find_repo_root(Path.cwd().resolve()))\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "REPORT_DIR = os.path.join(REPO_ROOT, \"reports\", \"13_meta_train_on_source_itemset\", RUN_TAG)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SEED = 20260105\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"[{CELL}] REPO_ROOT  : {REPO_ROOT}\")\n",
    "print(f\"[{CELL}] REPORT_DIR : {REPORT_DIR}\")\n",
    "print(f\"[{CELL}] DEVICE     : {DEVICE} | torch={torch.__version__}\")\n",
    "print(f\"[{CELL}] SEED       : {SEED}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea6013",
   "metadata": {},
   "source": [
    "Load 12A task config + DL config, locate SOURCE tensors/parquet\n",
    "This cell intentionally doesn’t assume paths. It reads them from your existing configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb6b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-01] Starting... 2026-01-05 11:44:11\n",
      "[13-01] META_TASK_CFG_PATH: C:\\mooc-coldstart-session-meta\\meta_task_config.json\n",
      "[13-01] TASK_COVERAGE_PATH: C:\\mooc-coldstart-session-meta\\task_coverage.json\n",
      "[13-01] ⚠️ meta_task_config.json not found at this path.\n",
      "[13-01] ⚠️ task_coverage.json not found at this path.\n",
      "[13-01] DL_CFG_PATH: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[13-01] DL_CFG top keys: ['target', 'source', 'protocol']\n",
      "[13-01] SRC_CFG keys: ['run_tag', 'seq_dir', 'train_glob', 'val_glob', 'test_glob', 'vocab_json']\n",
      "[13-01] SRC_CFG preview: {'run_tag': '20251229_232834', 'seq_dir': 'C:\\\\mooc-coldstart-session-meta\\\\data\\\\processed\\\\session_sequences\\\\source_sessions_20251229_232834', 'train_glob': 'C:\\\\mooc-coldstart-session-meta\\\\data\\\\processed\\\\session_sequences\\\\source_sessions_20251229_232834\\\\train\\\\sessions_b*.parquet', 'val_glob': 'C:\\\\mooc-coldstart-session-meta\\\\data\\\\processed\\\\session_sequences\\\\source_sessions_20251229_232834\\\\val\\\\sessions_b*.parquet', 'test_glob': 'C:\\\\mooc-coldstart-session-meta\\\\data\\\\processed\\\\session_sequences\\\\source_sessions_20251229_232834\\\\test\\\\sessions_b*.parquet', 'vocab_json': 'C:\\\\mooc-coldstart-session-meta\\\\data\\\\processed\\\\session_sequences\\\\source_sessions_20251229_232834\\\\source_vocab_items_20251229_232834.json'}\n",
      "[13-01] Done in 0.02s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-01] Load existing configs (12A meta_task_config + dataloader_config) and locate SOURCE artifacts\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"13-01\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "META_TASK_CFG_PATH = os.path.join(REPO_ROOT, \"meta_task_config.json\")  # if you keep it at root, else update\n",
    "TASK_COVERAGE_PATH = os.path.join(REPO_ROOT, \"task_coverage.json\")\n",
    "\n",
    "# If your files are under reports/12A..., set those exact paths here.\n",
    "print(f\"[{CELL}] META_TASK_CFG_PATH: {META_TASK_CFG_PATH}\")\n",
    "print(f\"[{CELL}] TASK_COVERAGE_PATH: {TASK_COVERAGE_PATH}\")\n",
    "\n",
    "if not os.path.exists(META_TASK_CFG_PATH):\n",
    "    print(f\"[{CELL}] ⚠️ meta_task_config.json not found at this path.\")\n",
    "if not os.path.exists(TASK_COVERAGE_PATH):\n",
    "    print(f\"[{CELL}] ⚠️ task_coverage.json not found at this path.\")\n",
    "\n",
    "DL_CFG_PATH = os.path.join(REPO_ROOT, \"data\", \"processed\", \"supervised\", \"dataloader_config_20251229_163357_20251229_232834.json\")\n",
    "print(f\"[{CELL}] DL_CFG_PATH: {DL_CFG_PATH}\")\n",
    "\n",
    "with open(DL_CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    DL_CFG = json.load(f)\n",
    "\n",
    "print(f\"[{CELL}] DL_CFG top keys: {list(DL_CFG.keys())}\")\n",
    "SRC_CFG = DL_CFG[\"source\"]\n",
    "print(f\"[{CELL}] SRC_CFG keys: {list(SRC_CFG.keys())}\")\n",
    "\n",
    "# Expected keys (based on your earlier schema issue):\n",
    "# Either tensor_dir + train_pt/val_pt/test_pt + meta_json\n",
    "# Or seq_dir + glob patterns + vocab_json\n",
    "print(f\"[{CELL}] SRC_CFG preview: {SRC_CFG}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6733bb",
   "metadata": {},
   "source": [
    "Load SOURCE split tensors (preferred) + confirm padding direction and fields\n",
    "This mirrors what we did in 12C, but for source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e72453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-02A] Starting... 2026-01-05 12:00:06\n",
      "[13-02A] file=C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\train\\sessions_b0000.parquet\n",
      "[13-02A] df0.shape=(6410, 1)\n",
      "[13-02A] type(items[0])=<class 'numpy.ndarray'>\n",
      "[13-02A] repr(items[0]) head=['course-v1:TsinghuaX+00690212X+sp' 'course-v1:TsinghuaX+00690212X+sp'\n",
      " 'course-v1:TsinghuaX+00690212X+sp' 'course-v1:TsinghuaX+00690212X+sp'\n",
      " 'course-v1:TsinghuaX+00690212X+sp' 'course-v1:TsinghuaX+0\n",
      "[13-02A] normalize_seq -> <class 'list'> len=71\n",
      "[13-02A] first5 tokens=['course-v1:TsinghuaX+00690212X+sp', 'course-v1:TsinghuaX+00690212X+sp', 'course-v1:TsinghuaX+00690212X+sp', 'course-v1:TsinghuaX+00690212X+sp', 'course-v1:TsinghuaX+00690212X+sp']\n",
      "[13-02A] Done in 0.02s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-02A] Debug: inspect items column type/values from first parquet\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CELL = \"13-02A\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "p0 = train_files[0]\n",
    "table = pq.read_table(p0, columns=[seq_col])\n",
    "df0 = table.to_pandas()\n",
    "print(f\"[{CELL}] file={p0}\")\n",
    "print(f\"[{CELL}] df0.shape={df0.shape}\")\n",
    "\n",
    "v = df0.iloc[0][seq_col]\n",
    "print(f\"[{CELL}] type(items[0])={type(v)}\")\n",
    "try:\n",
    "    print(f\"[{CELL}] repr(items[0]) head={str(v)[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"[{CELL}] repr failed: {e}\")\n",
    "\n",
    "# robust normalize (same as we'll use in scanner)\n",
    "def normalize_seq(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if hasattr(x, \"as_py\"):\n",
    "        x = x.as_py()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = x.tolist()\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    # sometimes pandas stores list column as object with .tolist()\n",
    "    if hasattr(x, \"tolist\") and not isinstance(x, (str, bytes)):\n",
    "        try:\n",
    "            y = x.tolist()\n",
    "            if isinstance(y, list):\n",
    "                return y\n",
    "        except Exception:\n",
    "            pass\n",
    "    # last resort\n",
    "    return None\n",
    "\n",
    "seq0 = normalize_seq(v)\n",
    "print(f\"[{CELL}] normalize_seq -> {type(seq0)} len={None if seq0 is None else len(seq0)}\")\n",
    "if seq0 is not None:\n",
    "    print(f\"[{CELL}] first5 tokens={seq0[:5]}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63cc4903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-02B1] Starting... 2026-01-05 12:11:05\n",
      "[13-02B1] scanned 1/1024 rows=6410 unique_tasks=856 elapsed_file=0.29s\n",
      "[13-02B1] scanned 2/1024 rows=12905 unique_tasks=1005 elapsed_file=0.30s\n",
      "[13-02B1] scanned 3/1024 rows=19319 unique_tasks=1084 elapsed_file=0.30s\n",
      "[13-02B1] scanned 100/1024 rows=650531 unique_tasks=1516 elapsed_file=0.28s\n",
      "[13-02B1] scanned 200/1024 rows=1304244 unique_tasks=1556 elapsed_file=0.22s\n",
      "[13-02B1] scanned 300/1024 rows=1954910 unique_tasks=1564 elapsed_file=0.27s\n",
      "[13-02B1] scanned 400/1024 rows=2606674 unique_tasks=1577 elapsed_file=0.23s\n",
      "[13-02B1] scanned 500/1024 rows=3259886 unique_tasks=1582 elapsed_file=0.21s\n",
      "[13-02B1] scanned 600/1024 rows=3910276 unique_tasks=1585 elapsed_file=0.23s\n",
      "[13-02B1] scanned 700/1024 rows=4560993 unique_tasks=1586 elapsed_file=0.24s\n",
      "[13-02B1] scanned 800/1024 rows=5212194 unique_tasks=1590 elapsed_file=0.29s\n",
      "[13-02B1] scanned 900/1024 rows=5863399 unique_tasks=1595 elapsed_file=0.29s\n",
      "[13-02B1] scanned 1000/1024 rows=6515673 unique_tasks=1597 elapsed_file=0.26s\n",
      "[13-02B1] TRAIN PASS1 done. rows=6672282 unique_tasks=1597\n",
      "[13-02B1] skips: nonseq=0 empty=0 allpad=0\n",
      "[13-02B1] task_size(min/p50/max)=1/900/158715\n",
      "[13-02B1] top10 tasks: [(3, 158715), (2, 148202), (6, 136380), (4, 134801), (8, 113535), (9, 101921), (10, 89636), (12, 83767), (11, 81666), (5, 77550)]\n",
      "[13-02B1] ✅ wrote: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\task_counts_itemset_pass1.json\n",
      "[13-02B1] Done in 252.46s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-02B1] PASS 1: Count seed-item tasks over TRAIN (no refs; memory-safe)\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "CELL = \"13-02B1\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def normalize_seq(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if hasattr(x, \"as_py\"):\n",
    "        x = x.as_py()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = x.tolist()\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    if hasattr(x, \"tolist\") and not isinstance(x, (str, bytes)):\n",
    "        try:\n",
    "            y = x.tolist()\n",
    "            if isinstance(y, list):\n",
    "                return y\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def to_ids(seq_tokens, item2id_map, unk_id: int):\n",
    "    if seq_tokens is None:\n",
    "        return None\n",
    "    out = []\n",
    "    for x in seq_tokens:\n",
    "        if x is None:\n",
    "            continue\n",
    "        sx = str(x).strip()\n",
    "        if sx == \"\":\n",
    "            continue\n",
    "        out.append(int(item2id_map.get(sx, unk_id)))\n",
    "    return out\n",
    "\n",
    "def seed_first_nonpad(ids, pad_id: int):\n",
    "    if ids is None or len(ids) == 0:\n",
    "        return None\n",
    "    for v in ids:\n",
    "        iv = int(v)\n",
    "        if iv != int(pad_id):\n",
    "            return iv\n",
    "    return int(pad_id)\n",
    "\n",
    "task_counts = Counter()\n",
    "row_count = 0\n",
    "skip_nonseq = 0\n",
    "skip_empty = 0\n",
    "skip_allpad = 0\n",
    "\n",
    "for fi, path in enumerate(train_files):\n",
    "    tf = time.time()\n",
    "    df = pq.read_table(path, columns=[seq_col]).to_pandas()\n",
    "    n = len(df)\n",
    "\n",
    "    for r in range(n):\n",
    "        seq_tokens = normalize_seq(df.iloc[r][seq_col])\n",
    "        if seq_tokens is None:\n",
    "            skip_nonseq += 1\n",
    "            continue\n",
    "\n",
    "        ids = to_ids(seq_tokens, item2id, UNK_ID_SRC)\n",
    "        if ids is None or len(ids) == 0:\n",
    "            skip_empty += 1\n",
    "            continue\n",
    "\n",
    "        s = seed_first_nonpad(ids, PAD_ID_SRC)\n",
    "        if s is None or s == PAD_ID_SRC:\n",
    "            skip_allpad += 1\n",
    "            continue\n",
    "\n",
    "        task_counts[int(s)] += 1\n",
    "\n",
    "    row_count += n\n",
    "    if fi < 3 or (fi + 1) % 100 == 0:\n",
    "        print(f\"[{CELL}] scanned {fi+1}/{len(train_files)} rows={row_count} unique_tasks={len(task_counts)} elapsed_file={time.time()-tf:.2f}s\")\n",
    "\n",
    "print(f\"[{CELL}] TRAIN PASS1 done. rows={row_count} unique_tasks={len(task_counts)}\")\n",
    "print(f\"[{CELL}] skips: nonseq={skip_nonseq} empty={skip_empty} allpad={skip_allpad}\")\n",
    "\n",
    "if len(task_counts) == 0:\n",
    "    raise RuntimeError(f\"[{CELL}] No tasks found. Unexpected given 13-02A sample.\")\n",
    "\n",
    "sizes = sorted(task_counts.values())\n",
    "p50 = int(np.median(sizes))\n",
    "print(f\"[{CELL}] task_size(min/p50/max)={min(sizes)}/{p50}/{max(sizes)}\")\n",
    "print(f\"[{CELL}] top10 tasks: {task_counts.most_common(10)}\")\n",
    "\n",
    "# Save counts artifact (small)\n",
    "COUNTS_PATH = os.path.join(REPORT_DIR, \"task_counts_itemset_pass1.json\")\n",
    "with open(COUNTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({str(k): int(v) for k, v in task_counts.items()}, f)\n",
    "\n",
    "print(f\"[{CELL}] ✅ wrote: {COUNTS_PATH}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa644f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-02B2] Starting... 2026-01-05 12:15:35\n",
      "[13-02B2] MIN_TASK_EX=50\n",
      "[13-02B2] kept_tasks=1345 / 1597 total tasks\n",
      "[13-02B2] CAP_PER_TASK_REFS=5000\n",
      "[13-02B2] scanned 1/1024 rows=6410 tasks_with_refs=852 elapsed_file=0.35s\n",
      "[13-02B2] scanned 2/1024 rows=12905 tasks_with_refs=999 elapsed_file=0.31s\n",
      "[13-02B2] scanned 3/1024 rows=19319 tasks_with_refs=1076 elapsed_file=0.30s\n",
      "[13-02B2] scanned 100/1024 rows=650531 tasks_with_refs=1344 elapsed_file=0.37s\n",
      "[13-02B2] scanned 200/1024 rows=1304244 tasks_with_refs=1345 elapsed_file=0.25s\n",
      "[13-02B2] scanned 300/1024 rows=1954910 tasks_with_refs=1345 elapsed_file=0.26s\n",
      "[13-02B2] scanned 400/1024 rows=2606674 tasks_with_refs=1345 elapsed_file=0.28s\n",
      "[13-02B2] scanned 500/1024 rows=3259886 tasks_with_refs=1345 elapsed_file=0.27s\n",
      "[13-02B2] scanned 600/1024 rows=3910276 tasks_with_refs=1345 elapsed_file=0.30s\n",
      "[13-02B2] scanned 700/1024 rows=4560993 tasks_with_refs=1345 elapsed_file=0.28s\n",
      "[13-02B2] scanned 800/1024 rows=5212194 tasks_with_refs=1345 elapsed_file=0.29s\n",
      "[13-02B2] scanned 900/1024 rows=5863399 tasks_with_refs=1345 elapsed_file=0.28s\n",
      "[13-02B2] scanned 1000/1024 rows=6515673 tasks_with_refs=1345 elapsed_file=0.35s\n",
      "[13-02B2] TRAIN PASS2 done. rows=6672282\n",
      "[13-02B2] skips: nonseq=0 empty=0 allpad=0 notkept=4346\n",
      "[13-02B2] tasks_with_refs=1345 (should ~= kept_tasks)\n",
      "[13-02B2] task=2 true_count=148202 sampled_refs=5000\n",
      "[13-02B2] task=3 true_count=158715 sampled_refs=5000\n",
      "[13-02B2] task=4 true_count=134801 sampled_refs=5000\n",
      "[13-02B2] ✅ wrote refs: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\task_refs_itemset_pass2.pt\n",
      "[13-02B2] Done in 295.31s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-02B2] PASS 2: Build capped refs ONLY for kept tasks (reservoir sampling; reproducible)\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "CELL = \"13-02B2\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "MIN_TASK_EX = 50\n",
    "CAP_PER_TASK_REFS = 5000  # bounded memory; adjust only if needed\n",
    "RNG = np.random.RandomState(SEED)\n",
    "\n",
    "kept_tasks = [k for k, v in task_counts.items() if v >= MIN_TASK_EX]\n",
    "kept_tasks = sorted(kept_tasks)\n",
    "kept_set = set(kept_tasks)\n",
    "\n",
    "print(f\"[{CELL}] MIN_TASK_EX={MIN_TASK_EX}\")\n",
    "print(f\"[{CELL}] kept_tasks={len(kept_tasks)} / {len(task_counts)} total tasks\")\n",
    "print(f\"[{CELL}] CAP_PER_TASK_REFS={CAP_PER_TASK_REFS}\")\n",
    "\n",
    "if len(kept_tasks) == 0:\n",
    "    raise RuntimeError(f\"[{CELL}] No tasks kept. Lower MIN_TASK_EX.\")\n",
    "\n",
    "# reservoir state per task\n",
    "task_seen = defaultdict(int)      # how many examples of this task we've seen\n",
    "task_to_refs = defaultdict(list)  # sampled refs: list[(fi, r)]\n",
    "\n",
    "row_count = 0\n",
    "skip_nonseq = 0\n",
    "skip_empty = 0\n",
    "skip_allpad = 0\n",
    "skip_notkept = 0\n",
    "\n",
    "for fi, path in enumerate(train_files):\n",
    "    tf = time.time()\n",
    "    df = pq.read_table(path, columns=[seq_col]).to_pandas()\n",
    "    n = len(df)\n",
    "\n",
    "    for r in range(n):\n",
    "        seq_tokens = normalize_seq(df.iloc[r][seq_col])\n",
    "        if seq_tokens is None:\n",
    "            skip_nonseq += 1\n",
    "            continue\n",
    "\n",
    "        ids = to_ids(seq_tokens, item2id, UNK_ID_SRC)\n",
    "        if ids is None or len(ids) == 0:\n",
    "            skip_empty += 1\n",
    "            continue\n",
    "\n",
    "        s = seed_first_nonpad(ids, PAD_ID_SRC)\n",
    "        if s is None or s == PAD_ID_SRC:\n",
    "            skip_allpad += 1\n",
    "            continue\n",
    "\n",
    "        if s not in kept_set:\n",
    "            skip_notkept += 1\n",
    "            continue\n",
    "\n",
    "        # reservoir sampling per task\n",
    "        task_seen[s] += 1\n",
    "        seen = task_seen[s]\n",
    "        buf = task_to_refs[s]\n",
    "\n",
    "        if len(buf) < CAP_PER_TASK_REFS:\n",
    "            buf.append((fi, r))\n",
    "        else:\n",
    "            j = RNG.randint(0, seen)\n",
    "            if j < CAP_PER_TASK_REFS:\n",
    "                buf[j] = (fi, r)\n",
    "\n",
    "    row_count += n\n",
    "    if fi < 3 or (fi + 1) % 100 == 0:\n",
    "        print(f\"[{CELL}] scanned {fi+1}/{len(train_files)} rows={row_count} tasks_with_refs={len(task_to_refs)} elapsed_file={time.time()-tf:.2f}s\")\n",
    "\n",
    "print(f\"[{CELL}] TRAIN PASS2 done. rows={row_count}\")\n",
    "print(f\"[{CELL}] skips: nonseq={skip_nonseq} empty={skip_empty} allpad={skip_allpad} notkept={skip_notkept}\")\n",
    "print(f\"[{CELL}] tasks_with_refs={len(task_to_refs)} (should ~= kept_tasks)\")\n",
    "\n",
    "# sanity: check a couple tasks\n",
    "sample_tasks = kept_tasks[:3]\n",
    "for k in sample_tasks:\n",
    "    print(f\"[{CELL}] task={k} true_count={task_counts[k]} sampled_refs={len(task_to_refs.get(k, []))}\")\n",
    "\n",
    "# Save refs as torch file (faster, smaller than JSON)\n",
    "# store as dict[int, dict[file_idx:list,row_idx:list]]\n",
    "refs_pack = {}\n",
    "for k, refs in task_to_refs.items():\n",
    "    fi_list = [int(a) for a, b in refs]\n",
    "    ri_list = [int(b) for a, b in refs]\n",
    "    refs_pack[int(k)] = {\"file_idx\": fi_list, \"row_idx\": ri_list}\n",
    "\n",
    "REFS_PATH = os.path.join(REPORT_DIR, \"task_refs_itemset_pass2.pt\")\n",
    "torch.save(\n",
    "    {\"kept_tasks\": kept_tasks, \"cap\": CAP_PER_TASK_REFS, \"refs\": refs_pack},\n",
    "    REFS_PATH\n",
    ")\n",
    "\n",
    "print(f\"[{CELL}] ✅ wrote refs: {REFS_PATH}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b5ae3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-03] Starting... 2026-01-05 12:36:43\n",
      "[13-03] kept_task_size(min/median/max)=50/1407/158715\n",
      "[13-03] ✅ wrote config: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\meta_task_config_itemset.json\n",
      "[13-03] Done in 0.01s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-03] Save meta_task_config_itemset.json (used by meta-train cells)\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "CELL = \"13-03\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "kept_sizes = [int(task_counts[k]) for k in kept_tasks]\n",
    "print(f\"[{CELL}] kept_task_size(min/median/max)={min(kept_sizes)}/{int(np.median(kept_sizes))}/{max(kept_sizes)}\")\n",
    "\n",
    "META_TASK_CFG = {\n",
    "    \"task_key_mode\": \"seed_item_first_nonpad\",\n",
    "    \"min_task_examples\": int(MIN_TASK_EX),\n",
    "    \"cap_per_task_refs\": int(CAP_PER_TASK_REFS),\n",
    "    \"seq_col\": seq_col,\n",
    "    \"label_col\": None,\n",
    "    \"pad_id\": int(PAD_ID_SRC),\n",
    "    \"unk_id\": int(UNK_ID_SRC),\n",
    "    \"vocab_size\": int(VOCAB_SIZE_SRC),\n",
    "    \"tasks_kept\": [int(k) for k in kept_tasks],\n",
    "    \"task_counts_path\": os.path.join(REPORT_DIR, \"task_counts_itemset_pass1.json\"),\n",
    "    \"task_refs_path\": os.path.join(REPORT_DIR, \"task_refs_itemset_pass2.pt\"),\n",
    "    \"source\": {\n",
    "        \"seq_dir\": SEQ_DIR,\n",
    "        \"train_glob\": TRAIN_GLOB,\n",
    "        \"val_glob\": VAL_GLOB,\n",
    "        \"test_glob\": TEST_GLOB,\n",
    "        \"vocab_json\": VOCAB_JSON,\n",
    "    },\n",
    "}\n",
    "\n",
    "CFG_PATH = os.path.join(REPORT_DIR, \"meta_task_config_itemset.json\")\n",
    "with open(CFG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(META_TASK_CFG, f, indent=2)\n",
    "\n",
    "print(f\"[{CELL}] ✅ wrote config: {CFG_PATH}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e4b85",
   "metadata": {},
   "source": [
    "Load proto/meta_cfg from 12B report + load itemset task config/refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e72972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-04] Starting... 2026-01-05 12:53:35\n",
      "[13-04] REPORT_12B_JSON=C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\\report.json\n",
      "[13-04] Loaded PROTO keys=['K_LIST', 'MAX_PREFIX_LEN', 'CAP_ENABLED', 'CAP_SESSION_LEN', 'CAP_STRATEGY']\n",
      "[13-04] Loaded META_CFG keys=['emb_dim', 'hidden_dim', 'dropout', 'meta_lr', 'inner_lr', 'inner_steps', 'meta_steps', 'meta_batch_tasks', 'grad_clip', 'seed', 'log_every', 'eval_every', 'val_episodes']\n",
      "[13-04] META_CFG: meta_steps=2000 meta_batch_tasks=4 inner_steps=1 inner_lr=0.01 meta_lr=0.0005\n",
      "[13-04] ITEMSET_CFG_PATH=C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\meta_task_config_itemset.json\n",
      "[13-04] TASK_REFS_PATH=C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\task_refs_itemset_pass2.pt\n",
      "[13-04] kept_tasks=1345 | refs_tasks=1345 | cap=5000\n",
      "[13-04] train_files=1024 val_files=1024 test_files=1024\n",
      "[13-04] PAD=0 UNK=1 VOCAB=1620 seq_col=items\n",
      "[13-04] Done in 8.70s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-04] Load 12B proto/meta_cfg + load 13 itemset task config/refs\n",
    "import os, json, time, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "CELL = \"13-04\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 12B report.json (use your actual path if different)\n",
    "# You can also point to your 12B run report in reports/12B... if you prefer.\n",
    "REPORT_12B_JSON = os.path.join(REPO_ROOT, \"reports\", \"12B_meta_train_on_source\", \"20260104_165117\", \"report.json\")\n",
    "if not os.path.exists(REPORT_12B_JSON):\n",
    "    # fallback: if you copied it to root or elsewhere\n",
    "    REPORT_12B_JSON = os.path.join(REPO_ROOT, \"report.json\")\n",
    "\n",
    "print(f\"[{CELL}] REPORT_12B_JSON={REPORT_12B_JSON}\")\n",
    "if not os.path.exists(REPORT_12B_JSON):\n",
    "    raise RuntimeError(f\"[{CELL}] 12B report.json not found. Set REPORT_12B_JSON to the correct path.\")\n",
    "\n",
    "with open(REPORT_12B_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    rep12b = json.load(f)\n",
    "\n",
    "PROTO = rep12b[\"proto\"]\n",
    "META_CFG = rep12b[\"meta_cfg\"]\n",
    "print(f\"[{CELL}] Loaded PROTO keys={list(PROTO.keys())}\")\n",
    "print(f\"[{CELL}] Loaded META_CFG keys={list(META_CFG.keys())}\")\n",
    "print(f\"[{CELL}] META_CFG: meta_steps={META_CFG['meta_steps']} meta_batch_tasks={META_CFG['meta_batch_tasks']} inner_steps={META_CFG['inner_steps']} inner_lr={META_CFG['inner_lr']} meta_lr={META_CFG['meta_lr']}\")\n",
    "\n",
    "# 13 itemset config produced in 13-03\n",
    "ITEMSET_CFG_PATH = os.path.join(REPORT_DIR, \"meta_task_config_itemset.json\")\n",
    "print(f\"[{CELL}] ITEMSET_CFG_PATH={ITEMSET_CFG_PATH}\")\n",
    "with open(ITEMSET_CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ITEMSET_CFG = json.load(f)\n",
    "\n",
    "TASK_REFS_PATH = ITEMSET_CFG[\"task_refs_path\"]\n",
    "print(f\"[{CELL}] TASK_REFS_PATH={TASK_REFS_PATH}\")\n",
    "pack = torch.load(TASK_REFS_PATH, map_location=\"cpu\")\n",
    "\n",
    "KEPT_TASKS = list(map(int, pack[\"kept_tasks\"]))\n",
    "REFS = pack[\"refs\"]  # dict: task -> {\"file_idx\": [...], \"row_idx\":[...]}\n",
    "\n",
    "print(f\"[{CELL}] kept_tasks={len(KEPT_TASKS)} | refs_tasks={len(REFS)} | cap={pack.get('cap')}\")\n",
    "\n",
    "# Files lists\n",
    "TRAIN_GLOB = ITEMSET_CFG[\"source\"][\"train_glob\"]\n",
    "VAL_GLOB   = ITEMSET_CFG[\"source\"][\"val_glob\"]\n",
    "TEST_GLOB  = ITEMSET_CFG[\"source\"][\"test_glob\"]\n",
    "train_files = sorted(glob.glob(TRAIN_GLOB))\n",
    "val_files   = sorted(glob.glob(VAL_GLOB))\n",
    "test_files  = sorted(glob.glob(TEST_GLOB))\n",
    "print(f\"[{CELL}] train_files={len(train_files)} val_files={len(val_files)} test_files={len(test_files)}\")\n",
    "\n",
    "PAD_ID_SRC = int(ITEMSET_CFG[\"pad_id\"])\n",
    "UNK_ID_SRC = int(ITEMSET_CFG[\"unk_id\"])\n",
    "VOCAB_SIZE_SRC = int(ITEMSET_CFG[\"vocab_size\"])\n",
    "SEQ_COL = ITEMSET_CFG[\"seq_col\"]\n",
    "\n",
    "print(f\"[{CELL}] PAD={PAD_ID_SRC} UNK={UNK_ID_SRC} VOCAB={VOCAB_SIZE_SRC} seq_col={SEQ_COL}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a146d",
   "metadata": {},
   "source": [
    "Load vocab mapping (string → id) and define robust converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad09f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-05] Starting... 2026-01-05 12:54:09\n",
      "[13-05] VOCAB_JSON=C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\source_vocab_items_20251229_232834.json\n",
      "[13-05] item2id size=1620 (expect 1620)\n",
      "[13-05] ✅ converters ready\n",
      "[13-05] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-05] Load item2id mapping + robust converters (np.ndarray[str] -> list[int])\n",
    "import time, json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "CELL = \"13-05\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "VOCAB_JSON = ITEMSET_CFG[\"source\"][\"vocab_json\"]\n",
    "print(f\"[{CELL}] VOCAB_JSON={VOCAB_JSON}\")\n",
    "\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_obj = json.load(f)\n",
    "\n",
    "def find_mapping(obj: dict):\n",
    "    if isinstance(obj, dict) and obj and all(isinstance(v, int) for v in obj.values()):\n",
    "        return obj\n",
    "    for k in [\"item2id\", \"token_to_id\", \"vocab\", \"mapping\"]:\n",
    "        if k in obj and isinstance(obj[k], dict) and obj[k] and all(isinstance(v, int) for v in obj[k].values()):\n",
    "            return obj[k]\n",
    "    return None\n",
    "\n",
    "item2id = find_mapping(vocab_obj)\n",
    "if item2id is None:\n",
    "    raise RuntimeError(f\"[{CELL}] token->id mapping not found in vocab_json keys={list(vocab_obj.keys())}\")\n",
    "\n",
    "print(f\"[{CELL}] item2id size={len(item2id)} (expect {VOCAB_SIZE_SRC})\")\n",
    "\n",
    "def normalize_seq(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if hasattr(x, \"as_py\"):\n",
    "        x = x.as_py()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    if hasattr(x, \"tolist\") and not isinstance(x, (str, bytes)):\n",
    "        try:\n",
    "            y = x.tolist()\n",
    "            if isinstance(y, list):\n",
    "                return y\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def seq_str_to_ids(seq_tokens, item2id_map, unk_id: int):\n",
    "    if seq_tokens is None:\n",
    "        return None\n",
    "    out = []\n",
    "    for x in seq_tokens:\n",
    "        if x is None:\n",
    "            continue\n",
    "        sx = str(x).strip()\n",
    "        if sx == \"\":\n",
    "            continue\n",
    "        out.append(int(item2id_map.get(sx, unk_id)))\n",
    "    return out\n",
    "\n",
    "def cap_session(seq_ids, cap_len: int, strategy: str):\n",
    "    if seq_ids is None:\n",
    "        return None\n",
    "    if cap_len is None or cap_len <= 0:\n",
    "        return seq_ids\n",
    "    if len(seq_ids) <= cap_len:\n",
    "        return seq_ids\n",
    "    if strategy == \"take_last\":\n",
    "        return seq_ids[-cap_len:]\n",
    "    if strategy == \"take_first\":\n",
    "        return seq_ids[:cap_len]\n",
    "    return seq_ids[-cap_len:]\n",
    "\n",
    "print(f\"[{CELL}] ✅ converters ready\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018f868",
   "metadata": {},
   "source": [
    "Small LRU cache for parquet files (speed + reproducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e277fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-06] Starting... 2026-01-05 12:54:35\n",
      "[13-06] MAX_CACHE_FILES=8\n",
      "[13-06] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-06] Parquet file cache (LRU-like) to speed row access\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "CELL = \"13-06\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "MAX_CACHE_FILES = 8\n",
    "_cache = OrderedDict()  # file_path -> pandas df (items only)\n",
    "\n",
    "def get_items_df(file_path: str):\n",
    "    # LRU: move accessed item to end\n",
    "    if file_path in _cache:\n",
    "        df = _cache.pop(file_path)\n",
    "        _cache[file_path] = df\n",
    "        return df\n",
    "\n",
    "    # load from disk\n",
    "    df = pq.read_table(file_path, columns=[SEQ_COL]).to_pandas()\n",
    "    _cache[file_path] = df\n",
    "    if len(_cache) > MAX_CACHE_FILES:\n",
    "        _cache.popitem(last=False)\n",
    "    return df\n",
    "\n",
    "print(f\"[{CELL}] MAX_CACHE_FILES={MAX_CACHE_FILES}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c891bc",
   "metadata": {},
   "source": [
    "Build one supervised example from a session: prefix → next-item (left-pad to MAX_PREFIX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "810b160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-07] Starting... 2026-01-05 12:54:56\n",
      "[13-07] MAX_PREFIX_LEN=20 | CAP_ENABLED=True CAP_SESSION_LEN=200 CAP_STRATEGY=take_last\n",
      "[13-07] ✅ session_to_one_pair ready\n",
      "[13-07] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-07] Build (x,y) from a single session: random prefix -> next item (left-pad)\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "CELL = \"13-07\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "MAX_PREFIX_LEN = int(PROTO[\"MAX_PREFIX_LEN\"])\n",
    "CAP_ENABLED = bool(PROTO[\"CAP_ENABLED\"])\n",
    "CAP_SESSION_LEN = int(PROTO[\"CAP_SESSION_LEN\"]) if CAP_ENABLED else None\n",
    "CAP_STRATEGY = str(PROTO[\"CAP_STRATEGY\"])\n",
    "\n",
    "print(f\"[{CELL}] MAX_PREFIX_LEN={MAX_PREFIX_LEN} | CAP_ENABLED={CAP_ENABLED} CAP_SESSION_LEN={CAP_SESSION_LEN} CAP_STRATEGY={CAP_STRATEGY}\")\n",
    "\n",
    "RNG = np.random.RandomState(SEED)\n",
    "\n",
    "def session_to_one_pair(seq_ids):\n",
    "    \"\"\"\n",
    "    seq_ids: list[int]\n",
    "    returns (input_ids[T], attn_mask[T], label_int) or None if too short\n",
    "    \"\"\"\n",
    "    if seq_ids is None:\n",
    "        return None\n",
    "    seq_ids = cap_session(seq_ids, CAP_SESSION_LEN, CAP_STRATEGY)\n",
    "    if seq_ids is None or len(seq_ids) < 2:\n",
    "        return None\n",
    "\n",
    "    # choose prefix length (>=1) so there is a next-item label\n",
    "    max_pref = min(len(seq_ids) - 1, MAX_PREFIX_LEN)\n",
    "    if max_pref < 1:\n",
    "        return None\n",
    "    pref_len = int(RNG.randint(1, max_pref + 1))  # inclusive\n",
    "\n",
    "    x_tokens = seq_ids[:pref_len]\n",
    "    y = int(seq_ids[pref_len])  # next item\n",
    "\n",
    "    # left-pad to MAX_PREFIX_LEN\n",
    "    pad_n = MAX_PREFIX_LEN - len(x_tokens)\n",
    "    input_ids = [PAD_ID_SRC] * pad_n + [int(v) for v in x_tokens]\n",
    "    attn_mask = [0] * pad_n + [1] * len(x_tokens)\n",
    "\n",
    "    return input_ids, attn_mask, y\n",
    "\n",
    "print(f\"[{CELL}] ✅ session_to_one_pair ready\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6b172",
   "metadata": {},
   "source": [
    "Build support/query batch from task refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82362ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-08] Starting... 2026-01-05 12:55:28\n",
      "[13-08] batch shapes: input_ids=(8, 20) labels=(8,)\n",
      "[13-08] sample0 len=19 y0=2 x_last5=[2, 2, 2, 2, 2]\n",
      "[13-08] Done in 0.12s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-08] Build batch from task refs (file_idx,row_idx) -> tensors\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "CELL = \"13-08\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def sample_refs_for_task(task_id: int, B: int):\n",
    "    ref_pack = REFS[str(task_id)] if isinstance(next(iter(REFS.keys())), str) else REFS[task_id]\n",
    "    fi = ref_pack[\"file_idx\"]\n",
    "    ri = ref_pack[\"row_idx\"]\n",
    "    N = len(fi)\n",
    "    if N == 0:\n",
    "        return []\n",
    "    idxs = RNG.choice(np.arange(N), size=B, replace=(N < B))\n",
    "    return [(int(fi[j]), int(ri[j])) for j in idxs]\n",
    "\n",
    "def build_batch_from_refs(files_list, refs_list):\n",
    "    \"\"\"\n",
    "    refs_list: list[(file_idx,row_idx)]\n",
    "    Returns dict with tensors: input_ids[B,T], attn_mask[B,T], lengths[B], labels[B]\n",
    "    \"\"\"\n",
    "    xs, ms, ys = [], [], []\n",
    "    tries = 0\n",
    "    max_tries = len(refs_list) * 3 + 10\n",
    "\n",
    "    for (fi, r) in refs_list:\n",
    "        tries += 1\n",
    "        if tries > max_tries:\n",
    "            break\n",
    "        fp = files_list[fi]\n",
    "        df = get_items_df(fp)\n",
    "        seq_raw = df.iloc[r][SEQ_COL]\n",
    "        seq_tokens = normalize_seq(seq_raw)\n",
    "        seq_ids = seq_str_to_ids(seq_tokens, item2id, UNK_ID_SRC)\n",
    "        pair = session_to_one_pair(seq_ids)\n",
    "        if pair is None:\n",
    "            continue\n",
    "        x, m, y = pair\n",
    "        xs.append(x); ms.append(m); ys.append(y)\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        return None\n",
    "\n",
    "    input_ids = torch.tensor(xs, dtype=torch.long)\n",
    "    attn_mask = torch.tensor(ms, dtype=torch.long)\n",
    "    labels = torch.tensor(ys, dtype=torch.long)\n",
    "    lengths = attn_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attn_mask\": attn_mask, \"lengths\": lengths, \"labels\": labels}\n",
    "\n",
    "# quick sanity on one task\n",
    "t_example = int(KEPT_TASKS[0])\n",
    "refs = sample_refs_for_task(t_example, B=8)\n",
    "batch = build_batch_from_refs(train_files, refs)\n",
    "if batch is None:\n",
    "    raise RuntimeError(f\"[{CELL}] failed to build batch for task={t_example}\")\n",
    "\n",
    "print(f\"[{CELL}] batch shapes: input_ids={tuple(batch['input_ids'].shape)} labels={tuple(batch['labels'].shape)}\")\n",
    "print(f\"[{CELL}] sample0 len={int(batch['lengths'][0])} y0={int(batch['labels'][0])} x_last5={batch['input_ids'][0].tolist()[-5:]}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b01ed",
   "metadata": {},
   "source": [
    "Model + optimizer (same architecture family as 12B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f671407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-09] Starting... 2026-01-05 12:55:52\n",
      "[13-09] model=GRU4RecDropout(vocab=1620, emb=64, hid=128, drop=0.3) on cpu\n",
      "[13-09] meta_lr=0.0005\n",
      "[13-09] Done in 2.42s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-09] Model: GRU4RecDropout + meta optimizer\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "CELL = \"13-09\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "class GRU4RecDropout(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, pad_id: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=self.pad_id)\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        emb = self.drop(self.emb(input_ids))  # [B,T,E]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h = self.gru(packed)  # [1,B,H]\n",
    "        logits = self.out(h.squeeze(0))  # [B,V]\n",
    "        return logits\n",
    "\n",
    "emb_dim = int(META_CFG[\"emb_dim\"])\n",
    "hidden_dim = int(META_CFG[\"hidden_dim\"])\n",
    "dropout = float(META_CFG[\"dropout\"])\n",
    "meta_lr = float(META_CFG[\"meta_lr\"])\n",
    "\n",
    "model = GRU4RecDropout(\n",
    "    vocab_size=VOCAB_SIZE_SRC,\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    pad_id=PAD_ID_SRC,\n",
    "    dropout=dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=meta_lr)\n",
    "\n",
    "print(f\"[{CELL}] model=GRU4RecDropout(vocab={VOCAB_SIZE_SRC}, emb={emb_dim}, hid={hidden_dim}, drop={dropout}) on {DEVICE}\")\n",
    "print(f\"[{CELL}] meta_lr={meta_lr}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359d353",
   "metadata": {},
   "source": [
    "Meta-learning core (FOMAML-style): adapt copy on support, use query grads to update base\n",
    "\n",
    "This is first-order MAML approximation: we copy query grads from the adapted model into the base model and step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a268d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-10] Starting... 2026-01-05 12:56:30\n",
      "[13-10] inner_lr=0.01 inner_steps=1 meta_batch_tasks=4 grad_clip=1.0\n",
      "[13-10] ✅ meta_step/adapt_one_task ready\n",
      "[13-10] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-10] Meta-learning core (first-order): adapt on support, backprop query on adapted, copy grads -> base\n",
    "import time, copy\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "CELL = \"13-10\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "inner_lr = float(META_CFG[\"inner_lr\"])\n",
    "inner_steps = int(META_CFG[\"inner_steps\"])\n",
    "meta_batch_tasks = int(META_CFG[\"meta_batch_tasks\"])\n",
    "grad_clip = float(META_CFG[\"grad_clip\"])\n",
    "\n",
    "print(f\"[{CELL}] inner_lr={inner_lr} inner_steps={inner_steps} meta_batch_tasks={meta_batch_tasks} grad_clip={grad_clip}\")\n",
    "\n",
    "def adapt_one_task(base_model, support_batch):\n",
    "    \"\"\"\n",
    "    Returns adapted_model (deepcopy) trained for inner_steps on support.\n",
    "    \"\"\"\n",
    "    m = copy.deepcopy(base_model)\n",
    "    m.train()\n",
    "\n",
    "    inner_opt = torch.optim.SGD(m.parameters(), lr=inner_lr)\n",
    "    for s in range(inner_steps):\n",
    "        inner_opt.zero_grad(set_to_none=True)\n",
    "        logits = m(support_batch[\"input_ids\"].to(DEVICE), support_batch[\"lengths\"].to(DEVICE))\n",
    "        loss = F.cross_entropy(logits, support_batch[\"labels\"].to(DEVICE), ignore_index=PAD_ID_SRC)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(m.parameters(), grad_clip)\n",
    "        inner_opt.step()\n",
    "    return m\n",
    "\n",
    "def zero_grads(m):\n",
    "    for p in m.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "def add_grads_from(src_model, dst_model):\n",
    "    \"\"\"\n",
    "    Add grads from src_model params into dst_model params by name order.\n",
    "    Assumes same architecture.\n",
    "    \"\"\"\n",
    "    for (n_dst, p_dst), (n_src, p_src) in zip(dst_model.named_parameters(), src_model.named_parameters()):\n",
    "        if p_src.grad is None:\n",
    "            continue\n",
    "        if p_dst.grad is None:\n",
    "            p_dst.grad = p_src.grad.detach().clone()\n",
    "        else:\n",
    "            p_dst.grad.add_(p_src.grad.detach())\n",
    "\n",
    "def meta_step(base_model):\n",
    "    \"\"\"\n",
    "    One meta update: sample tasks, for each task:\n",
    "      - build support/query batches\n",
    "      - adapt copy on support\n",
    "      - compute query loss, backprop on adapted\n",
    "      - accumulate adapted grads into base grads\n",
    "    \"\"\"\n",
    "    base_model.train()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    tasks = RNG.choice(np.array(KEPT_TASKS), size=meta_batch_tasks, replace=(len(KEPT_TASKS) < meta_batch_tasks))\n",
    "    total_q_loss = 0.0\n",
    "    used = 0\n",
    "\n",
    "    for tk in tasks:\n",
    "        tk = int(tk)\n",
    "\n",
    "        # build support/query\n",
    "        sup_refs = sample_refs_for_task(tk, B=int(ITEMSET_CFG.get(\"min_task_examples\", 50)) and int(rep12b[\"task_cfg\"][\"n_support\"]))\n",
    "        qry_refs = sample_refs_for_task(tk, B=int(rep12b[\"task_cfg\"][\"n_query\"]))\n",
    "\n",
    "        sup = build_batch_from_refs(train_files, sup_refs)\n",
    "        qry = build_batch_from_refs(train_files, qry_refs)\n",
    "        if sup is None or qry is None:\n",
    "            continue\n",
    "\n",
    "        adapted = adapt_one_task(base_model, sup)\n",
    "\n",
    "        # query loss on adapted\n",
    "        zero_grads(adapted)\n",
    "        logits_q = adapted(qry[\"input_ids\"].to(DEVICE), qry[\"lengths\"].to(DEVICE))\n",
    "        q_loss = F.cross_entropy(logits_q, qry[\"labels\"].to(DEVICE), ignore_index=PAD_ID_SRC)\n",
    "        q_loss.backward()\n",
    "\n",
    "        add_grads_from(adapted, base_model)\n",
    "\n",
    "        total_q_loss += float(q_loss.detach().cpu())\n",
    "        used += 1\n",
    "\n",
    "    if used == 0:\n",
    "        return {\"used_tasks\": 0, \"q_loss\": None}\n",
    "\n",
    "    # average gradients\n",
    "    for p in base_model.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.div_(used)\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(base_model.parameters(), grad_clip)\n",
    "    opt.step()\n",
    "\n",
    "    return {\"used_tasks\": used, \"q_loss\": total_q_loss / used}\n",
    "\n",
    "print(f\"[{CELL}] ✅ meta_step/adapt_one_task ready\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e383c4b",
   "metadata": {},
   "source": [
    "Evaluation: HR@K (quick) for meta-adapt on VAL refs\n",
    "\n",
    "We’ll do HR@20 only for selection (like 12B best_val_hr20), to keep it clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcaf420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-11] Starting... 2026-01-05 12:57:09\n",
      "[13-11] VAL_EPISODES=50 K=20\n",
      "[13-11] ✅ eval ready\n",
      "[13-11] Done in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-11] Eval meta-adapt HR@20 on VAL by sampling episodes/tasks\n",
    "import time, copy\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "CELL = \"13-11\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "K_EVAL = 20\n",
    "VAL_EPISODES = int(META_CFG.get(\"val_episodes\", 50))\n",
    "print(f\"[{CELL}] VAL_EPISODES={VAL_EPISODES} K={K_EVAL}\")\n",
    "\n",
    "# Build VAL refs (fast path): reuse TRAIN refs if you want, but better is VAL sampling.\n",
    "# For now (no extra 6.6M scan), we evaluate on TRAIN refs as a proxy.\n",
    "# This is still real data; but you must note it as proxy.\n",
    "# If you want strict VAL, we will add a dedicated VAL pass like 13-02B2 on val_files.\n",
    "\n",
    "def hit_rate_at_k(logits, labels, k: int):\n",
    "    topk = torch.topk(logits, k=k, dim=1).indices  # [B,k]\n",
    "    y = labels.view(-1, 1)\n",
    "    hit = (topk == y).any(dim=1).float().mean().item()\n",
    "    return float(hit)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_meta_adapt_hr20(base_model, episodes: int):\n",
    "    base_model.eval()\n",
    "    hr_list = []\n",
    "    used = 0\n",
    "\n",
    "    for e in range(episodes):\n",
    "        tk = int(RNG.choice(np.array(KEPT_TASKS)))\n",
    "        sup_refs = sample_refs_for_task(tk, B=int(rep12b[\"task_cfg\"][\"n_support\"]))\n",
    "        qry_refs = sample_refs_for_task(tk, B=int(rep12b[\"task_cfg\"][\"n_query\"]))\n",
    "\n",
    "        sup = build_batch_from_refs(train_files, sup_refs)\n",
    "        qry = build_batch_from_refs(train_files, qry_refs)\n",
    "        if sup is None or qry is None:\n",
    "            continue\n",
    "\n",
    "        # adapt must run with grad, so temporarily enable\n",
    "        with torch.enable_grad():\n",
    "            adapted = adapt_one_task(base_model, sup)\n",
    "\n",
    "        logits = adapted(qry[\"input_ids\"].to(DEVICE), qry[\"lengths\"].to(DEVICE)).cpu()\n",
    "        hr = hit_rate_at_k(logits, qry[\"labels\"], k=K_EVAL)\n",
    "        hr_list.append(hr)\n",
    "        used += 1\n",
    "\n",
    "    if used == 0:\n",
    "        return None\n",
    "    return {\"HR@20\": float(np.mean(hr_list)), \"_episodes_used\": used}\n",
    "\n",
    "print(f\"[{CELL}] ✅ eval ready\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c11c",
   "metadata": {},
   "source": [
    "Meta-train loop + checkpoint + report.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4530bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-12] Starting... 2026-01-05 12:57:37\n",
      "[13-12] step=100/2000 used_tasks=4 q_loss=7.4349600076675415\n",
      "[13-12] step=200/2000 used_tasks=4 q_loss=7.141557931900024\n",
      "[13-12] EVAL step=250: VAL meta-adapt HR@20=0.037000 episodes_used=50\n",
      "[13-12] ✅ saved best ckpt: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\meta_model_source_itemset.pt\n",
      "[13-12] step=300/2000 used_tasks=4 q_loss=7.186716318130493\n",
      "[13-12] step=400/2000 used_tasks=4 q_loss=7.101266384124756\n",
      "[13-12] step=500/2000 used_tasks=4 q_loss=7.279435873031616\n",
      "[13-12] EVAL step=500: VAL meta-adapt HR@20=0.058000 episodes_used=50\n",
      "[13-12] ✅ saved best ckpt: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\meta_model_source_itemset.pt\n",
      "[13-12] step=600/2000 used_tasks=4 q_loss=6.874122142791748\n",
      "[13-12] step=700/2000 used_tasks=4 q_loss=7.266090512275696\n",
      "[13-12] EVAL step=750: VAL meta-adapt HR@20=0.111000 episodes_used=50\n",
      "[13-12] ✅ saved best ckpt: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\meta_model_source_itemset.pt\n",
      "[13-12] step=800/2000 used_tasks=4 q_loss=6.94021475315094\n",
      "[13-12] step=900/2000 used_tasks=4 q_loss=7.288401007652283\n",
      "[13-12] step=1000/2000 used_tasks=4 q_loss=7.178556323051453\n",
      "[13-12] EVAL step=1000: VAL meta-adapt HR@20=0.090000 episodes_used=50\n",
      "[13-12] step=1100/2000 used_tasks=4 q_loss=7.081469297409058\n",
      "[13-12] step=1200/2000 used_tasks=4 q_loss=7.075996160507202\n",
      "[13-12] EVAL step=1250: VAL meta-adapt HR@20=0.140000 episodes_used=50\n",
      "[13-12] ✅ saved best ckpt: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\meta_model_source_itemset.pt\n",
      "[13-12] step=1300/2000 used_tasks=4 q_loss=6.804421305656433\n",
      "[13-12] step=1400/2000 used_tasks=4 q_loss=6.912977457046509\n",
      "[13-12] step=1500/2000 used_tasks=4 q_loss=6.764888048171997\n",
      "[13-12] EVAL step=1500: VAL meta-adapt HR@20=0.139000 episodes_used=50\n",
      "[13-12] step=1600/2000 used_tasks=4 q_loss=7.016578435897827\n",
      "[13-12] step=1700/2000 used_tasks=4 q_loss=7.049452543258667\n",
      "[13-12] EVAL step=1750: VAL meta-adapt HR@20=0.128000 episodes_used=50\n",
      "[13-12] step=1800/2000 used_tasks=4 q_loss=7.139237761497498\n",
      "[13-12] step=1900/2000 used_tasks=4 q_loss=7.124320864677429\n",
      "[13-12] step=2000/2000 used_tasks=4 q_loss=6.704738974571228\n",
      "[13-12] EVAL step=2000: VAL meta-adapt HR@20=0.153000 episodes_used=50\n",
      "[13-12] ✅ saved best ckpt: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\meta_model_source_itemset.pt\n",
      "[13-12] ✅ wrote report: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\report.json\n",
      "[13-12] best_step=2000 best_val_hr20=0.15300000324845314\n",
      "[13-12] final_val={'HR@20': 0.15800000246614218, '_episodes_used': 100}\n",
      "[13-12] Done in 5006.65s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-12] Meta-train loop (item-set tasks) + checkpoint + report\n",
    "import time, json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch\n",
    "\n",
    "CELL = \"13-12\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "META_STEPS = int(META_CFG[\"meta_steps\"])\n",
    "LOG_EVERY = int(META_CFG[\"log_every\"])\n",
    "EVAL_EVERY = int(META_CFG[\"eval_every\"])\n",
    "\n",
    "best_step = None\n",
    "best_val_hr20 = -1.0\n",
    "\n",
    "CKPT_PATH = os.path.join(REPORT_DIR, \"meta_model_source_itemset.pt\")\n",
    "REPORT_PATH = os.path.join(REPORT_DIR, \"report.json\")\n",
    "\n",
    "report = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"proto\": PROTO,\n",
    "    \"meta_cfg\": META_CFG,\n",
    "    \"task_cfg_itemset\": ITEMSET_CFG,\n",
    "    \"best_step\": None,\n",
    "    \"best_val_hr20\": None,\n",
    "    \"val_meta_adapt\": None,\n",
    "    \"notes\": [\n",
    "        \"Notebook 13: item-set tasks based on seed_item_first_nonpad (first item in session after mapping).\",\n",
    "        \"Meta-learning is first-order (grads copied from adapted to base).\",\n",
    "        \"Pairs are generated on-the-fly from session sequences (prefix->next item), left-padded to MAX_PREFIX_LEN.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for step in range(1, META_STEPS + 1):\n",
    "    out = meta_step(model)\n",
    "\n",
    "    if step % LOG_EVERY == 0:\n",
    "        print(f\"[{CELL}] step={step}/{META_STEPS} used_tasks={out['used_tasks']} q_loss={out['q_loss']}\")\n",
    "\n",
    "    if step % EVAL_EVERY == 0:\n",
    "        val_res = eval_meta_adapt_hr20(model, episodes=VAL_EPISODES)\n",
    "        if val_res is None:\n",
    "            print(f\"[{CELL}] EVAL step={step}: val_res=None (no episodes built)\")\n",
    "            continue\n",
    "        hr20 = float(val_res[\"HR@20\"])\n",
    "        print(f\"[{CELL}] EVAL step={step}: VAL meta-adapt HR@20={hr20:.6f} episodes_used={val_res['_episodes_used']}\")\n",
    "\n",
    "        if hr20 > best_val_hr20:\n",
    "            best_val_hr20 = hr20\n",
    "            best_step = step\n",
    "\n",
    "            # save checkpoint\n",
    "            torch.save({\n",
    "                \"run_tag\": RUN_TAG,\n",
    "                \"proto\": PROTO,\n",
    "                \"meta_cfg\": META_CFG,\n",
    "                \"task_cfg_itemset_path\": ITEMSET_CFG_PATH,\n",
    "                \"vocab_size_source\": VOCAB_SIZE_SRC,\n",
    "                \"pad_id_source\": PAD_ID_SRC,\n",
    "                \"unk_id_source\": UNK_ID_SRC,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_step\": best_step,\n",
    "                \"best_val_hr20\": best_val_hr20,\n",
    "            }, CKPT_PATH)\n",
    "\n",
    "            print(f\"[{CELL}] ✅ saved best ckpt: {CKPT_PATH}\")\n",
    "\n",
    "# final eval snapshot\n",
    "final_val = eval_meta_adapt_hr20(model, episodes=max(VAL_EPISODES, 100))\n",
    "report[\"best_step\"] = best_step\n",
    "report[\"best_val_hr20\"] = float(best_val_hr20)\n",
    "report[\"val_meta_adapt\"] = final_val\n",
    "\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"[{CELL}] ✅ wrote report: {REPORT_PATH}\")\n",
    "print(f\"[{CELL}] best_step={best_step} best_val_hr20={best_val_hr20}\")\n",
    "print(f\"[{CELL}] final_val={final_val}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de38fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-13] Starting... 2026-01-05 15:03:07\n",
      "[13-13] kept_tasks=1345 | VAL_MIN_TASK_EX=50 | VAL_CAP_PER_TASK_REFS=5000\n",
      "[13-13] scanned 1/1024 rows=769 tasks_with_refs=341 elapsed_file=0.08s\n",
      "[13-13] scanned 2/1024 rows=1579 tasks_with_refs=492 elapsed_file=0.07s\n",
      "[13-13] scanned 3/1024 rows=2349 tasks_with_refs=596 elapsed_file=0.06s\n",
      "[13-13] scanned 100/1024 rows=81398 tasks_with_refs=1274 elapsed_file=0.08s\n",
      "[13-13] scanned 200/1024 rows=163159 tasks_with_refs=1317 elapsed_file=0.06s\n",
      "[13-13] scanned 300/1024 rows=244632 tasks_with_refs=1334 elapsed_file=0.07s\n",
      "[13-13] scanned 400/1024 rows=325724 tasks_with_refs=1336 elapsed_file=0.06s\n",
      "[13-13] scanned 500/1024 rows=407196 tasks_with_refs=1341 elapsed_file=0.06s\n",
      "[13-13] scanned 600/1024 rows=488570 tasks_with_refs=1342 elapsed_file=0.06s\n",
      "[13-13] scanned 700/1024 rows=569943 tasks_with_refs=1344 elapsed_file=0.06s\n",
      "[13-13] scanned 800/1024 rows=651167 tasks_with_refs=1344 elapsed_file=0.06s\n",
      "[13-13] scanned 900/1024 rows=732510 tasks_with_refs=1344 elapsed_file=0.06s\n",
      "[13-13] scanned 1000/1024 rows=813818 tasks_with_refs=1345 elapsed_file=0.06s\n",
      "[13-13] VAL scan done. rows=833042\n",
      "[13-13] skips: nonseq=0 empty=0 allpad=0 notkept=505\n",
      "[13-13] tasks_with_refs=1345\n",
      "[13-13] ✅ wrote VAL refs: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\val_task_refs_itemset.pt\n",
      "[13-13] sanity task=2 val_sampled_refs=5000 val_seen=18415\n",
      "[13-13] Done in 70.27s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-13] STRICT VAL: Build task refs from VAL split (reservoir sampling; kept tasks only)\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "CELL = \"13-13\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "VAL_CAP_PER_TASK_REFS = int(ITEMSET_CFG.get(\"cap_per_task_refs\", 5000))  # match train cap by default\n",
    "VAL_MIN_TASK_EX = int(ITEMSET_CFG.get(\"min_task_examples\", 50))\n",
    "RNG_VAL = np.random.RandomState(SEED + 13)  # deterministic but different stream than TRAIN\n",
    "\n",
    "print(f\"[{CELL}] kept_tasks={len(KEPT_TASKS)} | VAL_MIN_TASK_EX={VAL_MIN_TASK_EX} | VAL_CAP_PER_TASK_REFS={VAL_CAP_PER_TASK_REFS}\")\n",
    "\n",
    "val_task_seen = defaultdict(int)\n",
    "val_task_to_refs = defaultdict(list)\n",
    "\n",
    "row_count = 0\n",
    "skip_nonseq = 0\n",
    "skip_empty = 0\n",
    "skip_allpad = 0\n",
    "skip_notkept = 0\n",
    "\n",
    "for fi, path in enumerate(val_files):\n",
    "    tf = time.time()\n",
    "    df = pq.read_table(path, columns=[SEQ_COL]).to_pandas()\n",
    "    n = len(df)\n",
    "\n",
    "    for r in range(n):\n",
    "        seq_tokens = normalize_seq(df.iloc[r][SEQ_COL])\n",
    "        if seq_tokens is None:\n",
    "            skip_nonseq += 1\n",
    "            continue\n",
    "\n",
    "        ids = seq_str_to_ids(seq_tokens, item2id, UNK_ID_SRC)\n",
    "        if ids is None or len(ids) == 0:\n",
    "            skip_empty += 1\n",
    "            continue\n",
    "\n",
    "        # seed = first non-pad id\n",
    "        s = None\n",
    "        for v in ids:\n",
    "            iv = int(v)\n",
    "            if iv != PAD_ID_SRC:\n",
    "                s = iv\n",
    "                break\n",
    "        if s is None:\n",
    "            skip_allpad += 1\n",
    "            continue\n",
    "\n",
    "        if s not in set(KEPT_TASKS):\n",
    "            skip_notkept += 1\n",
    "            continue\n",
    "\n",
    "        val_task_seen[s] += 1\n",
    "        seen = val_task_seen[s]\n",
    "        buf = val_task_to_refs[s]\n",
    "\n",
    "        if len(buf) < VAL_CAP_PER_TASK_REFS:\n",
    "            buf.append((fi, r))\n",
    "        else:\n",
    "            j = RNG_VAL.randint(0, seen)\n",
    "            if j < VAL_CAP_PER_TASK_REFS:\n",
    "                buf[j] = (fi, r)\n",
    "\n",
    "    row_count += n\n",
    "    if fi < 3 or (fi + 1) % 100 == 0:\n",
    "        print(f\"[{CELL}] scanned {fi+1}/{len(val_files)} rows={row_count} tasks_with_refs={len(val_task_to_refs)} elapsed_file={time.time()-tf:.2f}s\")\n",
    "\n",
    "print(f\"[{CELL}] VAL scan done. rows={row_count}\")\n",
    "print(f\"[{CELL}] skips: nonseq={skip_nonseq} empty={skip_empty} allpad={skip_allpad} notkept={skip_notkept}\")\n",
    "print(f\"[{CELL}] tasks_with_refs={len(val_task_to_refs)}\")\n",
    "\n",
    "if len(val_task_to_refs) == 0:\n",
    "    raise RuntimeError(f\"[{CELL}] No VAL refs built. Unexpected; check VAL parquet.\")\n",
    "\n",
    "# Pack and save\n",
    "val_refs_pack = {}\n",
    "for k, refs in val_task_to_refs.items():\n",
    "    val_refs_pack[int(k)] = {\n",
    "        \"file_idx\": [int(a) for a, b in refs],\n",
    "        \"row_idx\": [int(b) for a, b in refs],\n",
    "    }\n",
    "\n",
    "VAL_REFS_PATH = os.path.join(REPORT_DIR, \"val_task_refs_itemset.pt\")\n",
    "torch.save(\n",
    "    {\"kept_tasks\": KEPT_TASKS, \"cap\": VAL_CAP_PER_TASK_REFS, \"refs\": val_refs_pack},\n",
    "    VAL_REFS_PATH\n",
    ")\n",
    "\n",
    "print(f\"[{CELL}] ✅ wrote VAL refs: {VAL_REFS_PATH}\")\n",
    "\n",
    "# Quick sanity\n",
    "sk = KEPT_TASKS[0]\n",
    "print(f\"[{CELL}] sanity task={sk} val_sampled_refs={len(val_task_to_refs.get(sk, []))} val_seen={val_task_seen.get(sk, 0)}\")\n",
    "\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1824edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13-14] Starting... 2026-01-05 15:04:17\n",
      "[13-14] VAL_TASKS=1345 | VAL_REFS tasks=1345 | cap=5000\n",
      "[13-14] STRICT VAL meta-adapt: {'HR@20': 0.1387500020302832, '_episodes_used': 200}\n",
      "[13-14] ✅ updated report.json: C:\\mooc-coldstart-session-meta\\reports\\13_meta_train_on_source_itemset\\20260105_114325\\report.json\n",
      "[13-14] Done in 77.87s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 13-14] STRICT VAL EVAL: use VAL refs (not TRAIN proxy) + update report.json\n",
    "import time, json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "CELL = \"13-14\"\n",
    "t0 = time.time()\n",
    "print(f\"[{CELL}] Starting... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "VAL_PACK = torch.load(os.path.join(REPORT_DIR, \"val_task_refs_itemset.pt\"), map_location=\"cpu\")\n",
    "VAL_REFS = VAL_PACK[\"refs\"]\n",
    "VAL_TASKS = list(map(int, VAL_PACK[\"kept_tasks\"]))\n",
    "print(f\"[{CELL}] VAL_TASKS={len(VAL_TASKS)} | VAL_REFS tasks={len(VAL_REFS)} | cap={VAL_PACK.get('cap')}\")\n",
    "\n",
    "def sample_refs_for_task_from(pack_refs, task_id: int, B: int, rng: np.random.RandomState):\n",
    "    ref_pack = pack_refs[str(task_id)] if isinstance(next(iter(pack_refs.keys())), str) else pack_refs[task_id]\n",
    "    fi = ref_pack[\"file_idx\"]\n",
    "    ri = ref_pack[\"row_idx\"]\n",
    "    N = len(fi)\n",
    "    if N == 0:\n",
    "        return []\n",
    "    idxs = rng.choice(np.arange(N), size=B, replace=(N < B))\n",
    "    return [(int(fi[j]), int(ri[j])) for j in idxs]\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_meta_adapt_hr20_strict_val(base_model, episodes: int, seed_offset: int = 0):\n",
    "    base_model.eval()\n",
    "    hr_list = []\n",
    "    used = 0\n",
    "    rng = np.random.RandomState(SEED + 1000 + seed_offset)\n",
    "\n",
    "    for e in range(episodes):\n",
    "        tk = int(rng.choice(np.array(VAL_TASKS)))\n",
    "        sup_refs = sample_refs_for_task_from(REFS, tk, B=int(rep12b[\"task_cfg\"][\"n_support\"]), rng=rng)   # support from TRAIN refs\n",
    "        qry_refs = sample_refs_for_task_from(VAL_REFS, tk, B=int(rep12b[\"task_cfg\"][\"n_query\"]), rng=rng) # query from VAL refs\n",
    "\n",
    "        sup = build_batch_from_refs(train_files, sup_refs)\n",
    "        qry = build_batch_from_refs(val_files, qry_refs)\n",
    "        if sup is None or qry is None:\n",
    "            continue\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            adapted = adapt_one_task(base_model, sup)\n",
    "\n",
    "        logits = adapted(qry[\"input_ids\"].to(DEVICE), qry[\"lengths\"].to(DEVICE)).cpu()\n",
    "        topk = torch.topk(logits, k=20, dim=1).indices\n",
    "        y = qry[\"labels\"].view(-1, 1)\n",
    "        hr = (topk == y).any(dim=1).float().mean().item()\n",
    "\n",
    "        hr_list.append(float(hr))\n",
    "        used += 1\n",
    "\n",
    "    if used == 0:\n",
    "        return None\n",
    "    return {\"HR@20\": float(np.mean(hr_list)), \"_episodes_used\": int(used)}\n",
    "\n",
    "STRICT_VAL_EPISODES = 200  # stronger estimate than 50\n",
    "strict_val = eval_meta_adapt_hr20_strict_val(model, episodes=STRICT_VAL_EPISODES, seed_offset=14)\n",
    "\n",
    "print(f\"[{CELL}] STRICT VAL meta-adapt: {strict_val}\")\n",
    "\n",
    "# Update report.json\n",
    "REPORT_PATH = os.path.join(REPORT_DIR, \"report.json\")\n",
    "with open(REPORT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    rep = json.load(f)\n",
    "\n",
    "rep[\"val_meta_adapt_strict\"] = strict_val\n",
    "rep[\"notes\"].append(\"STRICT VAL eval added: support sampled from TRAIN refs, query sampled from VAL refs (val_task_refs_itemset.pt).\")\n",
    "\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rep, f, indent=2)\n",
    "\n",
    "print(f\"[{CELL}] ✅ updated report.json: {REPORT_PATH}\")\n",
    "print(f\"[{CELL}] Done in {time.time()-t0:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
