{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef80d86",
   "metadata": {},
   "source": [
    "Imports + env info (Session-KNN baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29de88ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-00] Imports OK\n",
      "[08-00] torch: 2.9.1+cpu\n",
      "[08-00] pandas: 2.3.3\n",
      "[08-00] numpy: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-00] Imports + env info (Session-KNN baseline)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"[08-00] Imports OK\")\n",
    "print(\"[08-00] torch:\", torch.__version__)\n",
    "print(\"[08-00] pandas:\", pd.__version__)\n",
    "print(\"[08-00] numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4750da",
   "metadata": {},
   "source": [
    "Locate repo root + run tags + load protocol/config artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a43fb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[08-01] RUN_TAG: 20260102_140713\n",
      "[08-01] Loaded dataloader_config keys: ['target', 'source', 'protocol']\n",
      "[08-01] Loaded sanity_metrics keys: ['run_tag_target', 'run_tag_source', 'created_at', 'target', 'source', 'notes']\n",
      "[08-01] Loaded session_gap_thresholds keys: ['generated_from_run_tag', 'generated_at', 'target', 'source', 'decision_notes']\n",
      "[08-01] ✅ Session gaps confirmed: target=30m, source=10m\n",
      "[08-01] Protocol from 06:\n",
      "  K_LIST: [5, 10, 20]\n",
      "  MAX_PREFIX_LEN: 20\n",
      "  CAP_ENABLED: True\n",
      "  CAP_SESSION_LEN: 200\n",
      "  CAP_STRATEGY: take_last\n",
      "\n",
      "[08-01] CHECKPOINT A\n",
      "Confirm all three JSON files loaded and session gaps asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-01] Locate repo root + run tags + load protocol/config artifacts\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists() and (p / \"meta.json\").exists():\n",
    "            return p\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not locate repo root (expected PROJECT_STATE.md).\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
    "print(\"[08-01] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[08-01] RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "# Fixed upstream run tags (do NOT change)\n",
    "TARGET_TAG = \"20251229_163357\"\n",
    "SOURCE_TAG = \"20251229_232834\"\n",
    "\n",
    "def load_json(path: Path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "cfg_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"dataloader_config_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "sanity_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"sanity_metrics_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "gaps_path_repo = REPO_ROOT / \"data/processed/normalized_events\" / \"session_gap_thresholds.json\"\n",
    "\n",
    "dataloader_cfg = load_json(cfg_path_repo)\n",
    "sanity_metrics = load_json(sanity_path_repo)\n",
    "session_gaps = load_json(gaps_path_repo)\n",
    "\n",
    "print(\"[08-01] Loaded dataloader_config keys:\", list(dataloader_cfg.keys()))\n",
    "print(\"[08-01] Loaded sanity_metrics keys:\", list(sanity_metrics.keys()))\n",
    "print(\"[08-01] Loaded session_gap_thresholds keys:\", list(session_gaps.keys()))\n",
    "\n",
    "# Enforce fixed decisions\n",
    "assert session_gaps[\"target\"][\"primary_threshold_seconds\"] == 1800, \"Target gap must be 30m (1800s).\"\n",
    "assert session_gaps[\"source\"][\"primary_threshold_seconds\"] == 600, \"Source gap must be 10m (600s).\"\n",
    "print(\"[08-01] ✅ Session gaps confirmed: target=30m, source=10m\")\n",
    "\n",
    "proto = dataloader_cfg[\"protocol\"]\n",
    "K_LIST = [5, 10, 20]\n",
    "MAX_K = max(K_LIST)\n",
    "\n",
    "MAX_PREFIX_LEN = int(proto[\"max_prefix_len\"])\n",
    "CAP_ENABLED = bool(proto[\"source_long_session_policy\"][\"enabled\"])\n",
    "CAP_SESSION_LEN = int(proto[\"source_long_session_policy\"][\"cap_session_len\"])\n",
    "CAP_STRATEGY = str(proto[\"source_long_session_policy\"][\"cap_strategy\"])\n",
    "\n",
    "print(\"[08-01] Protocol from 06:\")\n",
    "print(\"  K_LIST:\", K_LIST)\n",
    "print(\"  MAX_PREFIX_LEN:\", MAX_PREFIX_LEN)\n",
    "print(\"  CAP_ENABLED:\", CAP_ENABLED)\n",
    "print(\"  CAP_SESSION_LEN:\", CAP_SESSION_LEN)\n",
    "print(\"  CAP_STRATEGY:\", CAP_STRATEGY)\n",
    "\n",
    "print(\"\\n[08-01] CHECKPOINT A\")\n",
    "print(\"Confirm all three JSON files loaded and session gaps asserts passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b158668",
   "metadata": {},
   "source": [
    "Resolve artifact paths (target tensors + source sequences) + existence checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81024094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-02] ✅ All required artifacts exist\n",
      "\n",
      "[08-02] CHECKPOINT B\n",
      "If any artifact missing, STOP and paste the error.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-02] Resolve artifact paths (target tensors + source sequences) + existence checks\n",
    "\n",
    "def must_exist(p: Path, label: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "    return p\n",
    "\n",
    "# TARGET tensors (05B output)\n",
    "TARGET_TENSOR_DIR = REPO_ROOT / \"data/processed/tensor_target\"\n",
    "target_train_pt = TARGET_TENSOR_DIR / f\"target_tensor_train_{TARGET_TAG}.pt\"\n",
    "target_val_pt   = TARGET_TENSOR_DIR / f\"target_tensor_val_{TARGET_TAG}.pt\"\n",
    "target_test_pt  = TARGET_TENSOR_DIR / f\"target_tensor_test_{TARGET_TAG}.pt\"\n",
    "target_vocab_json = TARGET_TENSOR_DIR / f\"target_vocab_items_{TARGET_TAG}.json\"\n",
    "target_tensor_meta_json = TARGET_TENSOR_DIR / f\"target_tensor_metadata_{TARGET_TAG}.json\"\n",
    "\n",
    "# SOURCE sequences (05C output)\n",
    "SOURCE_SEQ_ROOT = REPO_ROOT / \"data/processed/session_sequences\" / f\"source_sessions_{SOURCE_TAG}\"\n",
    "source_train_dir = SOURCE_SEQ_ROOT / \"train\"\n",
    "source_val_dir   = SOURCE_SEQ_ROOT / \"val\"\n",
    "source_test_dir  = SOURCE_SEQ_ROOT / \"test\"\n",
    "source_vocab_json = SOURCE_SEQ_ROOT / f\"source_vocab_items_{SOURCE_TAG}.json\"\n",
    "\n",
    "# checks\n",
    "for p, lbl in [\n",
    "    (target_train_pt, \"target_train_pt\"),\n",
    "    (target_val_pt, \"target_val_pt\"),\n",
    "    (target_test_pt, \"target_test_pt\"),\n",
    "    (target_vocab_json, \"target_vocab_json\"),\n",
    "    (target_tensor_meta_json, \"target_tensor_meta_json\"),\n",
    "    (source_train_dir, \"source_train_dir\"),\n",
    "    (source_val_dir, \"source_val_dir\"),\n",
    "    (source_test_dir, \"source_test_dir\"),\n",
    "    (source_vocab_json, \"source_vocab_json\"),\n",
    "]:\n",
    "    must_exist(p, lbl)\n",
    "\n",
    "print(\"[08-02] ✅ All required artifacts exist\")\n",
    "\n",
    "print(\"\\n[08-02] CHECKPOINT B\")\n",
    "print(\"If any artifact missing, STOP and paste the error.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d17ee",
   "metadata": {},
   "source": [
    "Torch loader (PyTorch 2.6+) + vocab loading + infer vocab sizes + PAD/UNK resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a954a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-03] TARGET: vocab_size from max(vocab values)+1 (token->id) = 747\n",
      "[08-03] SOURCE: vocab_size from key 'vocab_size' = 1620\n",
      "[08-03] PAD_ID_TARGET: 0 | UNK_ID_TARGET: 1\n",
      "[08-03] PAD_ID_SOURCE: 0 | UNK_ID_SOURCE: 1\n",
      "[08-03] source_token_to_id size: 1620\n",
      "[08-03] ✅ Vocab + mapping ready\n",
      "\n",
      "[08-03] CHECKPOINT C\n",
      "Confirm vocab sizes + PAD/UNK printed as expected before building KNN indexes.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-03] Torch loader (PyTorch 2.6+) + vocab loading + infer vocab sizes + PAD/UNK resolve\n",
    "\n",
    "def torch_load_repo_artifact(path, map_location=\"cpu\"):\n",
    "    path = str(path)\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=map_location, weights_only=False)\n",
    "        print(f\"[08-03] torch.load OK (weights_only=False): {path}\")\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        obj = torch.load(path, map_location=map_location)\n",
    "        print(f\"[08-03] torch.load OK (no weights_only arg): {path}\")\n",
    "        return obj\n",
    "\n",
    "target_vocab = load_json(target_vocab_json)\n",
    "source_vocab = load_json(source_vocab_json)\n",
    "\n",
    "def infer_vocab_size(vocab: dict, name: str) -> int:\n",
    "    for k in [\"vocab_size\", \"n_items\", \"num_items\", \"size\"]:\n",
    "        if k in vocab:\n",
    "            vs = int(vocab[k])\n",
    "            print(f\"[08-03] {name}: vocab_size from key '{k}' = {vs}\")\n",
    "            return vs\n",
    "\n",
    "    if \"vocab\" in vocab and isinstance(vocab[\"vocab\"], dict):\n",
    "        d = vocab[\"vocab\"]\n",
    "        if len(d) == 0:\n",
    "            print(f\"[08-03] {name}: vocab empty -> 0\")\n",
    "            return 0\n",
    "        sample_k = next(iter(d.keys()))\n",
    "        sample_v = d[sample_k]\n",
    "        if isinstance(sample_v, int):\n",
    "            ids = list(d.values())\n",
    "            vs = max(ids) + 1 if len(ids) else 0\n",
    "            print(f\"[08-03] {name}: vocab_size from max(vocab values)+1 (token->id) = {vs}\")\n",
    "            return vs\n",
    "        try:\n",
    "            keys_int = [int(k) for k in d.keys()]\n",
    "            vs = max(keys_int) + 1 if len(keys_int) else 0\n",
    "            print(f\"[08-03] {name}: vocab_size from max(vocab keys)+1 (id->token) = {vs}\")\n",
    "            return vs\n",
    "        except Exception:\n",
    "            vs = len(d)\n",
    "            print(f\"[08-03] {name}: vocab_size fallback len(vocab) = {vs}\")\n",
    "            return vs\n",
    "\n",
    "    if \"item2id\" in vocab and isinstance(vocab[\"item2id\"], dict):\n",
    "        ids = list(vocab[\"item2id\"].values())\n",
    "        vs = max(ids) + 1 if len(ids) else 0\n",
    "        print(f\"[08-03] {name}: vocab_size from max(item2id values)+1 = {vs}\")\n",
    "        return vs\n",
    "\n",
    "    if \"items\" in vocab and isinstance(vocab[\"items\"], list):\n",
    "        vs = len(vocab[\"items\"])\n",
    "        print(f\"[08-03] {name}: vocab_size from len(items) = {vs}\")\n",
    "        return vs\n",
    "\n",
    "    raise KeyError(f\"[08-03] {name}: Could not infer vocab_size. Keys={list(vocab.keys())}\")\n",
    "\n",
    "vocab_size_target = infer_vocab_size(target_vocab, \"TARGET\")\n",
    "vocab_size_source = infer_vocab_size(source_vocab, \"SOURCE\")\n",
    "\n",
    "# Resolve PAD/UNK for target (target vocab uses \"pad_token\"/\"unk_token\" + \"vocab\" token->id)\n",
    "def get_special_id(vocab_obj: dict, token_key: str, fallback: int, name: str) -> int:\n",
    "    tok = vocab_obj.get(token_key, None)\n",
    "    if tok is None:\n",
    "        print(f\"[08-03] {name}: missing {token_key}, fallback id={fallback}\")\n",
    "        return fallback\n",
    "    mapping = vocab_obj.get(\"vocab\", {})\n",
    "    if isinstance(mapping, dict) and tok in mapping and isinstance(mapping[tok], int):\n",
    "        return int(mapping[tok])\n",
    "    print(f\"[08-03] {name}: could not resolve {token_key}='{tok}' in vocab mapping, fallback id={fallback}\")\n",
    "    return fallback\n",
    "\n",
    "PAD_ID_TARGET = get_special_id(target_vocab, \"pad_token\", 0, \"TARGET\")\n",
    "UNK_ID_TARGET = get_special_id(target_vocab, \"unk_token\", 1, \"TARGET\")\n",
    "\n",
    "# Resolve PAD/UNK for source (explicit)\n",
    "PAD_ID_SOURCE = int(source_vocab.get(\"pad_id\", 0))\n",
    "UNK_ID_SOURCE = int(source_vocab.get(\"unk_id\", 1))\n",
    "\n",
    "print(\"[08-03] PAD_ID_TARGET:\", PAD_ID_TARGET, \"| UNK_ID_TARGET:\", UNK_ID_TARGET)\n",
    "print(\"[08-03] PAD_ID_SOURCE:\", PAD_ID_SOURCE, \"| UNK_ID_SOURCE:\", UNK_ID_SOURCE)\n",
    "\n",
    "assert PAD_ID_TARGET == 0, \"Target PAD must be 0.\"\n",
    "assert PAD_ID_SOURCE == 0, \"Source PAD must be 0.\"\n",
    "\n",
    "# Build source token->id mapping (strings -> ids)\n",
    "def build_token_to_id(vocab_obj: dict) -> dict:\n",
    "    if \"vocab\" in vocab_obj and isinstance(vocab_obj[\"vocab\"], dict):\n",
    "        d = vocab_obj[\"vocab\"]\n",
    "        if len(d) > 0:\n",
    "            sample_k = next(iter(d.keys()))\n",
    "            if isinstance(d[sample_k], int):\n",
    "                return d\n",
    "            try:\n",
    "                _ = int(sample_k)\n",
    "                return {v: int(k) for k, v in d.items()}\n",
    "            except Exception:\n",
    "                pass\n",
    "    if \"item2id\" in vocab_obj and isinstance(vocab_obj[\"item2id\"], dict):\n",
    "        return vocab_obj[\"item2id\"]\n",
    "    if \"items\" in vocab_obj and isinstance(vocab_obj[\"items\"], list):\n",
    "        return {tok: i for i, tok in enumerate(vocab_obj[\"items\"])}\n",
    "    raise KeyError(f\"[08-03] Could not build token_to_id. Keys={list(vocab_obj.keys())}\")\n",
    "\n",
    "source_token_to_id = build_token_to_id(source_vocab)\n",
    "print(\"[08-03] source_token_to_id size:\", len(source_token_to_id))\n",
    "\n",
    "# Map a source seq (np.ndarray/list of strings) to ids\n",
    "def map_source_seq_to_ids(seq) -> np.ndarray:\n",
    "    if seq is None:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    if isinstance(seq, np.ndarray):\n",
    "        seq_list = seq.tolist()\n",
    "    else:\n",
    "        seq_list = list(seq)\n",
    "    if len(seq_list) == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    # already ints?\n",
    "    if isinstance(seq_list[0], (int, np.integer)):\n",
    "        return np.asarray(seq_list, dtype=np.int64)\n",
    "    out = np.fromiter((source_token_to_id.get(tok, UNK_ID_SOURCE) for tok in seq_list), dtype=np.int64)\n",
    "    return out\n",
    "\n",
    "print(\"[08-03] ✅ Vocab + mapping ready\")\n",
    "\n",
    "print(\"\\n[08-03] CHECKPOINT C\")\n",
    "print(\"Confirm vocab sizes + PAD/UNK printed as expected before building KNN indexes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43561068",
   "metadata": {},
   "source": [
    "Metrics (reuse 06 protocol): HR/MRR/NDCG @ K={5,10,20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a6782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-04] ✅ Metric functions ready\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-04] Metrics (reuse 06 protocol): HR/MRR/NDCG @ K={5,10,20}\n",
    "\n",
    "def init_metrics():\n",
    "    return {f\"{m}@{k}\": 0.0 for m in [\"HR\", \"MRR\", \"NDCG\"] for k in K_LIST}\n",
    "\n",
    "def update_metrics_from_rank(metrics: dict, rank0: int | None):\n",
    "    if rank0 is None:\n",
    "        return\n",
    "    r = rank0 + 1\n",
    "    for k in K_LIST:\n",
    "        if r <= k:\n",
    "            metrics[f\"HR@{k}\"] += 1.0\n",
    "            metrics[f\"MRR@{k}\"] += 1.0 / r\n",
    "            metrics[f\"NDCG@{k}\"] += 1.0 / math.log2(r + 1.0)\n",
    "\n",
    "def finalize_metrics(metrics: dict, n: int) -> dict:\n",
    "    return {k: (float(v / n) if n > 0 else 0.0) for k, v in metrics.items()}\n",
    "\n",
    "print(\"[08-04] ✅ Metric functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5421d7a",
   "metadata": {},
   "source": [
    "Session-KNN core (inverted index) + scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079a6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-05] SKNN_CFG: {'k_neighbors': 200, 'candidate_sessions_cap': 50000, 'pos_weighting': 'inv_dist', 'exclude_pad': True}\n",
      "[08-05] ✅ Session-KNN helpers ready\n",
      "\n",
      "[08-05] CHECKPOINT D\n",
      "If you want to adjust SKNN_CFG (k_neighbors/caps/pos_weighting), do it now before building indexes.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-05] Session-KNN core (inverted index) + scoring\n",
    "# Similarity: cosine on binary sets: sim = overlap / sqrt(|Q|*|S|)\n",
    "# Scoring: sum(sim * pos_weight) over neighbor sessions and their items\n",
    "# pos_weight: 1/(distance_from_end) where end distance = 1..L\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "SKNN_CFG = {\n",
    "    \"k_neighbors\": 200,          # neighbors to use in scoring\n",
    "    \"candidate_sessions_cap\": 50000,  # safety cap for candidate sessions per query\n",
    "    \"pos_weighting\": \"inv_dist\", # inv_dist | none\n",
    "    \"exclude_pad\": True,\n",
    "}\n",
    "\n",
    "print(\"[08-05] SKNN_CFG:\", SKNN_CFG)\n",
    "\n",
    "def build_inverted_index(sessions_items: list[np.ndarray], pad_id: int) -> tuple[list[int], dict[int, list[int]]]:\n",
    "    \"\"\"\n",
    "    sessions_items: list of np.ndarray[int] per session/prefix\n",
    "    Returns:\n",
    "      - session_lens: list[int]\n",
    "      - inv: dict[item_id] -> list[session_idx]\n",
    "    \"\"\"\n",
    "    inv = defaultdict(list)\n",
    "    session_lens = []\n",
    "    for s_idx, arr in enumerate(sessions_items):\n",
    "        if arr is None or arr.size == 0:\n",
    "            session_lens.append(0)\n",
    "            continue\n",
    "        # unique items for similarity postings\n",
    "        uniq = np.unique(arr)\n",
    "        if pad_id in uniq:\n",
    "            uniq = uniq[uniq != pad_id]\n",
    "        session_lens.append(int(len(uniq)))\n",
    "        for it in uniq:\n",
    "            inv[int(it)].append(s_idx)\n",
    "    return session_lens, inv\n",
    "\n",
    "def score_items_from_neighbors(\n",
    "    query_items: np.ndarray,\n",
    "    sessions_items: list[np.ndarray],\n",
    "    session_lens: list[int],\n",
    "    inv: dict[int, list[int]],\n",
    "    k_neighbors: int,\n",
    "    candidate_sessions_cap: int,\n",
    "    pos_weighting: str,\n",
    "    pad_id: int,\n",
    ") -> dict[int, float]:\n",
    "    \"\"\"\n",
    "    Returns item->score for query next-item ranking.\n",
    "    \"\"\"\n",
    "    # query unique items for similarity\n",
    "    q = np.unique(query_items)\n",
    "    q = q[q != pad_id]\n",
    "    if q.size == 0:\n",
    "        return {}\n",
    "\n",
    "    # accumulate overlap counts using inverted index\n",
    "    overlap = defaultdict(int)\n",
    "    candidates = 0\n",
    "\n",
    "    for it in q:\n",
    "        posting = inv.get(int(it), [])\n",
    "        # iterate through postings\n",
    "        for s_idx in posting:\n",
    "            overlap[s_idx] += 1\n",
    "            candidates += 1\n",
    "            if candidates >= candidate_sessions_cap:\n",
    "                break\n",
    "        if candidates >= candidate_sessions_cap:\n",
    "            break\n",
    "\n",
    "    if len(overlap) == 0:\n",
    "        return {}\n",
    "\n",
    "    # compute similarity for candidate sessions\n",
    "    q_len = float(len(q))\n",
    "    sims = []\n",
    "    for s_idx, inter in overlap.items():\n",
    "        s_len = float(session_lens[s_idx])\n",
    "        if s_len <= 0:\n",
    "            continue\n",
    "        sim = float(inter) / math.sqrt(q_len * s_len)\n",
    "        if sim > 0:\n",
    "            sims.append((sim, s_idx))\n",
    "\n",
    "    if len(sims) == 0:\n",
    "        return {}\n",
    "\n",
    "    sims.sort(reverse=True, key=lambda x: x[0])\n",
    "    sims = sims[:k_neighbors]\n",
    "\n",
    "    # score items from neighbor sessions\n",
    "    scores = defaultdict(float)\n",
    "    for sim, s_idx in sims:\n",
    "        items = sessions_items[s_idx]\n",
    "        if items is None or items.size == 0:\n",
    "            continue\n",
    "        # exclude PAD\n",
    "        if pad_id is not None:\n",
    "            items = items[items != pad_id]\n",
    "        if items.size == 0:\n",
    "            continue\n",
    "\n",
    "        if pos_weighting == \"none\":\n",
    "            for it in items:\n",
    "                scores[int(it)] += sim\n",
    "        elif pos_weighting == \"inv_dist\":\n",
    "            L = int(items.size)\n",
    "            # distance_from_end: 1..L\n",
    "            for pos, it in enumerate(items):\n",
    "                dist = (L - pos)\n",
    "                scores[int(it)] += sim * (1.0 / float(dist))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pos_weighting: {pos_weighting}\")\n",
    "\n",
    "    return dict(scores)\n",
    "\n",
    "def topk_from_scores(scores: dict[int, float], k: int, pad_id: int) -> list[int]:\n",
    "    if not scores:\n",
    "        return []\n",
    "    # remove PAD if it exists\n",
    "    if pad_id in scores:\n",
    "        scores.pop(pad_id, None)\n",
    "    # sort by score desc then item_id asc\n",
    "    items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "    return [it for it, _ in items[:k]]\n",
    "\n",
    "print(\"[08-05] ✅ Session-KNN helpers ready\")\n",
    "\n",
    "print(\"\\n[08-05] CHECKPOINT D\")\n",
    "print(\"If you want to adjust SKNN_CFG (k_neighbors/caps/pos_weighting), do it now before building indexes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377cf56",
   "metadata": {},
   "source": [
    "TARGET: build KNN index from target TRAIN prefixes (small => full in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf4e0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[08-06] target_train input_ids: (1944, 20) attn_mask: (1944, 20)\n",
      "[08-06] target_train_sessions: 1944 | avg_len: 6.370884773662551\n",
      "[08-06] target_inv items: 631\n",
      "[08-06] posting sizes: min/median/p95/max = 1 14 62 174\n",
      "\n",
      "[08-06] CHECKPOINT E\n",
      "Target KNN index built. Next we evaluate on target VAL/TEST.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-06] TARGET: build KNN index from target TRAIN prefixes (small => full in-memory)\n",
    "# Training \"sessions\" = each TRAIN row prefix items (PAD removed, attn_mask used).\n",
    "# This keeps everything consistent with the supervised next-item setting.\n",
    "\n",
    "train_obj = torch_load_repo_artifact(target_train_pt, map_location=\"cpu\")\n",
    "assert isinstance(train_obj, dict), \"Expected dict in target train pt\"\n",
    "\n",
    "input_ids = torch.as_tensor(train_obj[\"input_ids\"]).detach().cpu().long()\n",
    "attn_mask = torch.as_tensor(train_obj[\"attn_mask\"]).detach().cpu().long()\n",
    "\n",
    "print(\"[08-06] target_train input_ids:\", tuple(input_ids.shape), \"attn_mask:\", tuple(attn_mask.shape))\n",
    "\n",
    "target_train_sessions = []\n",
    "for i in range(input_ids.shape[0]):\n",
    "    mask = attn_mask[i].numpy().astype(bool)\n",
    "    arr = input_ids[i].numpy()[mask].astype(np.int64)\n",
    "    arr = arr[arr != PAD_ID_TARGET]\n",
    "    # keep only last MAX_PREFIX_LEN (should already be)\n",
    "    if arr.size > MAX_PREFIX_LEN:\n",
    "        arr = arr[-MAX_PREFIX_LEN:]\n",
    "    target_train_sessions.append(arr)\n",
    "\n",
    "print(\"[08-06] target_train_sessions:\", len(target_train_sessions),\n",
    "      \"| avg_len:\", float(np.mean([len(s) for s in target_train_sessions])))\n",
    "\n",
    "target_session_lens, target_inv = build_inverted_index(target_train_sessions, PAD_ID_TARGET)\n",
    "print(\"[08-06] target_inv items:\", len(target_inv))\n",
    "\n",
    "# quick posting stats\n",
    "post_sizes = np.array([len(v) for v in target_inv.values()], dtype=np.int64)\n",
    "print(\"[08-06] posting sizes: min/median/p95/max =\",\n",
    "      int(post_sizes.min()), int(np.median(post_sizes)), int(np.quantile(post_sizes, 0.95)), int(post_sizes.max()))\n",
    "\n",
    "print(\"\\n[08-06] CHECKPOINT E\")\n",
    "print(\"Target KNN index built. Next we evaluate on target VAL/TEST.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95548bc",
   "metadata": {},
   "source": [
    "TARGET evaluation (VAL + TEST) with Session-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5095af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[08-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[08-07] TARGET VAL (Session-KNN): {'HR@5': 0.30687830687830686, 'HR@10': 0.48148148148148145, 'HR@20': 0.6243386243386243, 'MRR@5': 0.11940035273368603, 'MRR@10': 0.14264508272444779, 'MRR@20': 0.15248653060646283, 'NDCG@5': 0.1662376128266329, 'NDCG@10': 0.22261874958963768, 'NDCG@20': 0.25863209838882517, '_n_examples': 189}\n",
      "[08-07] TARGET TEST (Session-KNN): {'HR@5': 0.255, 'HR@10': 0.37, 'HR@20': 0.49, 'MRR@5': 0.08908333333333332, 'MRR@10': 0.10474999999999998, 'MRR@20': 0.1125188543826315, 'NDCG@5': 0.1301628535815534, 'NDCG@10': 0.16767645572690335, 'NDCG@20': 0.1973000384141053, '_n_examples': 200}\n",
      "\n",
      "[08-07] CHECKPOINT F\n",
      "Paste TARGET KNN metrics if you want to sanity-check before running SOURCE (heavy).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-07] TARGET evaluation (VAL + TEST) with Session-KNN\n",
    "\n",
    "def eval_target_knn(pt_path: Path, split_name: str) -> dict:\n",
    "    obj = torch_load_repo_artifact(pt_path, map_location=\"cpu\")\n",
    "    assert isinstance(obj, dict), f\"{split_name}: expected dict\"\n",
    "    x = torch.as_tensor(obj[\"input_ids\"]).detach().cpu().long()\n",
    "    m = torch.as_tensor(obj[\"attn_mask\"]).detach().cpu().long()\n",
    "    y = torch.as_tensor(obj[\"labels\"]).detach().cpu().long().numpy()\n",
    "\n",
    "    metrics = init_metrics()\n",
    "    n = 0\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        if int(y[i]) == PAD_ID_TARGET:\n",
    "            continue\n",
    "\n",
    "        mask = m[i].numpy().astype(bool)\n",
    "        q = x[i].numpy()[mask].astype(np.int64)\n",
    "        q = q[q != PAD_ID_TARGET]\n",
    "        if q.size > MAX_PREFIX_LEN:\n",
    "            q = q[-MAX_PREFIX_LEN:]\n",
    "\n",
    "        scores = score_items_from_neighbors(\n",
    "            query_items=q,\n",
    "            sessions_items=target_train_sessions,\n",
    "            session_lens=target_session_lens,\n",
    "            inv=target_inv,\n",
    "            k_neighbors=int(SKNN_CFG[\"k_neighbors\"]),\n",
    "            candidate_sessions_cap=int(SKNN_CFG[\"candidate_sessions_cap\"]),\n",
    "            pos_weighting=str(SKNN_CFG[\"pos_weighting\"]),\n",
    "            pad_id=PAD_ID_TARGET,\n",
    "        )\n",
    "\n",
    "        top = topk_from_scores(scores, MAX_K, PAD_ID_TARGET)\n",
    "        top_index = {it: idx for idx, it in enumerate(top)}\n",
    "\n",
    "        rank0 = top_index.get(int(y[i]), None)\n",
    "        update_metrics_from_rank(metrics, rank0)\n",
    "        n += 1\n",
    "\n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f\"[08-07] {split_name}: processed {i+1}/{x.shape[0]}\")\n",
    "\n",
    "    out = finalize_metrics(metrics, n)\n",
    "    out[\"_n_examples\"] = int(n)\n",
    "    return out\n",
    "\n",
    "t_val_knn = eval_target_knn(target_val_pt, \"target_val\")\n",
    "t_test_knn = eval_target_knn(target_test_pt, \"target_test\")\n",
    "\n",
    "print(\"[08-07] TARGET VAL (Session-KNN):\", t_val_knn)\n",
    "print(\"[08-07] TARGET TEST (Session-KNN):\", t_test_knn)\n",
    "\n",
    "print(\"\\n[08-07] CHECKPOINT F\")\n",
    "print(\"Paste TARGET KNN metrics if you want to sanity-check before running SOURCE (heavy).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f8539",
   "metadata": {},
   "source": [
    "SOURCE: build a memory-safe KNN index from a deterministic sample of TRAIN sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e5eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-08] SOURCE_INDEX_CFG: {'sample_size_sessions': 100000, 'sample_mod': 50, 'max_shards_to_scan': None}\n",
      "[08-08] Source shards: train= 1024 val= 1024 test= 1024\n",
      "[08-08] Using seq_col: items\n",
      "[08-08] scanned_shards=25/1024 | scanned_sessions=162,528 kept=3,310 | elapsed=0.9s\n",
      "[08-08] scanned_shards=50/1024 | scanned_sessions=325,575 kept=6,585 | elapsed=1.7s\n",
      "[08-08] scanned_shards=75/1024 | scanned_sessions=488,607 kept=9,823 | elapsed=2.7s\n",
      "[08-08] scanned_shards=100/1024 | scanned_sessions=650,531 kept=13,105 | elapsed=3.5s\n",
      "[08-08] scanned_shards=125/1024 | scanned_sessions=813,823 kept=16,526 | elapsed=4.4s\n",
      "[08-08] scanned_shards=150/1024 | scanned_sessions=977,651 kept=19,826 | elapsed=5.3s\n",
      "[08-08] scanned_shards=175/1024 | scanned_sessions=1,140,398 kept=23,115 | elapsed=6.4s\n",
      "[08-08] scanned_shards=200/1024 | scanned_sessions=1,304,244 kept=26,373 | elapsed=7.4s\n",
      "[08-08] scanned_shards=225/1024 | scanned_sessions=1,466,729 kept=29,689 | elapsed=8.6s\n",
      "[08-08] scanned_shards=250/1024 | scanned_sessions=1,629,328 kept=33,011 | elapsed=9.6s\n",
      "[08-08] scanned_shards=275/1024 | scanned_sessions=1,791,971 kept=36,203 | elapsed=10.4s\n",
      "[08-08] scanned_shards=300/1024 | scanned_sessions=1,954,910 kept=39,410 | elapsed=11.1s\n",
      "[08-08] scanned_shards=325/1024 | scanned_sessions=2,117,773 kept=42,766 | elapsed=12.0s\n",
      "[08-08] scanned_shards=350/1024 | scanned_sessions=2,280,185 kept=46,045 | elapsed=12.7s\n",
      "[08-08] scanned_shards=375/1024 | scanned_sessions=2,443,311 kept=49,321 | elapsed=13.5s\n",
      "[08-08] scanned_shards=400/1024 | scanned_sessions=2,606,674 kept=52,495 | elapsed=14.3s\n",
      "[08-08] scanned_shards=425/1024 | scanned_sessions=2,770,227 kept=55,757 | elapsed=15.1s\n",
      "[08-08] scanned_shards=450/1024 | scanned_sessions=2,933,414 kept=58,992 | elapsed=15.9s\n",
      "[08-08] scanned_shards=475/1024 | scanned_sessions=3,096,595 kept=62,308 | elapsed=16.6s\n",
      "[08-08] scanned_shards=500/1024 | scanned_sessions=3,259,886 kept=65,555 | elapsed=17.6s\n",
      "[08-08] scanned_shards=525/1024 | scanned_sessions=3,422,251 kept=68,767 | elapsed=18.6s\n",
      "[08-08] scanned_shards=550/1024 | scanned_sessions=3,584,635 kept=72,070 | elapsed=19.7s\n",
      "[08-08] scanned_shards=575/1024 | scanned_sessions=3,747,666 kept=75,354 | elapsed=20.7s\n",
      "[08-08] scanned_shards=600/1024 | scanned_sessions=3,910,276 kept=78,496 | elapsed=21.7s\n",
      "[08-08] scanned_shards=625/1024 | scanned_sessions=4,072,887 kept=81,744 | elapsed=22.5s\n",
      "[08-08] scanned_shards=650/1024 | scanned_sessions=4,235,275 kept=85,010 | elapsed=23.2s\n",
      "[08-08] scanned_shards=675/1024 | scanned_sessions=4,398,231 kept=88,242 | elapsed=24.0s\n",
      "[08-08] scanned_shards=700/1024 | scanned_sessions=4,560,993 kept=91,562 | elapsed=24.7s\n",
      "[08-08] scanned_shards=725/1024 | scanned_sessions=4,723,591 kept=94,907 | elapsed=25.6s\n",
      "[08-08] scanned_shards=750/1024 | scanned_sessions=4,886,691 kept=98,244 | elapsed=26.3s\n",
      "[08-08] ✅ Source index sample built: kept_sessions= 100,000 | scanned_sessions= 4,972,757 | shards_scanned= 764 | elapsed= 26.7s\n",
      "[08-08] source_inv items: 1461\n",
      "[08-08] posting sizes: min/median/p95/max = 1 27 348 2698\n",
      "\n",
      "[08-08] CHECKPOINT G\n",
      "If kept_sessions is too low, reduce sample_mod (e.g., 20) and rerun this cell.\n",
      "Next we evaluate source val/test with an optional pair cap for compute.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-08] SOURCE: build a memory-safe KNN index from a deterministic sample of TRAIN sessions\n",
    "# Source is huge, so we build an index from a reproducible subset of sessions.\n",
    "# IMPORTANT: This is model-side sampling (not protocol). It is logged in run_meta.json.\n",
    "\n",
    "SOURCE_INDEX_CFG = {\n",
    "    \"sample_size_sessions\": 100_000,   # adjust if you have RAM; 100k is a reasonable starting point\n",
    "    \"sample_mod\": 50,                  # keep session if md5(session_id)%sample_mod == 0\n",
    "    \"max_shards_to_scan\": None,        # None = scan until sample_size reached (or end)\n",
    "}\n",
    "\n",
    "print(\"[08-08] SOURCE_INDEX_CFG:\", SOURCE_INDEX_CFG)\n",
    "\n",
    "def list_parquet_shards(dir_path: Path) -> list[Path]:\n",
    "    files = sorted([Path(p) for p in glob.glob(str(dir_path / \"sessions_b*.parquet\"))])\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(f\"No shards found under {dir_path} (expected sessions_b*.parquet)\")\n",
    "    return files\n",
    "\n",
    "train_shards = list_parquet_shards(source_train_dir)\n",
    "val_shards   = list_parquet_shards(source_val_dir)\n",
    "test_shards  = list_parquet_shards(source_test_dir)\n",
    "\n",
    "print(\"[08-08] Source shards:\", \"train=\", len(train_shards), \"val=\", len(val_shards), \"test=\", len(test_shards))\n",
    "\n",
    "# detect seq col\n",
    "probe = pd.read_parquet(train_shards[0])\n",
    "seq_col = \"items\" if \"items\" in probe.columns else None\n",
    "if seq_col is None:\n",
    "    # fallback detector\n",
    "    def detect_sequence_column(df: pd.DataFrame) -> str:\n",
    "        for c in [\"items\", \"item_ids\", \"sequence\", \"seq\", \"course_ids\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        for c in df.columns:\n",
    "            s = df[c].dropna()\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            v = s.iloc[0]\n",
    "            if isinstance(v, (list, tuple, np.ndarray)):\n",
    "                return c\n",
    "        raise KeyError(f\"Could not detect sequence column from columns={list(df.columns)}\")\n",
    "    seq_col = detect_sequence_column(probe)\n",
    "\n",
    "print(\"[08-08] Using seq_col:\", seq_col)\n",
    "assert \"session_id\" in probe.columns, \"[08-08] Expected session_id column for deterministic sampling\"\n",
    "\n",
    "def stable_mod(session_id, mod: int) -> int:\n",
    "    s = str(session_id).encode(\"utf-8\")\n",
    "    h = hashlib.md5(s).hexdigest()\n",
    "    return int(h, 16) % int(mod)\n",
    "\n",
    "source_index_sessions = []\n",
    "source_index_session_ids = []\n",
    "scanned_sessions = 0\n",
    "kept_sessions = 0\n",
    "\n",
    "t0 = time.time()\n",
    "max_shards = SOURCE_INDEX_CFG[\"max_shards_to_scan\"]\n",
    "for shard_i, fp in enumerate(train_shards, 1):\n",
    "    if max_shards is not None and shard_i > int(max_shards):\n",
    "        break\n",
    "\n",
    "    df = pd.read_parquet(fp, columns=[\"session_id\", seq_col])\n",
    "\n",
    "    for sid, seq in zip(df[\"session_id\"].values, df[seq_col].values):\n",
    "        scanned_sessions += 1\n",
    "        if stable_mod(sid, int(SOURCE_INDEX_CFG[\"sample_mod\"])) != 0:\n",
    "            continue\n",
    "\n",
    "        # cap long sessions before mapping\n",
    "        if CAP_ENABLED and len(seq) > CAP_SESSION_LEN and CAP_STRATEGY == \"take_last\":\n",
    "            seq = seq[-CAP_SESSION_LEN:]\n",
    "\n",
    "        arr = map_source_seq_to_ids(seq)\n",
    "        arr = arr[arr != PAD_ID_SOURCE]\n",
    "        if arr.size < 2:\n",
    "            continue\n",
    "\n",
    "        # also cap to last MAX_PREFIX_LEN? (for similarity, we keep full capped session <=200 for source)\n",
    "        # KEEP full capped session (<=200) to match session-based neighbor structure\n",
    "\n",
    "        # range check\n",
    "        if arr.min() < 0 or arr.max() >= vocab_size_source:\n",
    "            raise ValueError(f\"[08-08] mapped source id out of range in {fp}: min={arr.min()} max={arr.max()} vocab={vocab_size_source}\")\n",
    "\n",
    "        source_index_sessions.append(arr.astype(np.int64))\n",
    "        source_index_session_ids.append(str(sid))\n",
    "        kept_sessions += 1\n",
    "\n",
    "        if kept_sessions >= int(SOURCE_INDEX_CFG[\"sample_size_sessions\"]):\n",
    "            break\n",
    "\n",
    "    if shard_i % 25 == 0:\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[08-08] scanned_shards={shard_i}/{len(train_shards)} | scanned_sessions={scanned_sessions:,} kept={kept_sessions:,} | elapsed={dt:.1f}s\")\n",
    "\n",
    "    if kept_sessions >= int(SOURCE_INDEX_CFG[\"sample_size_sessions\"]):\n",
    "        break\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(\"[08-08] ✅ Source index sample built:\",\n",
    "      \"kept_sessions=\", f\"{kept_sessions:,}\",\n",
    "      \"| scanned_sessions=\", f\"{scanned_sessions:,}\",\n",
    "      \"| shards_scanned=\", shard_i,\n",
    "      \"| elapsed=\", f\"{dt:.1f}s\")\n",
    "\n",
    "source_session_lens, source_inv = build_inverted_index(source_index_sessions, PAD_ID_SOURCE)\n",
    "print(\"[08-08] source_inv items:\", len(source_inv))\n",
    "\n",
    "post_sizes = np.array([len(v) for v in source_inv.values()], dtype=np.int64)\n",
    "print(\"[08-08] posting sizes: min/median/p95/max =\",\n",
    "      int(post_sizes.min()), int(np.median(post_sizes)), int(np.quantile(post_sizes, 0.95)), int(post_sizes.max()))\n",
    "\n",
    "print(\"\\n[08-08] CHECKPOINT G\")\n",
    "print(\"If kept_sessions is too low, reduce sample_mod (e.g., 20) and rerun this cell.\")\n",
    "print(\"Next we evaluate source val/test with an optional pair cap for compute.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2e775",
   "metadata": {},
   "source": [
    "SOURCE evaluation (VAL + TEST) with Session-KNN (streaming transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "932f8168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-09] SOURCE_EVAL_CFG: {'pair_cap': 300000, 'log_every_pairs': 100000}\n",
      "[08-09] source_val: pairs=100,000 sessions_seen=5,825 elapsed=244.6s\n",
      "[08-09] source_val: pairs=200,000 sessions_seen=12,015 elapsed=520.3s\n",
      "[08-09] source_val: pairs=300,000 sessions_seen=18,120 elapsed=774.9s\n",
      "[08-09] source_test: pairs=100,000 sessions_seen=6,060 elapsed=242.3s\n",
      "[08-09] source_test: pairs=200,000 sessions_seen=12,119 elapsed=478.2s\n",
      "[08-09] source_test: pairs=300,000 sessions_seen=18,311 elapsed=714.1s\n",
      "[08-09] SOURCE VAL (Session-KNN): {'HR@5': 0.9701633333333334, 'HR@10': 0.9728133333333333, 'HR@20': 0.9742666666666666, 'MRR@5': 0.939244777777809, 'MRR@10': 0.9396163240741069, 'MRR@20': 0.9397188795157997, 'NDCG@5': 0.947181040573676, 'NDCG@10': 0.9480558182513753, 'NDCG@20': 0.9484253802103635, '_n_pairs': 300000, '_n_sessions_seen': 18120, '_n_unk_labels': 2, '_pair_cap': 300000, '_note': 'Evaluation capped for compute feasibility (see SOURCE_EVAL_CFG).'}\n",
      "[08-09] SOURCE TEST (Session-KNN): {'HR@5': 0.9689733333333334, 'HR@10': 0.9716, 'HR@20': 0.9733133333333334, 'MRR@5': 0.9375912222222471, 'MRR@10': 0.9379577380952651, 'MRR@20': 0.9380807571385427, 'NDCG@5': 0.9456584209444998, 'NDCG@10': 0.9465238473584113, 'NDCG@20': 0.9469620279626358, '_n_pairs': 300000, '_n_sessions_seen': 18311, '_n_unk_labels': 0, '_pair_cap': 300000, '_note': 'Evaluation capped for compute feasibility (see SOURCE_EVAL_CFG).'}\n",
      "\n",
      "[08-09] CHECKPOINT H\n",
      "Paste SOURCE KNN metrics. If you want full eval, set SOURCE_EVAL_CFG['pair_cap']=None and rerun (may be long).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-09] SOURCE evaluation (VAL + TEST) with Session-KNN (streaming transitions)\n",
    "# NOTE: Source has ~13.8M pairs per split; Session-KNN is expensive.\n",
    "# We include an explicit compute cap (logged). Set to None to run full evaluation.\n",
    "# This does NOT change the protocol definition; it's a runtime limit for feasibility.\n",
    "\n",
    "SOURCE_EVAL_CFG = {\n",
    "    \"pair_cap\": 300_000,   # None for full (may take a very long time); otherwise evaluates first N pairs encountered\n",
    "    \"log_every_pairs\": 100_000,\n",
    "}\n",
    "\n",
    "print(\"[08-09] SOURCE_EVAL_CFG:\", SOURCE_EVAL_CFG)\n",
    "\n",
    "def eval_source_knn(shards: list[Path], split_name: str) -> dict:\n",
    "    metrics = init_metrics()\n",
    "    n_pairs = 0\n",
    "    n_sessions_seen = 0\n",
    "    n_unk_labels = 0\n",
    "\n",
    "    pair_cap = SOURCE_EVAL_CFG[\"pair_cap\"]\n",
    "    log_every = int(SOURCE_EVAL_CFG[\"log_every_pairs\"])\n",
    "\n",
    "    t0 = time.time()\n",
    "    for shard_i, fp in enumerate(shards, 1):\n",
    "        df = pd.read_parquet(fp, columns=[seq_col])\n",
    "\n",
    "        for seq in df[seq_col].values:\n",
    "            if seq is None:\n",
    "                continue\n",
    "\n",
    "            if CAP_ENABLED and len(seq) > CAP_SESSION_LEN and CAP_STRATEGY == \"take_last\":\n",
    "                seq = seq[-CAP_SESSION_LEN:]\n",
    "\n",
    "            arr = map_source_seq_to_ids(seq)\n",
    "            arr = arr[arr != PAD_ID_SOURCE]\n",
    "            L = int(arr.size)\n",
    "            if L < 2:\n",
    "                continue\n",
    "\n",
    "            n_sessions_seen += 1\n",
    "\n",
    "            # transitions t=1..L-1\n",
    "            for t in range(1, L):\n",
    "                y = int(arr[t])\n",
    "                if y == PAD_ID_SOURCE:\n",
    "                    continue\n",
    "                if y == UNK_ID_SOURCE:\n",
    "                    n_unk_labels += 1\n",
    "\n",
    "                # prefix = last MAX_PREFIX_LEN of arr[:t]\n",
    "                prefix = arr[:t]\n",
    "                if prefix.size > MAX_PREFIX_LEN:\n",
    "                    prefix = prefix[-MAX_PREFIX_LEN:]\n",
    "\n",
    "                scores = score_items_from_neighbors(\n",
    "                    query_items=prefix,\n",
    "                    sessions_items=source_index_sessions,\n",
    "                    session_lens=source_session_lens,\n",
    "                    inv=source_inv,\n",
    "                    k_neighbors=int(SKNN_CFG[\"k_neighbors\"]),\n",
    "                    candidate_sessions_cap=int(SKNN_CFG[\"candidate_sessions_cap\"]),\n",
    "                    pos_weighting=str(SKNN_CFG[\"pos_weighting\"]),\n",
    "                    pad_id=PAD_ID_SOURCE,\n",
    "                )\n",
    "                top = topk_from_scores(scores, MAX_K, PAD_ID_SOURCE)\n",
    "                top_index = {it: idx for idx, it in enumerate(top)}\n",
    "\n",
    "                rank0 = top_index.get(y, None)\n",
    "                update_metrics_from_rank(metrics, rank0)\n",
    "\n",
    "                n_pairs += 1\n",
    "\n",
    "                if (n_pairs % log_every) == 0:\n",
    "                    dt = time.time() - t0\n",
    "                    print(f\"[08-09] {split_name}: pairs={n_pairs:,} sessions_seen={n_sessions_seen:,} elapsed={dt:.1f}s\")\n",
    "\n",
    "                if pair_cap is not None and n_pairs >= int(pair_cap):\n",
    "                    out = finalize_metrics(metrics, n_pairs)\n",
    "                    out[\"_n_pairs\"] = int(n_pairs)\n",
    "                    out[\"_n_sessions_seen\"] = int(n_sessions_seen)\n",
    "                    out[\"_n_unk_labels\"] = int(n_unk_labels)\n",
    "                    out[\"_pair_cap\"] = int(pair_cap)\n",
    "                    out[\"_note\"] = \"Evaluation capped for compute feasibility (see SOURCE_EVAL_CFG).\"\n",
    "                    return out\n",
    "\n",
    "    out = finalize_metrics(metrics, n_pairs)\n",
    "    out[\"_n_pairs\"] = int(n_pairs)\n",
    "    out[\"_n_sessions_seen\"] = int(n_sessions_seen)\n",
    "    out[\"_n_unk_labels\"] = int(n_unk_labels)\n",
    "    out[\"_pair_cap\"] = None\n",
    "    return out\n",
    "\n",
    "s_val_knn = eval_source_knn(val_shards, \"source_val\")\n",
    "s_test_knn = eval_source_knn(test_shards, \"source_test\")\n",
    "\n",
    "print(\"[08-09] SOURCE VAL (Session-KNN):\", s_val_knn)\n",
    "print(\"[08-09] SOURCE TEST (Session-KNN):\", s_test_knn)\n",
    "\n",
    "print(\"\\n[08-09] CHECKPOINT H\")\n",
    "print(\"Paste SOURCE KNN metrics. If you want full eval, set SOURCE_EVAL_CFG['pair_cap']=None and rerun (may be long).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532897d",
   "metadata": {},
   "source": [
    "Write report artifacts to reports/08_session_knn_baseline/<RUN_TAG>/ + update meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5994dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-10] ✅ Wrote report files under: C:\\mooc-coldstart-session-meta\\reports\\08_session_knn_baseline\\20260102_140713\n",
      "[08-10] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[08-10] CHECKPOINT I\n",
      "Paste: report dir + key metrics summary after you run all cells.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-10] Write report artifacts to reports/08_session_knn_baseline/<RUN_TAG>/ + update meta.json\n",
    "\n",
    "REPORT_DIR = REPO_ROOT / \"reports\" / \"08_session_knn_baseline\" / RUN_TAG\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj: dict, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "run_meta = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"inputs\": {\n",
    "        \"target_run_tag\": TARGET_TAG,\n",
    "        \"source_run_tag\": SOURCE_TAG,\n",
    "        \"target_train_pt\": str(target_train_pt),\n",
    "        \"target_val_pt\": str(target_val_pt),\n",
    "        \"target_test_pt\": str(target_test_pt),\n",
    "        \"target_vocab_json\": str(target_vocab_json),\n",
    "        \"target_tensor_metadata_json\": str(target_tensor_meta_json),\n",
    "        \"source_train_dir\": str(source_train_dir),\n",
    "        \"source_val_dir\": str(source_val_dir),\n",
    "        \"source_test_dir\": str(source_test_dir),\n",
    "        \"source_vocab_json\": str(source_vocab_json),\n",
    "        \"dataloader_config\": str(cfg_path_repo),\n",
    "        \"sanity_metrics\": str(sanity_path_repo),\n",
    "        \"session_gap_thresholds\": str(gaps_path_repo),\n",
    "    },\n",
    "    \"protocol_reused_from_06\": {\n",
    "        \"K_LIST\": K_LIST,\n",
    "        \"MAX_PREFIX_LEN\": MAX_PREFIX_LEN,\n",
    "        \"PAD_ID_TARGET\": PAD_ID_TARGET,\n",
    "        \"PAD_ID_SOURCE\": PAD_ID_SOURCE,\n",
    "        \"CAP_ENABLED\": CAP_ENABLED,\n",
    "        \"CAP_SESSION_LEN\": CAP_SESSION_LEN,\n",
    "        \"CAP_STRATEGY\": CAP_STRATEGY,\n",
    "        \"pad_excluded_from_ranking\": True,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"Session-KNN\",\n",
    "        \"similarity\": \"cosine_binary\",\n",
    "        \"pos_weighting\": str(SKNN_CFG[\"pos_weighting\"]),\n",
    "        \"k_neighbors\": int(SKNN_CFG[\"k_neighbors\"]),\n",
    "        \"candidate_sessions_cap\": int(SKNN_CFG[\"candidate_sessions_cap\"]),\n",
    "    },\n",
    "    \"source_indexing\": {\n",
    "        \"sample_size_sessions\": int(SOURCE_INDEX_CFG[\"sample_size_sessions\"]),\n",
    "        \"sample_mod\": int(SOURCE_INDEX_CFG[\"sample_mod\"]),\n",
    "        \"max_shards_to_scan\": SOURCE_INDEX_CFG[\"max_shards_to_scan\"],\n",
    "        \"kept_sessions\": int(len(source_index_sessions)),\n",
    "        \"scanned_sessions\": int(scanned_sessions),\n",
    "        \"shards_scanned\": int(shard_i),\n",
    "    },\n",
    "    \"source_eval\": SOURCE_EVAL_CFG,\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"target\": {\n",
    "        \"val\": t_val_knn,\n",
    "        \"test\": t_test_knn,\n",
    "    },\n",
    "    \"source\": {\n",
    "        \"val\": s_val_knn,\n",
    "        \"test\": s_test_knn,\n",
    "    },\n",
    "}\n",
    "\n",
    "save_json(run_meta, REPORT_DIR / \"run_meta.json\")\n",
    "save_json(results, REPORT_DIR / \"results.json\")\n",
    "\n",
    "# Update meta.json\n",
    "meta_path = REPO_ROOT / \"meta.json\"\n",
    "meta = load_json(meta_path) if meta_path.exists() else {\"artifacts\": {}}\n",
    "\n",
    "meta.setdefault(\"artifacts\", {})\n",
    "meta[\"artifacts\"].setdefault(\"session_knn_baseline\", {})\n",
    "meta[\"artifacts\"][\"session_knn_baseline\"][RUN_TAG] = {\n",
    "    \"target_run_tag\": TARGET_TAG,\n",
    "    \"source_run_tag\": SOURCE_TAG,\n",
    "    \"report_dir\": str(REPORT_DIR),\n",
    "    \"results_json\": str(REPORT_DIR / \"results.json\"),\n",
    "    \"run_meta_json\": str(REPORT_DIR / \"run_meta.json\"),\n",
    "}\n",
    "meta[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "save_json(meta, meta_path)\n",
    "\n",
    "print(\"[08-10] ✅ Wrote report files under:\", REPORT_DIR)\n",
    "print(\"[08-10] ✅ Updated meta.json:\", meta_path)\n",
    "\n",
    "print(\"\\n[08-10] CHECKPOINT I\")\n",
    "print(\"Paste: report dir + key metrics summary after you run all cells.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475037fc",
   "metadata": {},
   "source": [
    "Footer summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c80868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 08 Session-KNN Baseline Summary ==========\n",
      "RUN_TAG: 20260102_140713\n",
      "--- TARGET ---\n",
      "VAL : {'HR@5': 0.30687830687830686, 'HR@10': 0.48148148148148145, 'HR@20': 0.6243386243386243, 'MRR@5': 0.11940035273368603, 'MRR@10': 0.14264508272444779, 'MRR@20': 0.15248653060646283, 'NDCG@5': 0.1662376128266329, 'NDCG@10': 0.22261874958963768, 'NDCG@20': 0.25863209838882517, '_n_examples': 189}\n",
      "TEST: {'HR@5': 0.255, 'HR@10': 0.37, 'HR@20': 0.49, 'MRR@5': 0.08908333333333332, 'MRR@10': 0.10474999999999998, 'MRR@20': 0.1125188543826315, 'NDCG@5': 0.1301628535815534, 'NDCG@10': 0.16767645572690335, 'NDCG@20': 0.1973000384141053, '_n_examples': 200}\n",
      "--- SOURCE ---\n",
      "VAL : {'HR@5': 0.9701633333333334, 'HR@10': 0.9728133333333333, 'HR@20': 0.9742666666666666, 'MRR@5': 0.939244777777809, 'MRR@10': 0.9396163240741069, 'MRR@20': 0.9397188795157997, 'NDCG@5': 0.947181040573676, 'NDCG@10': 0.9480558182513753, 'NDCG@20': 0.9484253802103635, '_n_pairs': 300000, '_n_sessions_seen': 18120, '_n_unk_labels': 2, '_pair_cap': 300000, '_note': 'Evaluation capped for compute feasibility (see SOURCE_EVAL_CFG).'}\n",
      "TEST: {'HR@5': 0.9689733333333334, 'HR@10': 0.9716, 'HR@20': 0.9733133333333334, 'MRR@5': 0.9375912222222471, 'MRR@10': 0.9379577380952651, 'MRR@20': 0.9380807571385427, 'NDCG@5': 0.9456584209444998, 'NDCG@10': 0.9465238473584113, 'NDCG@20': 0.9469620279626358, '_n_pairs': 300000, '_n_sessions_seen': 18311, '_n_unk_labels': 0, '_pair_cap': 300000, '_note': 'Evaluation capped for compute feasibility (see SOURCE_EVAL_CFG).'}\n",
      "Report dir: C:\\mooc-coldstart-session-meta\\reports\\08_session_knn_baseline\\20260102_140713\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-11] Footer summary\n",
    "\n",
    "print(\"========== 08 Session-KNN Baseline Summary ==========\")\n",
    "print(\"RUN_TAG:\", RUN_TAG)\n",
    "print(\"--- TARGET ---\")\n",
    "print(\"VAL :\", t_val_knn)\n",
    "print(\"TEST:\", t_test_knn)\n",
    "print(\"--- SOURCE ---\")\n",
    "print(\"VAL :\", s_val_knn)\n",
    "print(\"TEST:\", s_test_knn)\n",
    "print(\"Report dir:\", REPORT_DIR)\n",
    "print(\"====================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
