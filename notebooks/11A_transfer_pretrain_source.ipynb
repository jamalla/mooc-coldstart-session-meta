{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4cff67",
   "metadata": {},
   "source": [
    "Imports + versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a225288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-00] torch: 2.9.1+cpu\n",
      "[11A-00] pandas: 2.3.3\n",
      "[11A-00] numpy: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "#[CELL 11A-00] Imports + versions\n",
    "\n",
    "import os, json, time, math, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"[11A-00] torch:\", torch.__version__)\n",
    "print(\"[11A-00] pandas:\", pd.__version__)\n",
    "print(\"[11A-00] numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ffe8f",
   "metadata": {},
   "source": [
    "Repo root + run tag + load protocol JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f766f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[11A-01] RUN_TAG: 20260103_220933\n",
      "[11A-01] Expect config: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[11A-01] Expect sanity: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\sanity_metrics_20251229_163357_20251229_232834.json\n",
      "[11A-01] Expect gaps: C:\\mooc-coldstart-session-meta\\data\\processed\\normalized_events\\session_gap_thresholds.json\n",
      "[11A-01] Loaded dataloader_config keys: ['target', 'source', 'protocol']\n",
      "[11A-01] Loaded sanity_metrics keys: ['run_tag_target', 'run_tag_source', 'created_at', 'target', 'source', 'notes']\n",
      "[11A-01] Loaded session_gap_thresholds keys: ['generated_from_run_tag', 'generated_at', 'target', 'source', 'decision_notes']\n",
      "[11A-01] target keys: ['primary_threshold_seconds', 'primary_threshold_label']\n",
      "[11A-01] source keys: ['primary_threshold_seconds', 'primary_threshold_label', 'sampling']\n",
      "[11A-01] target: gap_minutes from 'primary_threshold_seconds'=1800s -> 30m | label=30m\n",
      "[11A-01] source: gap_minutes from 'primary_threshold_seconds'=600s -> 10m | label=10m\n",
      "[11A-01] ✅ Session gaps confirmed: target=30m, source=10m\n",
      "\n",
      "[11A-01] CHECKPOINT A\n",
      "Paste: inferred minutes + labels (if printed).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-01] Repo root + run tag + load protocol JSONs\n",
    "from datetime import datetime\n",
    "\n",
    "REPO_ROOT = Path(r\"C:\\mooc-coldstart-session-meta\").resolve()\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "cfg_path = REPO_ROOT / \"data/processed/supervised/dataloader_config_20251229_163357_20251229_232834.json\"\n",
    "sanity_path = REPO_ROOT / \"data/processed/supervised/sanity_metrics_20251229_163357_20251229_232834.json\"\n",
    "gaps_path = REPO_ROOT / \"data/processed/normalized_events/session_gap_thresholds.json\"\n",
    "\n",
    "print(\"[11A-01] REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"[11A-01] RUN_TAG:\", RUN_TAG)\n",
    "print(\"[11A-01] Expect config:\", cfg_path)\n",
    "print(\"[11A-01] Expect sanity:\", sanity_path)\n",
    "print(\"[11A-01] Expect gaps:\", gaps_path)\n",
    "\n",
    "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    DL_CFG = json.load(f)\n",
    "with open(sanity_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    SANITY = json.load(f)\n",
    "with open(gaps_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    GAPS = json.load(f)\n",
    "\n",
    "print(\"[11A-01] Loaded dataloader_config keys:\", list(DL_CFG.keys()))\n",
    "print(\"[11A-01] Loaded sanity_metrics keys:\", list(SANITY.keys()))\n",
    "print(\"[11A-01] Loaded session_gap_thresholds keys:\", list(GAPS.keys()))\n",
    "\n",
    "# --- [PATCH v2] robust gap minutes inference (now supports primary_threshold_seconds) ---\n",
    "\n",
    "def infer_gap_minutes(d: dict, name: str) -> int:\n",
    "    \"\"\"\n",
    "    Accept multiple schema variants.\n",
    "    Returns int minutes.\n",
    "    \"\"\"\n",
    "    # explicit primary key in your file\n",
    "    if \"primary_threshold_seconds\" in d:\n",
    "        v = int(d[\"primary_threshold_seconds\"])\n",
    "        m = int(round(v / 60))\n",
    "        lbl = d.get(\"primary_threshold_label\", None)\n",
    "        print(f\"[11A-01] {name}: gap_minutes from 'primary_threshold_seconds'={v}s -> {m}m | label={lbl}\")\n",
    "        return m\n",
    "\n",
    "    # common minute keys\n",
    "    for k in [\"gap_minutes\", \"session_gap_minutes\", \"threshold_minutes\", \"minutes\", \"gap_min\"]:\n",
    "        if k in d:\n",
    "            v = int(d[k])\n",
    "            print(f\"[11A-01] {name}: gap_minutes from key '{k}' = {v}\")\n",
    "            return v\n",
    "\n",
    "    # common second keys\n",
    "    for k in [\"gap_seconds\", \"session_gap_seconds\", \"threshold_seconds\", \"seconds\", \"gap_sec\"]:\n",
    "        if k in d:\n",
    "            v = int(d[k])\n",
    "            m = int(round(v / 60))\n",
    "            print(f\"[11A-01] {name}: gap_minutes inferred from '{k}'={v}s -> {m}m\")\n",
    "            return m\n",
    "\n",
    "    # sometimes nested threshold object\n",
    "    if \"gap\" in d and isinstance(d[\"gap\"], dict):\n",
    "        return infer_gap_minutes(d[\"gap\"], name + \".gap\")\n",
    "\n",
    "    raise KeyError(f\"[11A-01] {name}: Could not infer gap minutes. Keys={list(d.keys())}\")\n",
    "\n",
    "print(\"[11A-01] target keys:\", list(GAPS[\"target\"].keys()))\n",
    "print(\"[11A-01] source keys:\", list(GAPS[\"source\"].keys()))\n",
    "\n",
    "gap_target_m = infer_gap_minutes(GAPS[\"target\"], \"target\")\n",
    "gap_source_m = infer_gap_minutes(GAPS[\"source\"], \"source\")\n",
    "\n",
    "assert gap_target_m == 30, f\"target gap mismatch: got {gap_target_m}m\"\n",
    "assert gap_source_m == 10, f\"source gap mismatch: got {gap_source_m}m\"\n",
    "\n",
    "print(\"[11A-01] ✅ Session gaps confirmed:\", f\"target={gap_target_m}m,\", f\"source={gap_source_m}m\")\n",
    "\n",
    "print(\"\\n[11A-01] CHECKPOINT A\")\n",
    "print(\"Paste: inferred minutes + labels (if printed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88261d1f",
   "metadata": {},
   "source": [
    "Paths: source sequences + vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283c1ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-02] Source shards counts: train= 1024 val= 1024 test= 1024\n",
      "[11A-02] source_vocab: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\source_vocab_items_20251229_232834.json\n",
      "\n",
      "[11A-02] CHECKPOINT B\n",
      "Confirm all source dirs/files exist and shard counts look like 1024 each.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-02] Paths: source sequences + vocab\n",
    "SOURCE_RUN_TAG = \"20251229_232834\"\n",
    "\n",
    "source_train_dir = REPO_ROOT / f\"data/processed/session_sequences/source_sessions_{SOURCE_RUN_TAG}/train\"\n",
    "source_val_dir   = REPO_ROOT / f\"data/processed/session_sequences/source_sessions_{SOURCE_RUN_TAG}/val\"\n",
    "source_test_dir  = REPO_ROOT / f\"data/processed/session_sequences/source_sessions_{SOURCE_RUN_TAG}/test\"\n",
    "source_vocab_path = REPO_ROOT / f\"data/processed/session_sequences/source_sessions_{SOURCE_RUN_TAG}/source_vocab_items_{SOURCE_RUN_TAG}.json\"\n",
    "\n",
    "for p in [source_train_dir, source_val_dir, source_test_dir, source_vocab_path]:\n",
    "    assert p.exists(), f\"Missing: {p}\"\n",
    "\n",
    "train_files = sorted(source_train_dir.glob(\"*.parquet\"))\n",
    "val_files = sorted(source_val_dir.glob(\"*.parquet\"))\n",
    "test_files = sorted(source_test_dir.glob(\"*.parquet\"))\n",
    "\n",
    "print(\"[11A-02] Source shards counts: train=\", len(train_files), \"val=\", len(val_files), \"test=\", len(test_files))\n",
    "print(\"[11A-02] source_vocab:\", source_vocab_path)\n",
    "\n",
    "print(\"\\n[11A-02] CHECKPOINT B\")\n",
    "print(\"Confirm all source dirs/files exist and shard counts look like 1024 each.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85a536",
   "metadata": {},
   "source": [
    "Load source vocab + PAD/UNK + infer seq_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a340c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-03] source_vocab keys: ['run_tag_source', 'built_from', 'vocab_size', 'pad_id', 'unk_id', 'item2id']\n",
      "[11A-03] VOCAB_SIZE_SOURCE: 1620\n",
      "[11A-03] PAD_ID_SOURCE: 0 | UNK_ID_SOURCE: 1\n",
      "[11A-03] item2id size: 1620\n",
      "[11A-03] Probe columns: ['domain', 'user_id', 'session_id', 'session_length', 'start_ts', 'end_ts', 'items', 'split']\n",
      "[11A-03] seq_col: items | first seq type: <class 'numpy.ndarray'> | first elem type: <class 'str'>\n",
      "\n",
      "[11A-03] CHECKPOINT C\n",
      "Paste: VOCAB_SIZE_SOURCE, PAD/UNK, seq_col, and first element type (should be str).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-03] Load source vocab + PAD/UNK + infer seq_col\n",
    "with open(source_vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    source_vocab = json.load(f)\n",
    "\n",
    "PAD_ID_SOURCE = int(source_vocab.get(\"pad_id\", 0))\n",
    "UNK_ID_SOURCE = int(source_vocab.get(\"unk_id\", 1))\n",
    "VOCAB_SIZE_SOURCE = int(source_vocab[\"vocab_size\"])\n",
    "item2id = source_vocab[\"item2id\"]  # token->id\n",
    "\n",
    "print(\"[11A-03] source_vocab keys:\", list(source_vocab.keys()))\n",
    "print(\"[11A-03] VOCAB_SIZE_SOURCE:\", VOCAB_SIZE_SOURCE)\n",
    "print(\"[11A-03] PAD_ID_SOURCE:\", PAD_ID_SOURCE, \"| UNK_ID_SOURCE:\", UNK_ID_SOURCE)\n",
    "print(\"[11A-03] item2id size:\", len(item2id))\n",
    "\n",
    "probe = pd.read_parquet(train_files[0])\n",
    "print(\"[11A-03] Probe columns:\", list(probe.columns))\n",
    "\n",
    "# we already used \"items\" in 07/08\n",
    "seq_col = \"items\" if \"items\" in probe.columns else None\n",
    "assert seq_col is not None, \"Could not find seq column (expected 'items').\"\n",
    "\n",
    "x0 = probe[seq_col].iloc[0]\n",
    "print(\"[11A-03] seq_col:\", seq_col, \"| first seq type:\", type(x0), \"| first elem type:\", type(x0[0]))\n",
    "\n",
    "print(\"\\n[11A-03] CHECKPOINT C\")\n",
    "print(\"Paste: VOCAB_SIZE_SOURCE, PAD/UNK, seq_col, and first element type (should be str).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea197817",
   "metadata": {},
   "source": [
    "Add this new cell: Rebind PROTO safely\n",
    "Bind + normalize PROTO (handles key name variants safely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965acd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-03A] protocol keys: ['max_prefix_len', 'source_vocab_mode', 'source_pair_rule', 'source_long_session_policy', 'dataloader', 'seeds']\n",
      "[11A-03A] source_long_session_policy keys: ['enabled', 'cap_session_len', 'cap_strategy']\n",
      "[11A-03A] ✅ Normalized PROTO: {'K_LIST': [5, 10, 20], 'MAX_PREFIX_LEN': 20, 'CAP_ENABLED': True, 'CAP_SESSION_LEN': 200, 'CAP_STRATEGY': 'take_last'}\n",
      "[11A-03A] ✅ PROTO asserts passed (matches Notebook 06).\n",
      "\n",
      "[11A-03A] CHECKPOINT D\n",
      "Paste: protocol keys + source_long_session_policy keys + PROTO.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-03A] Bind + normalize PROTO from DL_CFG (actual schema)\n",
    "\n",
    "if \"DL_CFG\" not in globals():\n",
    "    cfg_path = REPO_ROOT / \"data/processed/supervised/dataloader_config_20251229_163357_20251229_232834.json\"\n",
    "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        DL_CFG = json.load(f)\n",
    "    print(\"[11A-03A] Re-loaded dataloader_config:\", cfg_path)\n",
    "\n",
    "P = DL_CFG[\"protocol\"]\n",
    "print(\"[11A-03A] protocol keys:\", list(P.keys()))\n",
    "\n",
    "# 1) MAX_PREFIX_LEN\n",
    "assert \"max_prefix_len\" in P, f\"[11A-03A] missing max_prefix_len. keys={list(P.keys())}\"\n",
    "MAX_PREFIX_LEN = int(P[\"max_prefix_len\"])\n",
    "\n",
    "# 2) K_LIST is a fixed decision from Notebook 06 (not stored in config)\n",
    "K_LIST = [5, 10, 20]\n",
    "\n",
    "# 3) CAP policy from source_long_session_policy\n",
    "# Expecting something like: {\"enabled\": true, \"cap_session_len\": 200, \"strategy\": \"take_last\"} (or similar)\n",
    "SLSP = P.get(\"source_long_session_policy\", {})\n",
    "print(\"[11A-03A] source_long_session_policy keys:\", list(SLSP.keys()))\n",
    "\n",
    "def pick(d, keys, default=None):\n",
    "    for k in keys:\n",
    "        if k in d:\n",
    "            return d[k]\n",
    "    return default\n",
    "\n",
    "CAP_ENABLED = bool(pick(SLSP, [\"enabled\", \"cap_enabled\", \"capEnabled\"], default=True))\n",
    "\n",
    "# Prefer explicit cap length if present, else fall back to fixed decision (200)\n",
    "CAP_SESSION_LEN = pick(SLSP, [\"cap_session_len\", \"cap_len\", \"max_len\", \"capSessionLen\"], default=200)\n",
    "CAP_SESSION_LEN = int(CAP_SESSION_LEN)\n",
    "\n",
    "# Prefer explicit strategy if present, else fixed decision (take_last)\n",
    "CAP_STRATEGY = pick(SLSP, [\"strategy\", \"cap_strategy\", \"capStrategy\"], default=\"take_last\")\n",
    "CAP_STRATEGY = str(CAP_STRATEGY)\n",
    "\n",
    "# Canonical dict used by remaining cells\n",
    "PROTO = {\n",
    "    \"K_LIST\": K_LIST,\n",
    "    \"MAX_PREFIX_LEN\": MAX_PREFIX_LEN,\n",
    "    \"CAP_ENABLED\": CAP_ENABLED,\n",
    "    \"CAP_SESSION_LEN\": CAP_SESSION_LEN,\n",
    "    \"CAP_STRATEGY\": CAP_STRATEGY,\n",
    "}\n",
    "\n",
    "print(\"[11A-03A] ✅ Normalized PROTO:\", PROTO)\n",
    "\n",
    "# Hard asserts (must match Notebook 06 / prior notebooks)\n",
    "assert PROTO[\"MAX_PREFIX_LEN\"] == 20, f\"MAX_PREFIX_LEN drift: {PROTO['MAX_PREFIX_LEN']}\"\n",
    "assert PROTO[\"CAP_ENABLED\"] is True, f\"CAP_ENABLED drift: {PROTO['CAP_ENABLED']}\"\n",
    "assert PROTO[\"CAP_SESSION_LEN\"] == 200, f\"CAP_SESSION_LEN drift: {PROTO['CAP_SESSION_LEN']}\"\n",
    "assert PROTO[\"CAP_STRATEGY\"] == \"take_last\", f\"CAP_STRATEGY drift: {PROTO['CAP_STRATEGY']}\"\n",
    "assert PROTO[\"K_LIST\"] == [5, 10, 20], f\"K_LIST drift: {PROTO['K_LIST']}\"\n",
    "\n",
    "print(\"[11A-03A] ✅ PROTO asserts passed (matches Notebook 06).\")\n",
    "\n",
    "print(\"\\n[11A-03A] CHECKPOINT D\")\n",
    "print(\"Paste: protocol keys + source_long_session_policy keys + PROTO.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55be57",
   "metadata": {},
   "source": [
    "Streaming batch generator (one-pair-per-session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8809dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-04] Built source_token_to_id from source_vocab['item2id'] size=1,620\n",
      "[11A-04] PAD/UNK: {'PAD_ID_SOURCE': 0, 'UNK_ID_SOURCE': 1}\n",
      "[11A-04] ✅ Streaming pair generator ready\n",
      "[11A-04] stable_mod probe: 9 9 (should match)\n",
      "\n",
      "[11A-04] CHECKPOINT D\n",
      "Next run [11A-05] to probe 3 yielded pairs (x[:10], mask sum, label).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-04] Streaming pair generator (one-pair-per-session) — SELF-SUFFICIENT\n",
    "# - Reads parquet shards lazily per file\n",
    "# - Maps string tokens -> ids using source_vocab[\"item2id\"]\n",
    "# - Applies CAP policy (take_last, 200) + MAX_PREFIX_LEN (20)\n",
    "# - Emits one (x_ids, attn_mask, y_id) per session\n",
    "# - Deterministic sampling for string session_id via stable hash\n",
    "\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ---- Protocol (must already be bound in [11A-03A]) ----\n",
    "MAX_LEN = int(PROTO[\"MAX_PREFIX_LEN\"])\n",
    "CAP_ENABLED = bool(PROTO[\"CAP_ENABLED\"])\n",
    "CAP_SESSION_LEN = int(PROTO[\"CAP_SESSION_LEN\"])\n",
    "CAP_STRATEGY = str(PROTO[\"CAP_STRATEGY\"])\n",
    "\n",
    "assert MAX_LEN == 20\n",
    "assert CAP_ENABLED is True\n",
    "assert CAP_SESSION_LEN == 200\n",
    "assert CAP_STRATEGY == \"take_last\"\n",
    "\n",
    "# ---- Ensure vocab mapping exists (rebuild if missing) ----\n",
    "# Requires: source_vocab loaded in [11A-03] and PAD/UNK ids resolved (or fallback).\n",
    "if \"source_vocab\" not in globals():\n",
    "    raise NameError(\"[11A-04] source_vocab not found. Run [11A-03] first (loads source_vocab JSON).\")\n",
    "\n",
    "if \"source_token_to_id\" not in globals():\n",
    "    # Build from source_vocab schema\n",
    "    if \"item2id\" not in source_vocab:\n",
    "        raise KeyError(f\"[11A-04] source_vocab missing 'item2id'. keys={list(source_vocab.keys())}\")\n",
    "    source_token_to_id = source_vocab[\"item2id\"]\n",
    "    print(f\"[11A-04] Built source_token_to_id from source_vocab['item2id'] size={len(source_token_to_id):,}\")\n",
    "else:\n",
    "    print(f\"[11A-04] source_token_to_id already in globals size={len(source_token_to_id):,}\")\n",
    "\n",
    "# Ensure PAD/UNK are present\n",
    "PAD_ID_SOURCE = int(source_vocab.get(\"pad_id\", 0)) if \"PAD_ID_SOURCE\" not in globals() else int(PAD_ID_SOURCE)\n",
    "UNK_ID_SOURCE = int(source_vocab.get(\"unk_id\", 1)) if \"UNK_ID_SOURCE\" not in globals() else int(UNK_ID_SOURCE)\n",
    "\n",
    "print(\"[11A-04] PAD/UNK:\", {\"PAD_ID_SOURCE\": PAD_ID_SOURCE, \"UNK_ID_SOURCE\": UNK_ID_SOURCE})\n",
    "\n",
    "# seq_col must be detected in [11A-03]\n",
    "if \"seq_col\" not in globals():\n",
    "    raise NameError(\"[11A-04] seq_col not found. Run [11A-03] first (detects seq_col).\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "def stable_mod(value, mod: int) -> int:\n",
    "    \"\"\"Deterministic mod for any session_id type (str/int/...).\"\"\"\n",
    "    s = str(value).encode(\"utf-8\")\n",
    "    h = hashlib.blake2b(s, digest_size=8).digest()\n",
    "    n = int.from_bytes(h, byteorder=\"little\", signed=False)\n",
    "    return n % mod\n",
    "\n",
    "def map_tokens_to_ids(tokens) -> np.ndarray:\n",
    "    \"\"\"tokens: iterable of str -> np.int64 ids with UNK fallback.\"\"\"\n",
    "    out = np.empty(len(tokens), dtype=np.int64)\n",
    "    for i, tok in enumerate(tokens):\n",
    "        out[i] = source_token_to_id.get(str(tok), UNK_ID_SOURCE)\n",
    "    return out\n",
    "\n",
    "def session_to_one_pair(seq_tokens):\n",
    "    \"\"\"\n",
    "    seq_tokens: list/np array of tokens (strings)\n",
    "    Returns (x_ids[int64, MAX_LEN], attn_mask[int64, MAX_LEN], y_id[int]) or None\n",
    "    \"\"\"\n",
    "    seq = seq_tokens\n",
    "    if not isinstance(seq, (np.ndarray, list, tuple)):\n",
    "        return None\n",
    "\n",
    "    # Cap long sessions first\n",
    "    if CAP_ENABLED and len(seq) > CAP_SESSION_LEN and CAP_STRATEGY == \"take_last\":\n",
    "        seq = seq[-CAP_SESSION_LEN:]\n",
    "\n",
    "    if len(seq) < 2:\n",
    "        return None\n",
    "\n",
    "    prefix_tokens = seq[:-1]\n",
    "    label_token = seq[-1]\n",
    "\n",
    "    # Cap prefix to MAX_LEN (take_last)\n",
    "    if len(prefix_tokens) > MAX_LEN:\n",
    "        prefix_tokens = prefix_tokens[-MAX_LEN:]\n",
    "\n",
    "    x_ids = map_tokens_to_ids(prefix_tokens)\n",
    "    y_id = int(source_token_to_id.get(str(label_token), UNK_ID_SOURCE))\n",
    "\n",
    "    # Left-pad to MAX_LEN\n",
    "    pad_len = MAX_LEN - len(x_ids)\n",
    "    if pad_len > 0:\n",
    "        x_ids = np.concatenate([np.full(pad_len, PAD_ID_SOURCE, dtype=np.int64), x_ids], axis=0)\n",
    "\n",
    "    attn_mask = (x_ids != PAD_ID_SOURCE).astype(np.int64)\n",
    "\n",
    "    if y_id == PAD_ID_SOURCE:\n",
    "        return None\n",
    "\n",
    "    return x_ids, attn_mask, y_id\n",
    "\n",
    "def iter_session_pairs_parquet(\n",
    "    parquet_files,\n",
    "    seed: int = 42,\n",
    "    sample_mod: int = 1,\n",
    "    sample_rem: int = 0,\n",
    "    log_every_files: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Yields (x_ids[int64], attn_mask[int64], y_id[int]) for one pair per session.\n",
    "    Sampling: keep session if stable_mod(session_id, sample_mod) == sample_rem\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    files = list(parquet_files)\n",
    "    rng.shuffle(files)\n",
    "\n",
    "    t0 = time.time()\n",
    "    n_files = 0\n",
    "    n_sessions = 0\n",
    "    n_yield = 0\n",
    "    n_short = 0\n",
    "    n_unk_label = 0\n",
    "\n",
    "    for fpath in files:\n",
    "        n_files += 1\n",
    "        df = pd.read_parquet(fpath, columns=[\"session_id\", seq_col])\n",
    "\n",
    "        for sid, seq in zip(df[\"session_id\"].values, df[seq_col].values):\n",
    "            n_sessions += 1\n",
    "\n",
    "            if sample_mod and sample_mod > 1:\n",
    "                if stable_mod(sid, sample_mod) != sample_rem:\n",
    "                    continue\n",
    "\n",
    "            pair = session_to_one_pair(seq)\n",
    "            if pair is None:\n",
    "                n_short += 1\n",
    "                continue\n",
    "\n",
    "            x_ids, attn_mask, y_id = pair\n",
    "            if y_id == UNK_ID_SOURCE:\n",
    "                n_unk_label += 1\n",
    "\n",
    "            n_yield += 1\n",
    "            yield torch.from_numpy(x_ids), torch.from_numpy(attn_mask), int(y_id)\n",
    "\n",
    "        if log_every_files and (n_files % log_every_files == 0):\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"[11A-04] scanned_files={n_files}/{len(files)} \"\n",
    "                f\"sessions_seen={n_sessions:,} yielded={n_yield:,} short={n_short:,} \"\n",
    "                f\"unk_labels={n_unk_label:,} elapsed={elapsed:.1f}s\"\n",
    "            )\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(\n",
    "        f\"[11A-04] DONE files={n_files} sessions_seen={n_sessions:,} yielded={n_yield:,} \"\n",
    "        f\"short={n_short:,} unk_labels={n_unk_label:,} elapsed={elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "print(\"[11A-04] ✅ Streaming pair generator ready\")\n",
    "print(\"[11A-04] stable_mod probe:\", stable_mod(\"3160332::21\", 10), stable_mod(\"3160332::21\", 10), \"(should match)\")\n",
    "\n",
    "print(\"\\n[11A-04] CHECKPOINT D\")\n",
    "print(\"Next run [11A-05] to probe 3 yielded pairs (x[:10], mask sum, label).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06f000",
   "metadata": {},
   "source": [
    " Probe generator output (must look sane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2f83be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-05] sample 0: x_nonzero=19 label=417\n",
      " x[:10]= [0, 133, 213, 251, 251, 133, 251, 251, 251, 251]\n",
      " m[:10]= [0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[11A-05] sample 1: x_nonzero=20 label=13\n",
      " x[:10]= [13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      " m[:10]= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[11A-05] sample 2: x_nonzero=18 label=14\n",
      " x[:10]= [0, 0, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      " m[:10]= [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "[11A-05] CHECKPOINT E\n",
      "Paste these 3 probe samples (x_nonzero + label).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-05] Probe generator output (must look sane)\n",
    "\n",
    "gen = iter_session_pairs_parquet(train_files[:2], seed=42, sample_mod=10, sample_rem=0, log_every_files=1)\n",
    "for j in range(3):\n",
    "    x, m, y = next(gen)\n",
    "    print(f\"[11A-05] sample {j}: x_nonzero={int(m.sum())} label={y}\")\n",
    "    print(\" x[:10]=\", x[:10].tolist())\n",
    "    print(\" m[:10]=\", m[:10].tolist())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n[11A-05] CHECKPOINT E\")\n",
    "print(\"Paste these 3 probe samples (x_nonzero + label).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f855bc3",
   "metadata": {},
   "source": [
    "Model choice for 11A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8522d",
   "metadata": {},
   "source": [
    "GRU4Rec model (encoder we will transfer later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a69cbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 11A-06] GRU4Rec model (encoder we will transfer later)\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, dropout, pad_id):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        x = self.emb(input_ids)              # [B,L,E]\n",
    "        x = self.drop(x)\n",
    "        lengths = attn_mask.sum(dim=1).clamp(min=1).to(torch.int64)  # [B]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True, total_length=input_ids.size(1))\n",
    "        idx = (lengths - 1).view(-1, 1, 1).expand(-1, 1, out.size(-1))\n",
    "        last_h = out.gather(1, idx).squeeze(1)  # [B,H]\n",
    "        logits = self.out(last_h)               # [B,V]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de63cc5",
   "metadata": {},
   "source": [
    "Train config + batching (stream to minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01108c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-07] PRETRAIN_CFG: {'model': 'gru4rec', 'emb_dim': 64, 'hidden_dim': 128, 'dropout': 0.3, 'batch_size': 512, 'lr': 0.001, 'weight_decay': 1e-06, 'grad_clip': 1.0, 'seed': 42, 'sample_mod': 5, 'sample_rem': 0, 'max_steps_per_epoch': 3000, 'max_epochs': 5, 'log_every_steps': 200}\n",
      "\n",
      "[11A-07] CHECKPOINT F\n",
      "If you want to adjust sample_mod or steps_per_epoch for CPU, do it now.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-07] Train config + batching (stream to minibatches)\n",
    "PRETRAIN_CFG = {\n",
    "    \"model\": \"gru4rec\",\n",
    "    \"emb_dim\": 64,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.3,\n",
    "    \"batch_size\": 512,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # streaming controls (compute parity)\n",
    "    \"sample_mod\": 5,        # keep 1/5 sessions\n",
    "    \"sample_rem\": 0,\n",
    "    \"max_steps_per_epoch\": 3000,  # hard cap on updates per epoch\n",
    "    \"max_epochs\": 5,\n",
    "    \"log_every_steps\": 200,\n",
    "}\n",
    "\n",
    "print(\"[11A-07] PRETRAIN_CFG:\", PRETRAIN_CFG)\n",
    "print(\"\\n[11A-07] CHECKPOINT F\")\n",
    "print(\"If you want to adjust sample_mod or steps_per_epoch for CPU, do it now.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a5a82",
   "metadata": {},
   "source": [
    "Training loop (source pretrain) + quick val eval hook\n",
    "For validation, will keep it small but real (cap pairs like we did in 08 for compute), but log it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f457213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-08] Model params: 387156\n",
      "[11A-08] Starting pretrain on SOURCE (streamed, capped steps).\n",
      "[11A-08] epoch=01 step=200 loss=4.5336 elapsed=37.4s\n",
      "[11A-08] epoch=01 step=400 loss=4.0626 elapsed=76.4s\n",
      "[11A-04] scanned_files=200/1024 sessions_seen=1,303,802 yielded=260,817 short=0 unk_labels=0 elapsed=96.7s\n",
      "[11A-08] epoch=01 step=600 loss=3.8752 elapsed=114.4s\n",
      "[11A-08] epoch=01 step=800 loss=3.8486 elapsed=152.6s\n",
      "[11A-08] epoch=01 step=1000 loss=3.8282 elapsed=191.3s\n",
      "[11A-04] scanned_files=400/1024 sessions_seen=2,606,471 yielded=521,582 short=0 unk_labels=0 elapsed=194.1s\n",
      "[11A-08] epoch=01 step=1200 loss=3.8084 elapsed=227.7s\n",
      "[11A-08] epoch=01 step=1400 loss=3.7458 elapsed=267.4s\n",
      "[11A-04] scanned_files=600/1024 sessions_seen=3,910,143 yielded=782,331 short=0 unk_labels=0 elapsed=290.0s\n",
      "[11A-08] epoch=01 step=1600 loss=3.7681 elapsed=303.0s\n",
      "[11A-08] epoch=01 step=1800 loss=3.7808 elapsed=343.8s\n",
      "[11A-08] epoch=01 step=2000 loss=3.7739 elapsed=380.7s\n",
      "[11A-04] scanned_files=800/1024 sessions_seen=5,213,245 yielded=1,042,961 short=0 unk_labels=0 elapsed=388.1s\n",
      "[11A-08] epoch=01 step=2200 loss=3.7470 elapsed=418.4s\n",
      "[11A-08] epoch=01 step=2400 loss=3.7677 elapsed=454.9s\n",
      "[11A-04] scanned_files=1000/1024 sessions_seen=6,515,452 yielded=1,303,639 short=0 unk_labels=0 elapsed=484.0s\n",
      "[11A-08] epoch=01 step=2600 loss=3.7797 elapsed=493.6s\n",
      "[11A-04] DONE files=1024 sessions_seen=6,672,282 yielded=1,335,215 short=0 unk_labels=0 elapsed=495.2s\n",
      "[11A-04] DONE files=1024 sessions_seen=833,042 yielded=166,684 short=0 unk_labels=0 elapsed=37.9s\n",
      "[11A-08] epoch=01 done | train_loss_mean=3.9493 | SOURCE_VAL(cap): {'HR@5': 0.45882211538461537, 'MRR@5': 0.4244458133012817, 'NDCG@5': 0.432931736248322, 'HR@10': 0.49818509615384615, 'MRR@10': 0.42963946123321095, 'NDCG@10': 0.4456002017431901, 'HR@20': 0.545733173076923, 'MRR@20': 0.43290112288686505, 'NDCG@20': 0.4575741913012473, '_n_pairs': 166400, '_pair_cap': 200000}\n",
      "[11A-08] epoch=02 step=200 loss=3.7643 elapsed=569.6s\n",
      "[11A-08] epoch=02 step=400 loss=3.7769 elapsed=607.5s\n",
      "[11A-04] scanned_files=200/1024 sessions_seen=1,304,167 yielded=261,062 short=0 unk_labels=0 elapsed=95.5s\n",
      "[11A-08] epoch=02 step=600 loss=3.7310 elapsed=646.4s\n",
      "[11A-08] epoch=02 step=800 loss=3.7121 elapsed=681.7s\n",
      "[11A-08] epoch=02 step=1000 loss=3.7221 elapsed=717.3s\n",
      "[11A-04] scanned_files=400/1024 sessions_seen=2,608,213 yielded=523,149 short=0 unk_labels=0 elapsed=188.4s\n",
      "[11A-08] epoch=02 step=1200 loss=3.7215 elapsed=754.0s\n",
      "[11A-08] epoch=02 step=1400 loss=3.7224 elapsed=789.6s\n",
      "[11A-04] scanned_files=600/1024 sessions_seen=3,909,958 yielded=783,498 short=0 unk_labels=0 elapsed=279.0s\n",
      "[11A-08] epoch=02 step=1600 loss=3.7407 elapsed=826.9s\n",
      "[11A-08] epoch=02 step=1800 loss=3.6990 elapsed=866.4s\n",
      "[11A-08] epoch=02 step=2000 loss=3.7838 elapsed=901.4s\n",
      "[11A-04] scanned_files=800/1024 sessions_seen=5,212,701 yielded=1,043,906 short=0 unk_labels=0 elapsed=375.1s\n",
      "[11A-08] epoch=02 step=2200 loss=3.7340 elapsed=937.6s\n",
      "[11A-08] epoch=02 step=2400 loss=3.7402 elapsed=974.5s\n",
      "[11A-04] scanned_files=1000/1024 sessions_seen=6,515,649 yielded=1,304,077 short=0 unk_labels=0 elapsed=467.2s\n",
      "[11A-08] epoch=02 step=2600 loss=3.7724 elapsed=1008.9s\n",
      "[11A-04] DONE files=1024 sessions_seen=6,672,282 yielded=1,335,215 short=0 unk_labels=0 elapsed=477.0s\n",
      "[11A-04] DONE files=1024 sessions_seen=833,042 yielded=166,684 short=0 unk_labels=0 elapsed=25.1s\n",
      "[11A-08] epoch=02 done | train_loss_mean=3.7356 | SOURCE_VAL(cap): {'HR@5': 0.4598137019230769, 'MRR@5': 0.4252625200320513, 'NDCG@5': 0.43379735639896205, 'HR@10': 0.4994771634615385, 'MRR@10': 0.4305336037660257, 'NDCG@10': 0.44660123366192467, 'HR@20': 0.5482151442307692, 'MRR@20': 0.4338991459294936, 'NDCG@20': 0.4589024864850366, '_n_pairs': 166400, '_pair_cap': 200000}\n",
      "[11A-08] epoch=03 step=200 loss=3.7149 elapsed=1072.0s\n",
      "[11A-08] epoch=03 step=400 loss=3.7474 elapsed=1103.4s\n",
      "[11A-04] scanned_files=200/1024 sessions_seen=1,303,367 yielded=261,580 short=0 unk_labels=0 elapsed=88.9s\n",
      "[11A-08] epoch=03 step=600 loss=3.7130 elapsed=1139.7s\n",
      "[11A-08] epoch=03 step=800 loss=3.7635 elapsed=1174.9s\n",
      "[11A-08] epoch=03 step=1000 loss=3.7285 elapsed=1214.9s\n",
      "[11A-04] scanned_files=400/1024 sessions_seen=2,606,662 yielded=521,018 short=0 unk_labels=0 elapsed=182.3s\n",
      "[11A-08] epoch=03 step=1200 loss=3.6920 elapsed=1253.3s\n",
      "[11A-08] epoch=03 step=1400 loss=3.7232 elapsed=1286.3s\n",
      "[11A-04] scanned_files=600/1024 sessions_seen=3,909,117 yielded=781,551 short=0 unk_labels=0 elapsed=271.9s\n",
      "[11A-08] epoch=03 step=1600 loss=3.7497 elapsed=1320.1s\n",
      "[11A-08] epoch=03 step=1800 loss=3.7078 elapsed=1351.5s\n",
      "[11A-08] epoch=03 step=2000 loss=3.7065 elapsed=1383.5s\n",
      "[11A-04] scanned_files=800/1024 sessions_seen=5,211,739 yielded=1,042,606 short=0 unk_labels=0 elapsed=354.4s\n",
      "[11A-08] epoch=03 step=2200 loss=3.7001 elapsed=1416.4s\n",
      "[11A-08] epoch=03 step=2400 loss=3.7144 elapsed=1451.7s\n",
      "[11A-04] scanned_files=1000/1024 sessions_seen=6,515,110 yielded=1,303,697 short=0 unk_labels=0 elapsed=440.6s\n",
      "[11A-08] epoch=03 step=2600 loss=3.7358 elapsed=1483.2s\n",
      "[11A-04] DONE files=1024 sessions_seen=6,672,282 yielded=1,335,215 short=0 unk_labels=0 elapsed=449.4s\n",
      "[11A-04] DONE files=1024 sessions_seen=833,042 yielded=166,684 short=0 unk_labels=0 elapsed=21.8s\n",
      "[11A-08] epoch=03 done | train_loss_mean=3.7156 | SOURCE_VAL(cap): {'HR@5': 0.4599158653846154, 'MRR@5': 0.4255876402243589, 'NDCG@5': 0.43407744243197016, 'HR@10': 0.50015625, 'MRR@10': 0.4309292200854702, 'NDCG@10': 0.4470621181938, 'HR@20': 0.5492007211538461, 'MRR@20': 0.4343169403665255, 'NDCG@20': 0.4594422373221476, '_n_pairs': 166400, '_pair_cap': 200000}\n",
      "[11A-08] epoch=04 step=200 loss=3.6788 elapsed=1538.6s\n",
      "[11A-08] epoch=04 step=400 loss=3.6500 elapsed=1572.7s\n",
      "[11A-04] scanned_files=200/1024 sessions_seen=1,303,761 yielded=260,578 short=0 unk_labels=0 elapsed=82.7s\n",
      "[11A-08] epoch=04 step=600 loss=3.7075 elapsed=1603.7s\n",
      "[11A-08] epoch=04 step=800 loss=3.7369 elapsed=1634.8s\n",
      "[11A-08] epoch=04 step=1000 loss=3.6904 elapsed=1668.1s\n",
      "[11A-04] scanned_files=400/1024 sessions_seen=2,606,337 yielded=521,483 short=0 unk_labels=0 elapsed=165.0s\n",
      "[11A-08] epoch=04 step=1200 loss=3.6932 elapsed=1700.8s\n",
      "[11A-08] epoch=04 step=1400 loss=3.7029 elapsed=1732.5s\n",
      "[11A-04] scanned_files=600/1024 sessions_seen=3,909,552 yielded=782,799 short=0 unk_labels=0 elapsed=247.1s\n",
      "[11A-08] epoch=04 step=1600 loss=3.6860 elapsed=1765.9s\n",
      "[11A-08] epoch=04 step=1800 loss=3.6987 elapsed=1800.2s\n",
      "[11A-08] epoch=04 step=2000 loss=3.7019 elapsed=1831.8s\n",
      "[11A-04] scanned_files=800/1024 sessions_seen=5,211,600 yielded=1,043,744 short=0 unk_labels=0 elapsed=331.1s\n",
      "[11A-08] epoch=04 step=2200 loss=3.7057 elapsed=1862.4s\n",
      "[11A-08] epoch=04 step=2400 loss=3.6771 elapsed=1894.6s\n",
      "[11A-04] scanned_files=1000/1024 sessions_seen=6,515,840 yielded=1,304,074 short=0 unk_labels=0 elapsed=412.4s\n",
      "[11A-08] epoch=04 step=2600 loss=3.7273 elapsed=1928.5s\n",
      "[11A-04] DONE files=1024 sessions_seen=6,672,282 yielded=1,335,215 short=0 unk_labels=0 elapsed=423.1s\n",
      "[11A-04] DONE files=1024 sessions_seen=833,042 yielded=166,684 short=0 unk_labels=0 elapsed=22.2s\n",
      "[11A-08] epoch=04 done | train_loss_mean=3.7034 | SOURCE_VAL(cap): {'HR@5': 0.4614783653846154, 'MRR@5': 0.4260316506410255, 'NDCG@5': 0.43478734603892727, 'HR@10': 0.5011298076923076, 'MRR@10': 0.4313156669719171, 'NDCG@10': 0.4476017775548399, 'HR@20': 0.5503966346153846, 'MRR@20': 0.434693601516312, 'NDCG@20': 0.4600058929970361, '_n_pairs': 166400, '_pair_cap': 200000}\n",
      "[11A-08] epoch=05 step=200 loss=3.7171 elapsed=1984.5s\n",
      "[11A-08] epoch=05 step=400 loss=3.7035 elapsed=2021.4s\n",
      "[11A-04] scanned_files=200/1024 sessions_seen=1,303,050 yielded=260,954 short=0 unk_labels=0 elapsed=91.8s\n",
      "[11A-08] epoch=05 step=600 loss=3.7038 elapsed=2060.5s\n",
      "[11A-08] epoch=05 step=800 loss=3.6921 elapsed=2103.7s\n",
      "[11A-08] epoch=05 step=1000 loss=3.7118 elapsed=2143.0s\n",
      "[11A-04] scanned_files=400/1024 sessions_seen=2,605,485 yielded=521,274 short=0 unk_labels=0 elapsed=195.1s\n",
      "[11A-08] epoch=05 step=1200 loss=3.6756 elapsed=2181.7s\n",
      "[11A-08] epoch=05 step=1400 loss=3.7041 elapsed=2217.9s\n",
      "[11A-04] scanned_files=600/1024 sessions_seen=3,909,055 yielded=782,092 short=0 unk_labels=0 elapsed=290.8s\n",
      "[11A-08] epoch=05 step=1600 loss=3.6760 elapsed=2256.5s\n",
      "[11A-08] epoch=05 step=1800 loss=3.7157 elapsed=2291.3s\n",
      "[11A-08] epoch=05 step=2000 loss=3.7060 elapsed=2329.9s\n",
      "[11A-04] scanned_files=800/1024 sessions_seen=5,212,642 yielded=1,042,708 short=0 unk_labels=0 elapsed=384.4s\n",
      "[11A-08] epoch=05 step=2200 loss=3.6931 elapsed=2365.3s\n",
      "[11A-08] epoch=05 step=2400 loss=3.6845 elapsed=2400.5s\n",
      "[11A-04] scanned_files=1000/1024 sessions_seen=6,516,559 yielded=1,303,868 short=0 unk_labels=0 elapsed=476.2s\n",
      "[11A-08] epoch=05 step=2600 loss=3.7001 elapsed=2438.1s\n",
      "[11A-04] DONE files=1024 sessions_seen=6,672,282 yielded=1,335,215 short=0 unk_labels=0 elapsed=487.7s\n",
      "[11A-04] DONE files=1024 sessions_seen=833,042 yielded=166,684 short=0 unk_labels=0 elapsed=22.5s\n",
      "[11A-08] epoch=05 done | train_loss_mean=3.6951 | SOURCE_VAL(cap): {'HR@5': 0.46146634615384613, 'MRR@5': 0.4259734575320513, 'NDCG@5': 0.43474004344695855, 'HR@10': 0.5021033653846154, 'MRR@10': 0.4313856360653237, 'NDCG@10': 0.4478695616722559, 'HR@20': 0.5511418269230769, 'MRR@20': 0.4347381770722851, 'NDCG@20': 0.46020487755975864, '_n_pairs': 166400, '_pair_cap': 200000}\n",
      "[11A-08] ✅ Best epoch: 5 best HR@20: 0.5511418269230769\n",
      "\n",
      "[11A-08] CHECKPOINT G\n",
      "Paste: best epoch + val_res dict (HR/MRR/NDCG + _n_pairs/_pair_cap).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-08] Training loop (source pretrain) + quick val eval hook\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_source_pairs(model, files, pair_cap=200_000, seed=42, sample_mod=1, sample_rem=0):\n",
    "    model.eval()\n",
    "    K_LIST = PROTO[\"K_LIST\"]\n",
    "    pad_id = PAD_ID_SOURCE\n",
    "\n",
    "    # metric accumulators\n",
    "    hits = {k: 0 for k in K_LIST}\n",
    "    mrrs = {k: 0.0 for k in K_LIST}\n",
    "    ndcgs = {k: 0.0 for k in K_LIST}\n",
    "    n = 0\n",
    "\n",
    "    gen = iter_session_pairs_parquet(\n",
    "    files,\n",
    "    seed=seed,\n",
    "    sample_mod=sample_mod,\n",
    "    sample_rem=sample_rem,\n",
    "    log_every_files=999999\n",
    "    )\n",
    "\n",
    "    bs = 512\n",
    "    buf_x, buf_m, buf_y = [], [], []\n",
    "\n",
    "    def rank_metrics(logits, y, x_seen_mask=None):\n",
    "        # logits: [B,V], y: [B]\n",
    "        # exclude PAD from ranking\n",
    "        logits[:, pad_id] = -1e9\n",
    "\n",
    "        # topK\n",
    "        maxk = max(K_LIST)\n",
    "        topk = torch.topk(logits, k=maxk, dim=1).indices  # [B,maxk]\n",
    "        # compute metrics\n",
    "        out = {k: {\"hit\":0, \"mrr\":0.0, \"ndcg\":0.0} for k in K_LIST}\n",
    "        for i in range(topk.size(0)):\n",
    "            yi = int(y[i].item())\n",
    "            preds = topk[i].tolist()\n",
    "            for k in K_LIST:\n",
    "                p = preds[:k]\n",
    "                if yi in p:\n",
    "                    r = p.index(yi) + 1\n",
    "                    out[k][\"hit\"] += 1\n",
    "                    out[k][\"mrr\"] += 1.0 / r\n",
    "                    out[k][\"ndcg\"] += 1.0 / math.log2(r + 1)\n",
    "        return out\n",
    "\n",
    "    for x, m, y in gen:\n",
    "        buf_x.append(x); buf_m.append(m); buf_y.append(y)\n",
    "        if len(buf_x) == bs:\n",
    "            xb = torch.tensor(np.stack(buf_x), dtype=torch.long)\n",
    "            mb = torch.tensor(np.stack(buf_m), dtype=torch.long)\n",
    "            yb = torch.tensor(np.array(buf_y), dtype=torch.long)\n",
    "\n",
    "            logits = model(xb, mb)\n",
    "            out = rank_metrics(logits, yb)\n",
    "            for k in K_LIST:\n",
    "                hits[k] += out[k][\"hit\"]\n",
    "                mrrs[k] += out[k][\"mrr\"]\n",
    "                ndcgs[k] += out[k][\"ndcg\"]\n",
    "            n += xb.size(0)\n",
    "\n",
    "            buf_x, buf_m, buf_y = [], [], []\n",
    "            if pair_cap is not None and n >= pair_cap:\n",
    "                break\n",
    "\n",
    "    res = {}\n",
    "    for k in K_LIST:\n",
    "        res[f\"HR@{k}\"] = hits[k] / max(1, n)\n",
    "        res[f\"MRR@{k}\"] = mrrs[k] / max(1, n)\n",
    "        res[f\"NDCG@{k}\"] = ndcgs[k] / max(1, n)\n",
    "    res[\"_n_pairs\"] = n\n",
    "    res[\"_pair_cap\"] = pair_cap\n",
    "    return res\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "set_seed(PRETRAIN_CFG[\"seed\"])\n",
    "\n",
    "model = GRU4Rec(\n",
    "    vocab_size=VOCAB_SIZE_SOURCE,\n",
    "    emb_dim=PRETRAIN_CFG[\"emb_dim\"],\n",
    "    hidden_dim=PRETRAIN_CFG[\"hidden_dim\"],\n",
    "    dropout=PRETRAIN_CFG[\"dropout\"],\n",
    "    pad_id=PAD_ID_SOURCE,\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=PRETRAIN_CFG[\"lr\"], weight_decay=PRETRAIN_CFG[\"weight_decay\"])\n",
    "\n",
    "print(\"[11A-08] Model params:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"[11A-08] Starting pretrain on SOURCE (streamed, capped steps).\")\n",
    "\n",
    "best = {\"hr20\": -1, \"epoch\": -1, \"state\": None}\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, PRETRAIN_CFG[\"max_epochs\"] + 1):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    losses = []\n",
    "\n",
    "    gen = iter_session_pairs_parquet(\n",
    "    train_files,\n",
    "    seed=PRETRAIN_CFG[\"seed\"] + epoch,\n",
    "    sample_mod=PRETRAIN_CFG[\"sample_mod\"],\n",
    "    sample_rem=PRETRAIN_CFG[\"sample_rem\"],\n",
    "    log_every_files=200\n",
    ")\n",
    "\n",
    "\n",
    "    bs = PRETRAIN_CFG[\"batch_size\"]\n",
    "    buf_x, buf_m, buf_y = [], [], []\n",
    "\n",
    "    for x, m, y in gen:\n",
    "        buf_x.append(x); buf_m.append(m); buf_y.append(y)\n",
    "        if len(buf_x) == bs:\n",
    "            xb = torch.tensor(np.stack(buf_x), dtype=torch.long)\n",
    "            mb = torch.tensor(np.stack(buf_m), dtype=torch.long)\n",
    "            yb = torch.tensor(np.array(buf_y), dtype=torch.long)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb, mb)\n",
    "            loss = F.cross_entropy(logits, yb, ignore_index=PAD_ID_SOURCE)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), PRETRAIN_CFG[\"grad_clip\"])\n",
    "            opt.step()\n",
    "\n",
    "            losses.append(float(loss.item()))\n",
    "            step += 1\n",
    "            buf_x, buf_m, buf_y = [], [], []\n",
    "\n",
    "            if (step % PRETRAIN_CFG[\"log_every_steps\"]) == 0:\n",
    "                print(f\"[11A-08] epoch={epoch:02d} step={step} loss={np.mean(losses[-50:]):.4f} elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "            if step >= PRETRAIN_CFG[\"max_steps_per_epoch\"]:\n",
    "                break\n",
    "\n",
    "    val_res = eval_source_pairs(model, val_files, pair_cap=200_000, seed=42, sample_mod=5, sample_rem=0)\n",
    "    print(f\"[11A-08] epoch={epoch:02d} done | train_loss_mean={np.mean(losses):.4f} | SOURCE_VAL(cap): {val_res}\")\n",
    "\n",
    "    hr20 = val_res[\"HR@20\"]\n",
    "    if hr20 > best[\"hr20\"]:\n",
    "        best[\"hr20\"] = hr20\n",
    "        best[\"epoch\"] = epoch\n",
    "        best[\"state\"] = {k: v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "print(\"[11A-08] ✅ Best epoch:\", best[\"epoch\"], \"best HR@20:\", best[\"hr20\"])\n",
    "print(\"\\n[11A-08] CHECKPOINT G\")\n",
    "print(\"Paste: best epoch + val_res dict (HR/MRR/NDCG + _n_pairs/_pair_cap).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad95ab",
   "metadata": {},
   "source": [
    "Save pretrained checkpoint + reports + meta.json update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "254c9a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11A-09] ✅ Saved pretrained checkpoint: C:\\mooc-coldstart-session-meta\\reports\\11A_transfer_pretrain_source\\20260103_220933\\model_pretrained_source.pt\n",
      "[11A-09] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[11A-09] CHECKPOINT H\n",
      "Paste: report_dir path + confirm meta.json updated.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11A-09] Save pretrained checkpoint + reports + meta.json update\n",
    "\n",
    "report_dir = REPO_ROOT / \"reports\" / \"11A_transfer_pretrain_source\" / RUN_TAG\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"source_run_tag\": SOURCE_RUN_TAG,\n",
    "    \"protocol\": PROTO,\n",
    "    \"pretrain_cfg\": PRETRAIN_CFG,\n",
    "    \"vocab_size_source\": VOCAB_SIZE_SOURCE,\n",
    "    \"pad_id_source\": PAD_ID_SOURCE,\n",
    "    \"unk_id_source\": UNK_ID_SOURCE,\n",
    "    \"best_epoch\": best[\"epoch\"],\n",
    "    \"best_val_hr20_cap\": best[\"hr20\"],\n",
    "    # This is what we will transfer later:\n",
    "    \"transfer_notes\": {\n",
    "        \"transferable\": [\"gru.*\"],  # in 11B we will load only GRU weights, not embeddings/out head\n",
    "        \"not_transferable\": [\"emb.weight\", \"out.weight\", \"out.bias\"]\n",
    "    },\n",
    "    \"state_dict\": best[\"state\"],\n",
    "}\n",
    "\n",
    "ckpt_path = report_dir / \"model_pretrained_source.pt\"\n",
    "torch.save(ckpt, ckpt_path)\n",
    "print(\"[11A-09] ✅ Saved pretrained checkpoint:\", ckpt_path)\n",
    "\n",
    "# write meta.json update (append run)\n",
    "meta_path = REPO_ROOT / \"meta.json\"\n",
    "meta = {}\n",
    "if meta_path.exists():\n",
    "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "meta.setdefault(\"runs\", [])\n",
    "meta[\"runs\"].append({\n",
    "    \"notebook\": \"11A_transfer_pretrain_source.ipynb\",\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"source_run_tag\": SOURCE_RUN_TAG,\n",
    "    \"model\": PRETRAIN_CFG[\"model\"],\n",
    "    \"report_dir\": str(report_dir),\n",
    "    \"checkpoint\": str(ckpt_path),\n",
    "    \"notes\": \"Source pretraining (streamed one-pair-per-session, capped steps/epoch). Encoder weights intended for transfer; embeddings/head are source-specific.\"\n",
    "})\n",
    "\n",
    "meta_path.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(\"[11A-09] ✅ Updated meta.json:\", meta_path)\n",
    "\n",
    "print(\"\\n[11A-09] CHECKPOINT H\")\n",
    "print(\"Paste: report_dir path + confirm meta.json updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
