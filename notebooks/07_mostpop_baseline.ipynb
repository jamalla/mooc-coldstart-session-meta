{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03916c7",
   "metadata": {},
   "source": [
    "Notebook header + imports (MostPop baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b2ecc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-00] Imports OK\n",
      "[07-00] torch: 2.9.1+cpu\n",
      "[07-00] pandas: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-00] Notebook header + imports (MostPop baseline)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# torch is required (target tensors)\n",
    "import torch\n",
    "\n",
    "# pandas/pyarrow for source parquet shards (streaming)\n",
    "import pandas as pd\n",
    "\n",
    "print(\"[07-00] Imports OK\")\n",
    "print(\"[07-00] torch:\", torch.__version__)\n",
    "print(\"[07-00] pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162816f",
   "metadata": {},
   "source": [
    "Locate repo root + define RUN_TAG + load protocol/config artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112f5ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[07-01] RUN_TAG: 20260102_133019\n",
      "[07-01] Expect config: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[07-01] Expect sanity: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\sanity_metrics_20251229_163357_20251229_232834.json\n",
      "[07-01] Expect gaps: C:\\mooc-coldstart-session-meta\\data\\processed\\normalized_events\\session_gap_thresholds.json\n",
      "[07-01] Loaded dataloader_config keys: ['target', 'source', 'protocol']\n",
      "[07-01] Loaded sanity_metrics keys: ['run_tag_target', 'run_tag_source', 'created_at', 'target', 'source', 'notes']\n",
      "[07-01] Loaded session_gap_thresholds keys: ['generated_from_run_tag', 'generated_at', 'target', 'source', 'decision_notes']\n",
      "[07-01] ✅ Session gaps confirmed: target=30m, source=10m\n",
      "\n",
      "[07-01] CHECKPOINT A\n",
      "1) Confirm REPO_ROOT exists and points to mooc-coldstart-session-meta\n",
      "2) Confirm the three repo JSON files load without errors\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-01] Locate repo root + define RUN_TAG + load protocol/config artifacts\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists() and (p / \"meta.json\").exists():\n",
    "            return p\n",
    "    # fallback: PROJECT_STATE only\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not locate repo root (expected PROJECT_STATE.md and meta.json).\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
    "print(\"[07-01] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "# New run tag for 07 (do NOT touch earlier run tags)\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[07-01] RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "# Fixed upstream run tags (must not change)\n",
    "TARGET_TAG = \"20251229_163357\"\n",
    "SOURCE_TAG = \"20251229_232834\"\n",
    "\n",
    "# Load artifacts from repo (preferred) OR fallback to local uploaded copies (only for reading config values)\n",
    "def load_json(path: Path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "cfg_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"dataloader_config_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "sanity_path_repo = REPO_ROOT / \"data/processed/supervised\" / f\"sanity_metrics_{TARGET_TAG}_{SOURCE_TAG}.json\"\n",
    "gaps_path_repo = REPO_ROOT / \"data/processed/normalized_events\" / \"session_gap_thresholds.json\"\n",
    "\n",
    "# If repo paths aren't available (rare), you can point these to the uploaded copies manually.\n",
    "print(\"[07-01] Expect config:\", cfg_path_repo)\n",
    "print(\"[07-01] Expect sanity:\", sanity_path_repo)\n",
    "print(\"[07-01] Expect gaps:\", gaps_path_repo)\n",
    "\n",
    "dataloader_cfg = load_json(cfg_path_repo)\n",
    "sanity_metrics = load_json(sanity_path_repo)\n",
    "session_gaps = load_json(gaps_path_repo)\n",
    "\n",
    "print(\"[07-01] Loaded dataloader_config keys:\", list(dataloader_cfg.keys()))\n",
    "print(\"[07-01] Loaded sanity_metrics keys:\", list(sanity_metrics.keys()))\n",
    "print(\"[07-01] Loaded session_gap_thresholds keys:\", list(session_gaps.keys()))\n",
    "\n",
    "# Enforce fixed decisions\n",
    "assert session_gaps[\"target\"][\"primary_threshold_seconds\"] == 1800, \"Target gap must be 30m (1800s).\"\n",
    "assert session_gaps[\"source\"][\"primary_threshold_seconds\"] == 600, \"Source gap must be 10m (600s).\"\n",
    "\n",
    "print(\"[07-01] ✅ Session gaps confirmed: target=30m, source=10m\")\n",
    "\n",
    "print(\"\\n[07-01] CHECKPOINT A\")\n",
    "print(\"1) Confirm REPO_ROOT exists and points to mooc-coldstart-session-meta\")\n",
    "print(\"2) Confirm the three repo JSON files load without errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84657e89",
   "metadata": {},
   "source": [
    "Resolve known artifact paths (target tensors + source sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a9d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-02] Target train pt: C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[07-02] Source train dir: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\train\n",
      "[07-02] ✅ All required artifacts exist\n",
      "\n",
      "[07-02] CHECKPOINT B\n",
      "If any path is missing, paste the exact error here (do NOT proceed).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-02] Resolve known artifact paths (target tensors + source sequences)\n",
    "\n",
    "def must_exist(p: Path, label: str):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {p}\")\n",
    "    return p\n",
    "\n",
    "# TARGET tensors (05B output)\n",
    "TARGET_TENSOR_DIR = REPO_ROOT / \"data/processed/tensor_target\"\n",
    "target_train_pt = TARGET_TENSOR_DIR / f\"target_tensor_train_{TARGET_TAG}.pt\"\n",
    "target_val_pt   = TARGET_TENSOR_DIR / f\"target_tensor_val_{TARGET_TAG}.pt\"\n",
    "target_test_pt  = TARGET_TENSOR_DIR / f\"target_tensor_test_{TARGET_TAG}.pt\"\n",
    "target_vocab_json = TARGET_TENSOR_DIR / f\"target_vocab_items_{TARGET_TAG}.json\"\n",
    "target_tensor_meta_json = TARGET_TENSOR_DIR / f\"target_tensor_metadata_{TARGET_TAG}.json\"\n",
    "\n",
    "# SOURCE sequences (05C output)\n",
    "SOURCE_SEQ_ROOT = REPO_ROOT / \"data/processed/session_sequences\" / f\"source_sessions_{SOURCE_TAG}\"\n",
    "source_train_dir = SOURCE_SEQ_ROOT / \"train\"\n",
    "source_val_dir   = SOURCE_SEQ_ROOT / \"val\"\n",
    "source_test_dir  = SOURCE_SEQ_ROOT / \"test\"\n",
    "source_vocab_json = SOURCE_SEQ_ROOT / f\"source_vocab_items_{SOURCE_TAG}.json\"\n",
    "\n",
    "print(\"[07-02] Target train pt:\", target_train_pt)\n",
    "print(\"[07-02] Source train dir:\", source_train_dir)\n",
    "\n",
    "# Existence checks (hard fail early)\n",
    "must_exist(target_train_pt, \"target_train_pt\")\n",
    "must_exist(target_val_pt, \"target_val_pt\")\n",
    "must_exist(target_test_pt, \"target_test_pt\")\n",
    "must_exist(target_vocab_json, \"target_vocab_json\")\n",
    "must_exist(target_tensor_meta_json, \"target_tensor_meta_json\")\n",
    "\n",
    "must_exist(source_train_dir, \"source_train_dir\")\n",
    "must_exist(source_val_dir, \"source_val_dir\")\n",
    "must_exist(source_test_dir, \"source_test_dir\")\n",
    "must_exist(source_vocab_json, \"source_vocab_json\")\n",
    "\n",
    "print(\"[07-02] ✅ All required artifacts exist\")\n",
    "\n",
    "print(\"\\n[07-02] CHECKPOINT B\")\n",
    "print(\"If any path is missing, paste the exact error here (do NOT proceed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46def6ec",
   "metadata": {},
   "source": [
    "Reuse protocol from 06: metric suite + PAD exclusion + K list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e99cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-03] Protocol from 06:\n",
      "  MAX_PREFIX_LEN: 20\n",
      "  CAP_ENABLED: True\n",
      "  CAP_SESSION_LEN: 200\n",
      "  CAP_STRATEGY: take_last\n",
      "  K_LIST: [5, 10, 20]\n",
      "[07-03] SOURCE: missing pad_token, fallback id=0\n",
      "[07-03] SOURCE: missing unk_token, fallback id=1\n",
      "[07-03] PAD_ID_TARGET: 0 | UNK_ID_TARGET: 1\n",
      "[07-03] PAD_ID_SOURCE: 0 | UNK_ID_SOURCE: 1\n",
      "[07-03] ✅ Metric functions ready (HR/MRR/NDCG @ K={5,10,20}, PAD excluded via ranking construction)\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-03] Reuse protocol from 06: metric suite + PAD exclusion + K list\n",
    "\n",
    "K_LIST = [5, 10, 20]\n",
    "MAX_K = max(K_LIST)\n",
    "\n",
    "proto = dataloader_cfg[\"protocol\"]\n",
    "MAX_PREFIX_LEN = int(proto[\"max_prefix_len\"])\n",
    "CAP_ENABLED = bool(proto[\"source_long_session_policy\"][\"enabled\"])\n",
    "CAP_SESSION_LEN = int(proto[\"source_long_session_policy\"][\"cap_session_len\"])\n",
    "CAP_STRATEGY = str(proto[\"source_long_session_policy\"][\"cap_strategy\"])\n",
    "\n",
    "print(\"[07-03] Protocol from 06:\")\n",
    "print(\"  MAX_PREFIX_LEN:\", MAX_PREFIX_LEN)\n",
    "print(\"  CAP_ENABLED:\", CAP_ENABLED)\n",
    "print(\"  CAP_SESSION_LEN:\", CAP_SESSION_LEN)\n",
    "print(\"  CAP_STRATEGY:\", CAP_STRATEGY)\n",
    "print(\"  K_LIST:\", K_LIST)\n",
    "\n",
    "# Load vocab files to get PAD/UNK\n",
    "target_vocab = load_json(target_vocab_json)\n",
    "source_vocab = load_json(source_vocab_json)\n",
    "\n",
    "def get_special_id(vocab_obj: dict, token_key: str, fallback: int, name: str) -> int:\n",
    "    tok = vocab_obj.get(token_key, None)\n",
    "    if tok is None:\n",
    "        print(f\"[07-03] {name}: missing {token_key}, fallback id={fallback}\")\n",
    "        return fallback\n",
    "    mapping = vocab_obj.get(\"vocab\", {})\n",
    "    if isinstance(mapping, dict) and tok in mapping and isinstance(mapping[tok], int):\n",
    "        return int(mapping[tok])\n",
    "    print(f\"[07-03] {name}: could not resolve {token_key}='{tok}' in vocab mapping, fallback id={fallback}\")\n",
    "    return fallback\n",
    "\n",
    "PAD_ID_TARGET = get_special_id(target_vocab, \"pad_token\", 0, \"TARGET\")\n",
    "UNK_ID_TARGET = get_special_id(target_vocab, \"unk_token\", 1, \"TARGET\")\n",
    "PAD_ID_SOURCE = get_special_id(source_vocab, \"pad_token\", 0, \"SOURCE\")\n",
    "UNK_ID_SOURCE = get_special_id(source_vocab, \"unk_token\", 1, \"SOURCE\")\n",
    "\n",
    "print(\"[07-03] PAD_ID_TARGET:\", PAD_ID_TARGET, \"| UNK_ID_TARGET:\", UNK_ID_TARGET)\n",
    "print(\"[07-03] PAD_ID_SOURCE:\", PAD_ID_SOURCE, \"| UNK_ID_SOURCE:\", UNK_ID_SOURCE)\n",
    "\n",
    "assert PAD_ID_TARGET == 0, \"Target PAD must be 0 to match 06.\"\n",
    "assert PAD_ID_SOURCE == 0, \"Source PAD must be 0 to match 06.\"\n",
    "\n",
    "def init_metrics():\n",
    "    return {f\"{m}@{k}\": 0.0 for m in [\"HR\", \"MRR\", \"NDCG\"] for k in K_LIST}\n",
    "\n",
    "def update_metrics_from_rank(metrics: dict, rank0: int):\n",
    "    # rank0: 0-based rank position within top-MAX_K list; if not found => None\n",
    "    if rank0 is None:\n",
    "        return\n",
    "    r = rank0 + 1  # 1-based\n",
    "    for k in K_LIST:\n",
    "        if r <= k:\n",
    "            metrics[f\"HR@{k}\"] += 1.0\n",
    "            metrics[f\"MRR@{k}\"] += 1.0 / r\n",
    "            metrics[f\"NDCG@{k}\"] += 1.0 / math.log2(r + 1.0)\n",
    "\n",
    "def finalize_metrics(metrics: dict, n: int) -> dict:\n",
    "    out = {}\n",
    "    for k, v in metrics.items():\n",
    "        out[k] = float(v / n) if n > 0 else 0.0\n",
    "    return out\n",
    "\n",
    "print(\"[07-03] ✅ Metric functions ready (HR/MRR/NDCG @ K={5,10,20}, PAD excluded via ranking construction)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09772b25",
   "metadata": {},
   "source": [
    "PyTorch 2.6+ compatibility: load project-generated .pt artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ca0d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-03A] ✅ Loader ready\n",
      "[07-03A] CHECKPOINT: rerun from CELL 07-04 after this.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-03A] PyTorch 2.6+ compatibility: load project-generated .pt artifacts\n",
    "# Our .pt artifacts are produced inside this repo, so we can safely set weights_only=False.\n",
    "\n",
    "import pickle\n",
    "\n",
    "def torch_load_repo_artifact(path, map_location=\"cpu\"):\n",
    "    path = str(path)\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=map_location, weights_only=False)\n",
    "        print(f\"[07-03A] torch.load OK (weights_only=False): {path}\")\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        # Older torch versions don't have weights_only argument\n",
    "        obj = torch.load(path, map_location=map_location)\n",
    "        print(f\"[07-03A] torch.load OK (no weights_only arg): {path}\")\n",
    "        return obj\n",
    "\n",
    "print(\"[07-03A] ✅ Loader ready\")\n",
    "print(\"[07-03A] CHECKPOINT: rerun from CELL 07-04 after this.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db215015",
   "metadata": {},
   "source": [
    "Robust vocab loader: infer vocab_size from stored structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5473260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-03B] TARGET: vocab_size from max(vocab values)+1 (token->id) = 747\n",
      "[07-03B] SOURCE: vocab_size from key 'vocab_size' = 1620\n",
      "[07-03B] ✅ vocab sizes inferred: vocab_size_target= 747 | vocab_size_source= 1620\n",
      "\n",
      "[07-03B] CHECKPOINT: paste these two inferred sizes + the 'token->id' or 'id->token' message.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-03B] Robust vocab loader: infer vocab_size from stored structures (includes 'vocab' key)\n",
    "\n",
    "def infer_vocab_size(vocab: dict, name: str) -> int:\n",
    "    \"\"\"\n",
    "    Infer vocab size from common formats:\n",
    "    - vocab_size / n_items\n",
    "    - id2item (list or dict)\n",
    "    - item2id (dict)\n",
    "    - items (list)\n",
    "    - vocab (dict)  <-- your format\n",
    "    \"\"\"\n",
    "    # direct keys\n",
    "    for k in [\"vocab_size\", \"n_items\", \"num_items\", \"size\"]:\n",
    "        if k in vocab:\n",
    "            vs = int(vocab[k])\n",
    "            print(f\"[07-03B] {name}: vocab_size from key '{k}' = {vs}\")\n",
    "            return vs\n",
    "\n",
    "    # your format: vocab\n",
    "    if \"vocab\" in vocab and isinstance(vocab[\"vocab\"], dict):\n",
    "        d = vocab[\"vocab\"]\n",
    "        # Determine whether mapping is token->id or id->token\n",
    "        sample_key = next(iter(d.keys())) if len(d) else None\n",
    "        sample_val = d[sample_key] if sample_key is not None else None\n",
    "\n",
    "        if sample_key is None:\n",
    "            print(f\"[07-03B] {name}: vocab empty -> vocab_size=0\")\n",
    "            return 0\n",
    "\n",
    "        # token -> id\n",
    "        if isinstance(sample_val, int):\n",
    "            ids = list(d.values())\n",
    "            vs = max(ids) + 1 if len(ids) else 0\n",
    "            print(f\"[07-03B] {name}: vocab_size from max(vocab values)+1 (token->id) = {vs}\")\n",
    "            return vs\n",
    "\n",
    "        # id(str/int) -> token\n",
    "        try:\n",
    "            keys_int = [int(k) for k in d.keys()]\n",
    "            vs = max(keys_int) + 1 if len(keys_int) else 0\n",
    "            print(f\"[07-03B] {name}: vocab_size from max(vocab keys)+1 (id->token) = {vs}\")\n",
    "            return vs\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # fallback: just number of entries\n",
    "        vs = len(d)\n",
    "        print(f\"[07-03B] {name}: vocab_size fallback len(vocab) = {vs}\")\n",
    "        return vs\n",
    "\n",
    "    # id2item\n",
    "    if \"id2item\" in vocab:\n",
    "        v = vocab[\"id2item\"]\n",
    "        if isinstance(v, list):\n",
    "            vs = len(v)\n",
    "            print(f\"[07-03B] {name}: vocab_size from len(id2item list) = {vs}\")\n",
    "            return vs\n",
    "        if isinstance(v, dict):\n",
    "            keys = [int(x) for x in v.keys()]\n",
    "            vs = (max(keys) + 1) if keys else 0\n",
    "            print(f\"[07-03B] {name}: vocab_size from max(id2item dict keys)+1 = {vs}\")\n",
    "            return vs\n",
    "\n",
    "    # items list\n",
    "    if \"items\" in vocab and isinstance(vocab[\"items\"], list):\n",
    "        vs = len(vocab[\"items\"])\n",
    "        print(f\"[07-03B] {name}: vocab_size from len(items) = {vs}\")\n",
    "        return vs\n",
    "\n",
    "    # item2id\n",
    "    if \"item2id\" in vocab and isinstance(vocab[\"item2id\"], dict):\n",
    "        d = vocab[\"item2id\"]\n",
    "        ids = list(d.values())\n",
    "        vs = (max(ids) + 1) if len(ids) else 0\n",
    "        print(f\"[07-03B] {name}: vocab_size from max(item2id values)+1 = {vs}\")\n",
    "        return vs\n",
    "\n",
    "    raise KeyError(f\"[07-03B] {name}: Could not infer vocab_size. Keys={list(vocab.keys())}\")\n",
    "\n",
    "# infer sizes for target + source\n",
    "vocab_size_target = infer_vocab_size(target_vocab, \"TARGET\")\n",
    "vocab_size_source = infer_vocab_size(source_vocab, \"SOURCE\")\n",
    "\n",
    "print(\"[07-03B] ✅ vocab sizes inferred:\",\n",
    "      \"vocab_size_target=\", vocab_size_target,\n",
    "      \"| vocab_size_source=\", vocab_size_source)\n",
    "\n",
    "print(\"\\n[07-03B] CHECKPOINT: paste these two inferred sizes + the 'token->id' or 'id->token' message.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48ab6a",
   "metadata": {},
   "source": [
    "TARGET MostPop (A): popularity from TARGET train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea5e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-03A] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[07-04] Loaded target train .pt type: <class 'dict'>\n",
      "[07-04] Keys: ['input_ids', 'attn_mask', 'labels', 'session_id', 'user_id', 't', 'split']\n",
      "[07-04] train_labels shape: (1944,) dtype: torch.int64\n",
      "[07-04] Target MostPop top20: [410, 408, 308, 85, 230, 309, 562, 532, 310, 533, 84, 231, 563, 229, 536, 126, 151, 311, 531, 534]\n",
      "[07-04] Target MostPop top20 counts: [30, 23, 20, 19, 18, 18, 18, 17, 15, 15, 14, 14, 14, 13, 13, 12, 12, 12, 12, 11]\n",
      "\n",
      "[07-04] CHECKPOINT C\n",
      "Paste the printed top20 + counts if you want me to sanity-check distribution before evaluation.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-04] TARGET MostPop (A): popularity from TARGET train split\n",
    "# Popularity definition (target): count next-item labels in target TRAIN split.\n",
    "\n",
    "train_obj = torch_load_repo_artifact(target_train_pt, map_location=\"cpu\")\n",
    "print(\"[07-04] Loaded target train .pt type:\", type(train_obj))\n",
    "\n",
    "# Robustly find labels tensor\n",
    "if isinstance(train_obj, dict):\n",
    "    keys = list(train_obj.keys())\n",
    "    print(\"[07-04] Keys:\", keys)\n",
    "    if \"labels\" in train_obj:\n",
    "        train_labels = train_obj[\"labels\"]\n",
    "    elif \"y\" in train_obj:\n",
    "        train_labels = train_obj[\"y\"]\n",
    "    else:\n",
    "        raise KeyError(f\"Could not find labels in target train object keys={keys}\")\n",
    "else:\n",
    "    raise TypeError(\"Unexpected target train object type; expected dict with labels.\")\n",
    "\n",
    "train_labels = torch.as_tensor(train_labels).detach().cpu().long()\n",
    "print(\"[07-04] train_labels shape:\", tuple(train_labels.shape), \"dtype:\", train_labels.dtype)\n",
    "\n",
    "# Count label frequencies\n",
    "# vocab_size_target = int(target_vocab[\"vocab_size\"]) vocab_size_target inferred in [CELL 07-03B]\n",
    "counts = np.zeros(vocab_size_target, dtype=np.int64)\n",
    "\n",
    "labels_np = train_labels.numpy()\n",
    "if labels_np.min() < 0 or labels_np.max() >= vocab_size_target:\n",
    "    raise ValueError(f\"Target labels out of range: min={labels_np.min()} max={labels_np.max()} vocab={vocab_size_target}\")\n",
    "\n",
    "# exclude PAD if it ever appears (shouldn't)\n",
    "labels_np = labels_np[labels_np != PAD_ID_TARGET]\n",
    "np.add.at(counts, labels_np, 1)\n",
    "\n",
    "# Build MostPop ranking (top MAX_K), excluding PAD\n",
    "forbidden = {PAD_ID_TARGET}\n",
    "\n",
    "all_items = np.arange(vocab_size_target, dtype=np.int64)\n",
    "valid_mask = np.ones(vocab_size_target, dtype=bool)\n",
    "for fid in forbidden:\n",
    "    if 0 <= fid < vocab_size_target:\n",
    "        valid_mask[fid] = False\n",
    "\n",
    "valid_items = all_items[valid_mask]\n",
    "valid_counts = counts[valid_mask]\n",
    "\n",
    "# argsort descending by count then item_id for stability\n",
    "order = np.lexsort((valid_items, -valid_counts))\n",
    "ranked_items = valid_items[order]\n",
    "top_items_target = ranked_items[:MAX_K].tolist()\n",
    "\n",
    "print(\"[07-04] Target MostPop top20:\", top_items_target)\n",
    "print(\"[07-04] Target MostPop top20 counts:\", [int(counts[i]) for i in top_items_target])\n",
    "\n",
    "print(\"\\n[07-04] CHECKPOINT C\")\n",
    "print(\"Paste the printed top20 + counts if you want me to sanity-check distribution before evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d27b2",
   "metadata": {},
   "source": [
    "TARGET eval on VAL + TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76dd25e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-03A] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[07-03A] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[07-05] TARGET VAL: {'HR@5': 0.07407407407407407, 'HR@10': 0.13227513227513227, 'HR@20': 0.1746031746031746, 'MRR@5': 0.05026455026455026, 'MRR@10': 0.058704963466868224, 'MRR@20': 0.06130174366942758, 'NDCG@5': 0.05620747419730002, 'NDCG@10': 0.07570020394614688, 'NDCG@20': 0.08597813493276235, '_n_examples': 189}\n",
      "[07-05] TARGET TEST: {'HR@5': 0.075, 'HR@10': 0.125, 'HR@20': 0.185, 'MRR@5': 0.045, 'MRR@10': 0.05177579365079365, 'MRR@20': 0.0553870221840423, 'NDCG@5': 0.05258000942002037, 'NDCG@10': 0.06885437485718231, 'NDCG@20': 0.08332921918090552, '_n_examples': 200}\n",
      "\n",
      "[07-05] CHECKPOINT D\n",
      "Paste TARGET VAL/TEST metrics here so we lock them in before source.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-05] TARGET eval on VAL + TEST\n",
    "\n",
    "def eval_target_split(pt_path: Path, top_items: list[int], split_name: str) -> dict:\n",
    "    obj = torch_load_repo_artifact(pt_path, map_location=\"cpu\")\n",
    "    if not isinstance(obj, dict):\n",
    "        raise TypeError(f\"{split_name}: expected dict .pt, got {type(obj)}\")\n",
    "\n",
    "    if \"labels\" in obj:\n",
    "        labels = obj[\"labels\"]\n",
    "    elif \"y\" in obj:\n",
    "        labels = obj[\"y\"]\n",
    "    else:\n",
    "        raise KeyError(f\"{split_name}: could not find labels key\")\n",
    "\n",
    "    labels = torch.as_tensor(labels).detach().cpu().long().numpy()\n",
    "    labels = labels[labels != PAD_ID_TARGET]\n",
    "\n",
    "    top_index = {item_id: idx for idx, item_id in enumerate(top_items)}\n",
    "    metrics = init_metrics()\n",
    "    n = 0\n",
    "\n",
    "    for y in labels:\n",
    "        n += 1\n",
    "        rank0 = top_index.get(int(y), None)\n",
    "        update_metrics_from_rank(metrics, rank0)\n",
    "\n",
    "    out = finalize_metrics(metrics, n)\n",
    "    out[\"_n_examples\"] = int(n)\n",
    "    return out\n",
    "\n",
    "t_val = eval_target_split(target_val_pt, top_items_target, \"target_val\")\n",
    "t_test = eval_target_split(target_test_pt, top_items_target, \"target_test\")\n",
    "\n",
    "print(\"[07-05] TARGET VAL:\", t_val)\n",
    "print(\"[07-05] TARGET TEST:\", t_test)\n",
    "\n",
    "print(\"\\n[07-05] CHECKPOINT D\")\n",
    "print(\"Paste TARGET VAL/TEST metrics here so we lock them in before source.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f940f32",
   "metadata": {},
   "source": [
    "SOURCE MostPop (B): popularity from SOURCE train session sequences (streaming / memory-safe)\n",
    "- Handles source sequences whose `items` are strings (course IDs), by mapping via source_vocab.\n",
    "- Must reuse protocol from 06: cap long sessions if enabled, PAD excluded from counting/ranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7628be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-06] Source shards counts: train= 1024 val= 1024 test= 1024\n",
      "[07-06] Probe columns: ['domain', 'user_id', 'session_id', 'session_length', 'start_ts', 'end_ts', 'items', 'split']\n",
      "[07-06] Detected source sequence column: items\n",
      "[07-06] Probe first seq type: <class 'numpy.ndarray'> | len: 71\n",
      "[07-06] Probe first element type: <class 'str'> | value sample: course-v1:TsinghuaX+00690212X+sp\n",
      "[07-06] source_vocab keys: ['run_tag_source', 'built_from', 'vocab_size', 'pad_id', 'unk_id', 'item2id']\n",
      "[07-06] Built source_token_to_id size: 1620\n",
      "[07-06] source_token_to_id id-range: {'min_id': 0, 'max_id': 1619, 'vocab_size_source': 1620}\n",
      "[07-06] ✅ Resolved SOURCE PAD/UNK: {'PAD_ID_SOURCE': 0, 'UNK_ID_SOURCE': 1}\n",
      "[07-06] Counted shards 50/1024 | sessions=325,575 events=5,721,409 unk=0 | elapsed=8.8s\n",
      "[07-06] Counted shards 100/1024 | sessions=650,531 events=11,461,691 unk=0 | elapsed=17.7s\n",
      "[07-06] Counted shards 150/1024 | sessions=977,651 events=17,233,757 unk=0 | elapsed=26.5s\n",
      "[07-06] Counted shards 200/1024 | sessions=1,304,244 events=23,006,952 unk=0 | elapsed=34.5s\n",
      "[07-06] Counted shards 250/1024 | sessions=1,629,328 events=28,735,371 unk=0 | elapsed=42.8s\n",
      "[07-06] Counted shards 300/1024 | sessions=1,954,910 events=34,459,106 unk=0 | elapsed=51.7s\n",
      "[07-06] Counted shards 350/1024 | sessions=2,280,185 events=40,202,207 unk=0 | elapsed=61.0s\n",
      "[07-06] Counted shards 400/1024 | sessions=2,606,674 events=45,995,055 unk=0 | elapsed=69.8s\n",
      "[07-06] Counted shards 450/1024 | sessions=2,933,414 events=51,753,823 unk=0 | elapsed=77.7s\n",
      "[07-06] Counted shards 500/1024 | sessions=3,259,886 events=57,541,958 unk=0 | elapsed=86.2s\n",
      "[07-06] Counted shards 550/1024 | sessions=3,584,635 events=63,290,589 unk=0 | elapsed=94.9s\n",
      "[07-06] Counted shards 600/1024 | sessions=3,910,276 events=69,054,672 unk=0 | elapsed=103.6s\n",
      "[07-06] Counted shards 650/1024 | sessions=4,235,275 events=74,794,425 unk=0 | elapsed=111.7s\n",
      "[07-06] Counted shards 700/1024 | sessions=4,560,993 events=80,567,327 unk=0 | elapsed=120.1s\n",
      "[07-06] Counted shards 750/1024 | sessions=4,886,691 events=86,314,332 unk=0 | elapsed=129.0s\n",
      "[07-06] Counted shards 800/1024 | sessions=5,212,194 events=92,057,411 unk=0 | elapsed=137.8s\n",
      "[07-06] Counted shards 850/1024 | sessions=5,538,081 events=97,809,599 unk=0 | elapsed=146.0s\n",
      "[07-06] Counted shards 900/1024 | sessions=5,863,399 events=103,561,826 unk=0 | elapsed=154.8s\n",
      "[07-06] Counted shards 950/1024 | sessions=6,189,198 events=109,319,546 unk=0 | elapsed=164.8s\n",
      "[07-06] Counted shards 1000/1024 | sessions=6,515,673 events=115,088,504 unk=0 | elapsed=173.9s\n",
      "[07-06] Source MostPop top20: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[07-06] Source MostPop top20 counts: [4902340, 2435199, 2119141, 2095212, 2055980, 1774362, 1740811, 1687189, 1662289, 1332019, 1264149, 1259914, 1056983, 1002134, 962313, 915507, 910339, 896850, 891279, 888669]\n",
      "[07-06] sessions_counted: 6,672,282 | events_counted: 117,842,545 | unk_mapped: 0\n",
      "\n",
      "[07-06] CHECKPOINT E\n",
      "Paste: seq_col, PAD/UNK resolved, top20+counts, sessions/events/unk_mapped.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-06] SOURCE MostPop (B): popularity from SOURCE train session sequences (streaming / memory-safe)\n",
    "# Handles source sequences whose `items` are strings (course IDs), by mapping via source_vocab.\n",
    "# Must reuse protocol from 06: cap long sessions if enabled, PAD excluded from counting/ranking.\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def list_parquet_shards(dir_path: Path) -> list[Path]:\n",
    "    files = sorted([Path(p) for p in glob.glob(str(dir_path / \"sessions_b*.parquet\"))])\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(f\"No shards found under {dir_path} (expected sessions_b*.parquet)\")\n",
    "    return files\n",
    "\n",
    "train_shards = list_parquet_shards(source_train_dir)\n",
    "val_shards   = list_parquet_shards(source_val_dir)\n",
    "test_shards  = list_parquet_shards(source_test_dir)\n",
    "\n",
    "print(\"[07-06] Source shards counts:\", \"train=\", len(train_shards), \"val=\", len(val_shards), \"test=\", len(test_shards))\n",
    "\n",
    "# Detect sequence column from first shard\n",
    "probe = pd.read_parquet(train_shards[0])\n",
    "print(\"[07-06] Probe columns:\", list(probe.columns))\n",
    "\n",
    "def detect_sequence_column(df: pd.DataFrame) -> str:\n",
    "    for c in [\"items\", \"item_ids\", \"sequence\", \"seq\", \"course_ids\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        s = df[c].dropna()\n",
    "        if len(s) == 0:\n",
    "            continue\n",
    "        v = s.iloc[0]\n",
    "        if isinstance(v, (list, tuple, np.ndarray)):\n",
    "            return c\n",
    "    raise KeyError(f\"Could not detect sequence column from columns={list(df.columns)}\")\n",
    "\n",
    "seq_col = detect_sequence_column(probe)\n",
    "print(\"[07-06] Detected source sequence column:\", seq_col)\n",
    "if len(probe) > 0:\n",
    "    first_seq = probe[seq_col].iloc[0]\n",
    "    print(\"[07-06] Probe first seq type:\", type(first_seq), \"| len:\", (len(first_seq) if first_seq is not None else None))\n",
    "    if isinstance(first_seq, np.ndarray) and len(first_seq) > 0:\n",
    "        print(\"[07-06] Probe first element type:\", type(first_seq[0]), \"| value sample:\", first_seq[0])\n",
    "\n",
    "# ---- Build mapping: token(string) -> id(int) from source_vocab ----\n",
    "print(\"[07-06] source_vocab keys:\", list(source_vocab.keys()))\n",
    "\n",
    "def build_token_to_id(vocab_obj: dict) -> dict:\n",
    "    # 1) preferred: vocab_obj[\"vocab\"] token->id\n",
    "    if \"vocab\" in vocab_obj and isinstance(vocab_obj[\"vocab\"], dict):\n",
    "        d = vocab_obj[\"vocab\"]\n",
    "        # token->id if values are int\n",
    "        sample_k = next(iter(d.keys())) if len(d) else None\n",
    "        if sample_k is not None and isinstance(d[sample_k], int):\n",
    "            return d\n",
    "        # if keys are ids and values are tokens, invert\n",
    "        try:\n",
    "            sample_key_int = int(sample_k)\n",
    "            # invert id->token to token->id\n",
    "            inv = {v: int(k) for k, v in d.items()}\n",
    "            return inv\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) fallback: item2id\n",
    "    if \"item2id\" in vocab_obj and isinstance(vocab_obj[\"item2id\"], dict):\n",
    "        return vocab_obj[\"item2id\"]\n",
    "\n",
    "    # 3) fallback: items list (position is id)\n",
    "    if \"items\" in vocab_obj and isinstance(vocab_obj[\"items\"], list):\n",
    "        return {tok: i for i, tok in enumerate(vocab_obj[\"items\"])}\n",
    "\n",
    "    raise KeyError(f\"[07-06] Could not build token_to_id from source_vocab. Keys={list(vocab_obj.keys())}\")\n",
    "\n",
    "source_token_to_id = build_token_to_id(source_vocab)\n",
    "print(\"[07-06] Built source_token_to_id size:\", len(source_token_to_id))\n",
    "\n",
    "# Inspect id range to confirm PAD/UNK reservation (protocol requires PAD excluded)\n",
    "ids = list(source_token_to_id.values())\n",
    "min_id = int(min(ids)) if len(ids) else None\n",
    "max_id = int(max(ids)) if len(ids) else None\n",
    "print(\"[07-06] source_token_to_id id-range:\", {\"min_id\": min_id, \"max_id\": max_id, \"vocab_size_source\": vocab_size_source})\n",
    "\n",
    "# Resolve PAD/UNK for source robustly:\n",
    "# - If explicit pad_id/unk_id exist, use them.\n",
    "# - Else if min_id >= 2, assume PAD=0 UNK=1 (reserved).\n",
    "# - Else STOP (cannot guarantee protocol correctness).\n",
    "def resolve_pad_unk_source(vocab_obj: dict, min_id: int | None) -> tuple[int, int]:\n",
    "    if \"pad_id\" in vocab_obj and \"unk_id\" in vocab_obj:\n",
    "        return int(vocab_obj[\"pad_id\"]), int(vocab_obj[\"unk_id\"])\n",
    "    if min_id is not None and min_id >= 2:\n",
    "        return 0, 1\n",
    "    raise ValueError(\n",
    "        \"[07-06] CHECKPOINT STOP: Source vocab ids appear to start at 0/1 (min_id < 2) \"\n",
    "        \"but pad_id/unk_id are not explicitly stored. We cannot safely exclude PAD per protocol 06.\\n\"\n",
    "        f\"source_vocab keys={list(vocab_obj.keys())}\\n\"\n",
    "        f\"min_id={min_id}, max_id={max_id}, vocab_size_source={vocab_size_source}\\n\"\n",
    "        \"Paste this error + the printed keys/range.\"\n",
    "    )\n",
    "\n",
    "PAD_ID_SOURCE, UNK_ID_SOURCE = resolve_pad_unk_source(source_vocab, min_id)\n",
    "print(\"[07-06] ✅ Resolved SOURCE PAD/UNK:\", {\"PAD_ID_SOURCE\": PAD_ID_SOURCE, \"UNK_ID_SOURCE\": UNK_ID_SOURCE})\n",
    "\n",
    "# Mapping function (handles numpy arrays of strings)\n",
    "def map_seq_to_ids(seq) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map a sequence (np.ndarray/list) of tokens (strings or ints) to int ids.\n",
    "    Unknown tokens -> UNK_ID_SOURCE.\n",
    "    \"\"\"\n",
    "    if seq is None:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # ensure list-like\n",
    "    if isinstance(seq, np.ndarray):\n",
    "        seq_list = seq.tolist()\n",
    "    else:\n",
    "        seq_list = list(seq)\n",
    "\n",
    "    if len(seq_list) == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # fast-path: already ints\n",
    "    if isinstance(seq_list[0], (int, np.integer)):\n",
    "        return np.asarray(seq_list, dtype=np.int64)\n",
    "\n",
    "    # token strings -> ids\n",
    "    out = np.fromiter((source_token_to_id.get(tok, UNK_ID_SOURCE) for tok in seq_list), dtype=np.int64)\n",
    "    return out\n",
    "\n",
    "# Count popularity from TRAIN shards\n",
    "src_counts = np.zeros(vocab_size_source, dtype=np.int64)\n",
    "t0 = time.time()\n",
    "n_sessions = 0\n",
    "n_events = 0\n",
    "n_unk_mapped = 0  # diagnostic\n",
    "\n",
    "for i, fp in enumerate(train_shards, 1):\n",
    "    df = pd.read_parquet(fp, columns=[seq_col])\n",
    "\n",
    "    for seq in df[seq_col].values:\n",
    "        if seq is None:\n",
    "            continue\n",
    "\n",
    "        # cap long sessions before mapping (slice works for arrays/lists)\n",
    "        if CAP_ENABLED and len(seq) > CAP_SESSION_LEN and CAP_STRATEGY == \"take_last\":\n",
    "            seq = seq[-CAP_SESSION_LEN:]\n",
    "\n",
    "        arr = map_seq_to_ids(seq)\n",
    "\n",
    "        # exclude PAD\n",
    "        arr = arr[arr != PAD_ID_SOURCE]\n",
    "        if arr.size == 0:\n",
    "            continue\n",
    "\n",
    "        # range check\n",
    "        if arr.min() < 0 or arr.max() >= vocab_size_source:\n",
    "            raise ValueError(f\"[07-06] Source mapped id out of range in {fp}: min={arr.min()} max={arr.max()} vocab={vocab_size_source}\")\n",
    "\n",
    "        n_sessions += 1\n",
    "        n_events += int(arr.size)\n",
    "        n_unk_mapped += int((arr == UNK_ID_SOURCE).sum())\n",
    "\n",
    "        np.add.at(src_counts, arr, 1)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[07-06] Counted shards {i}/{len(train_shards)} | sessions={n_sessions:,} events={n_events:,} unk={n_unk_mapped:,} | elapsed={dt:.1f}s\")\n",
    "\n",
    "# Build MostPop top items (exclude PAD)\n",
    "forbidden_src = {PAD_ID_SOURCE}\n",
    "\n",
    "all_src_items = np.arange(vocab_size_source, dtype=np.int64)\n",
    "valid_mask = np.ones(vocab_size_source, dtype=bool)\n",
    "for fid in forbidden_src:\n",
    "    if 0 <= fid < vocab_size_source:\n",
    "        valid_mask[fid] = False\n",
    "\n",
    "valid_items = all_src_items[valid_mask]\n",
    "valid_counts = src_counts[valid_mask]\n",
    "\n",
    "order = np.lexsort((valid_items, -valid_counts))  # count desc, id asc\n",
    "ranked_items = valid_items[order]\n",
    "top_items_source = ranked_items[:MAX_K].tolist()\n",
    "\n",
    "print(\"[07-06] Source MostPop top20:\", top_items_source[:20])\n",
    "print(\"[07-06] Source MostPop top20 counts:\", [int(src_counts[i]) for i in top_items_source[:20]])\n",
    "print(\"[07-06] sessions_counted:\", f\"{n_sessions:,}\", \"| events_counted:\", f\"{n_events:,}\", \"| unk_mapped:\", f\"{n_unk_mapped:,}\")\n",
    "\n",
    "print(\"\\n[07-06] CHECKPOINT E\")\n",
    "print(\"Paste: seq_col, PAD/UNK resolved, top20+counts, sessions/events/unk_mapped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77911d0c",
   "metadata": {},
   "source": [
    "SOURCE eval on VAL + TEST (streaming transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79a4f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-07] source_val: shards 50/1024 | sessions=40,753 pairs=674,824 unk_labels=2 | elapsed=1.8s\n",
      "[07-07] source_val: shards 100/1024 | sessions=81,398 pairs=1,338,427 unk_labels=2 | elapsed=3.6s\n",
      "[07-07] source_val: shards 150/1024 | sessions=122,208 pairs=2,013,484 unk_labels=2 | elapsed=5.7s\n",
      "[07-07] source_val: shards 200/1024 | sessions=163,159 pairs=2,698,196 unk_labels=2 | elapsed=8.2s\n",
      "[07-07] source_val: shards 250/1024 | sessions=203,996 pairs=3,374,411 unk_labels=2 | elapsed=10.2s\n",
      "[07-07] source_val: shards 300/1024 | sessions=244,632 pairs=4,051,466 unk_labels=2 | elapsed=11.9s\n",
      "[07-07] source_val: shards 350/1024 | sessions=285,035 pairs=4,725,515 unk_labels=2 | elapsed=13.7s\n",
      "[07-07] source_val: shards 400/1024 | sessions=325,724 pairs=5,406,295 unk_labels=2 | elapsed=15.4s\n",
      "[07-07] source_val: shards 450/1024 | sessions=366,361 pairs=6,081,977 unk_labels=2 | elapsed=17.9s\n",
      "[07-07] source_val: shards 500/1024 | sessions=407,196 pairs=6,756,214 unk_labels=4 | elapsed=20.2s\n",
      "[07-07] source_val: shards 550/1024 | sessions=447,490 pairs=7,423,047 unk_labels=4 | elapsed=22.0s\n",
      "[07-07] source_val: shards 600/1024 | sessions=488,570 pairs=8,103,477 unk_labels=4 | elapsed=23.7s\n",
      "[07-07] source_val: shards 650/1024 | sessions=529,230 pairs=8,776,309 unk_labels=4 | elapsed=25.4s\n",
      "[07-07] source_val: shards 700/1024 | sessions=569,943 pairs=9,450,952 unk_labels=4 | elapsed=27.4s\n",
      "[07-07] source_val: shards 750/1024 | sessions=610,555 pairs=10,120,199 unk_labels=4 | elapsed=29.8s\n",
      "[07-07] source_val: shards 800/1024 | sessions=651,167 pairs=10,797,820 unk_labels=4 | elapsed=31.9s\n",
      "[07-07] source_val: shards 850/1024 | sessions=691,874 pairs=11,468,395 unk_labels=4 | elapsed=33.6s\n",
      "[07-07] source_val: shards 900/1024 | sessions=732,510 pairs=12,137,527 unk_labels=4 | elapsed=35.3s\n",
      "[07-07] source_val: shards 950/1024 | sessions=773,272 pairs=12,811,393 unk_labels=4 | elapsed=37.0s\n",
      "[07-07] source_val: shards 1000/1024 | sessions=813,818 pairs=13,483,409 unk_labels=4 | elapsed=39.4s\n",
      "[07-07] source_test: shards 50/1024 | sessions=40,673 pairs=668,289 unk_labels=0 | elapsed=2.2s\n",
      "[07-07] source_test: shards 100/1024 | sessions=81,515 pairs=1,345,802 unk_labels=0 | elapsed=4.0s\n",
      "[07-07] source_test: shards 150/1024 | sessions=122,292 pairs=2,017,908 unk_labels=0 | elapsed=5.8s\n",
      "[07-07] source_test: shards 200/1024 | sessions=163,104 pairs=2,692,241 unk_labels=0 | elapsed=7.6s\n",
      "[07-07] source_test: shards 250/1024 | sessions=204,147 pairs=3,377,863 unk_labels=0 | elapsed=10.1s\n",
      "[07-07] source_test: shards 300/1024 | sessions=245,282 pairs=4,069,484 unk_labels=0 | elapsed=12.7s\n",
      "[07-07] source_test: shards 350/1024 | sessions=285,878 pairs=4,748,159 unk_labels=0 | elapsed=14.5s\n",
      "[07-07] source_test: shards 400/1024 | sessions=326,900 pairs=5,431,686 unk_labels=0 | elapsed=16.3s\n",
      "[07-07] source_test: shards 450/1024 | sessions=367,796 pairs=6,113,790 unk_labels=0 | elapsed=18.1s\n",
      "[07-07] source_test: shards 500/1024 | sessions=408,633 pairs=6,802,590 unk_labels=0 | elapsed=20.0s\n",
      "[07-07] source_test: shards 550/1024 | sessions=449,518 pairs=7,492,566 unk_labels=2 | elapsed=22.6s\n",
      "[07-07] source_test: shards 600/1024 | sessions=489,990 pairs=8,166,566 unk_labels=6 | elapsed=25.0s\n",
      "[07-07] source_test: shards 650/1024 | sessions=530,637 pairs=8,844,409 unk_labels=6 | elapsed=26.8s\n",
      "[07-07] source_test: shards 700/1024 | sessions=571,285 pairs=9,518,511 unk_labels=6 | elapsed=28.5s\n",
      "[07-07] source_test: shards 750/1024 | sessions=611,856 pairs=10,198,288 unk_labels=6 | elapsed=30.1s\n",
      "[07-07] source_test: shards 800/1024 | sessions=652,616 pairs=10,879,998 unk_labels=6 | elapsed=31.9s\n",
      "[07-07] source_test: shards 850/1024 | sessions=693,289 pairs=11,560,120 unk_labels=11 | elapsed=34.3s\n",
      "[07-07] source_test: shards 900/1024 | sessions=734,022 pairs=12,237,718 unk_labels=19 | elapsed=36.6s\n",
      "[07-07] source_test: shards 950/1024 | sessions=774,845 pairs=12,930,918 unk_labels=19 | elapsed=38.3s\n",
      "[07-07] source_test: shards 1000/1024 | sessions=815,494 pairs=13,606,435 unk_labels=19 | elapsed=40.0s\n",
      "[07-07] SOURCE VAL: {'HR@5': 0.11741418178221939, 'HR@10': 0.18701743950367639, 'HR@20': 0.27324388679120887, 'MRR@5': 0.06704973488080263, 'MRR@10': 0.07616040761226384, 'MRR@20': 0.08207514133001786, 'NDCG@5': 0.0794066625164993, 'NDCG@10': 0.10173664076730178, 'NDCG@20': 0.1234496055743828, '_n_pairs': 13802285, '_n_sessions_seen': 833042, '_n_unk_labels': 9}\n",
      "[07-07] SOURCE TEST: {'HR@5': 0.11623230488663819, 'HR@10': 0.18684090886912577, 'HR@20': 0.27260656388896704, 'MRR@5': 0.06676961370437104, 'MRR@10': 0.07602570969994939, 'MRR@20': 0.08191117415357692, 'NDCG@5': 0.07890570917467557, 'NDCG@10': 0.1015724898141176, 'NDCG@20': 0.12317230266785058, '_n_pairs': 13936616, '_n_sessions_seen': 835233, '_n_unk_labels': 19}\n",
      "\n",
      "[07-07] CHECKPOINT F\n",
      "Paste SOURCE VAL/TEST metrics (incl _n_pairs/_n_sessions_seen/_n_unk_labels) before writing reports.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-07] SOURCE eval on VAL + TEST (streaming transitions)\n",
    "# Match Notebook 06 protocol:\n",
    "# for each session seq length L: pairs t=1..L-1, label = seq[t]\n",
    "# For MostPop, input unused but label counting must be exact.\n",
    "# Uses the SAME map_seq_to_ids() and PAD/UNK resolved in 07-06.\n",
    "\n",
    "def eval_source_split(shards: list[Path], top_items: list[int], split_name: str) -> dict:\n",
    "    top_index = {item_id: idx for idx, item_id in enumerate(top_items)}\n",
    "    metrics = init_metrics()\n",
    "\n",
    "    n_pairs = 0\n",
    "    n_sessions_seen = 0\n",
    "    n_unk_labels = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i, fp in enumerate(shards, 1):\n",
    "        df = pd.read_parquet(fp, columns=[seq_col])\n",
    "\n",
    "        for seq in df[seq_col].values:\n",
    "            if seq is None:\n",
    "                continue\n",
    "\n",
    "            if CAP_ENABLED and len(seq) > CAP_SESSION_LEN and CAP_STRATEGY == \"take_last\":\n",
    "                seq = seq[-CAP_SESSION_LEN:]\n",
    "\n",
    "            arr = map_seq_to_ids(seq)\n",
    "\n",
    "            # exclude PAD anywhere (shouldn't appear in raw, but protocol says exclude)\n",
    "            arr = arr[arr != PAD_ID_SOURCE]\n",
    "            L = int(arr.size)\n",
    "            if L < 2:\n",
    "                continue\n",
    "\n",
    "            n_sessions_seen += 1\n",
    "\n",
    "            # labels are arr[1:]\n",
    "            labels = arr[1:]\n",
    "            n_pairs += int(labels.size)\n",
    "            n_unk_labels += int((labels == UNK_ID_SOURCE).sum())\n",
    "\n",
    "            # MostPop ranking check\n",
    "            for y in labels:\n",
    "                rank0 = top_index.get(int(y), None)\n",
    "                update_metrics_from_rank(metrics, rank0)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            dt = time.time() - t0\n",
    "            print(f\"[07-07] {split_name}: shards {i}/{len(shards)} | sessions={n_sessions_seen:,} pairs={n_pairs:,} unk_labels={n_unk_labels:,} | elapsed={dt:.1f}s\")\n",
    "\n",
    "    out = finalize_metrics(metrics, n_pairs)\n",
    "    out[\"_n_pairs\"] = int(n_pairs)\n",
    "    out[\"_n_sessions_seen\"] = int(n_sessions_seen)\n",
    "    out[\"_n_unk_labels\"] = int(n_unk_labels)\n",
    "    return out\n",
    "\n",
    "s_val = eval_source_split(val_shards, top_items_source, \"source_val\")\n",
    "s_test = eval_source_split(test_shards, top_items_source, \"source_test\")\n",
    "\n",
    "print(\"[07-07] SOURCE VAL:\", s_val)\n",
    "print(\"[07-07] SOURCE TEST:\", s_test)\n",
    "\n",
    "print(\"\\n[07-07] CHECKPOINT F\")\n",
    "print(\"Paste SOURCE VAL/TEST metrics (incl _n_pairs/_n_sessions_seen/_n_unk_labels) before writing reports.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde71b9e",
   "metadata": {},
   "source": [
    "Write report artifacts to reports/07_mostpop_baseline/<RUN_TAG>/ + update meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a313e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07-08] ✅ Wrote report files under: C:\\mooc-coldstart-session-meta\\reports\\07_mostpop_baseline\\20260102_133019\n",
      "[07-08] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[07-08] CHECKPOINT G\n",
      "Paste: report_dir path + confirm meta.json updated.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-08] Write report artifacts to reports/07_mostpop_baseline/<RUN_TAG>/ + update meta.json\n",
    "# Writes:\n",
    "# - run_meta.json\n",
    "# - results.json\n",
    "# - target_mostpop_top20.csv  (top MAX_K, despite name)\n",
    "# - source_mostpop_top20.csv  (top MAX_K, despite name)\n",
    "# Updates repo meta.json (append-only record for this run)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "REPORT_DIR = REPO_ROOT / \"reports\" / \"07_mostpop_baseline\" / RUN_TAG\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj: dict, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "run_meta = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"inputs\": {\n",
    "        \"target_run_tag\": TARGET_TAG,\n",
    "        \"source_run_tag\": SOURCE_TAG,\n",
    "        \"target_train_pt\": str(target_train_pt),\n",
    "        \"target_val_pt\": str(target_val_pt),\n",
    "        \"target_test_pt\": str(target_test_pt),\n",
    "        \"target_vocab_json\": str(target_vocab_json),\n",
    "        \"target_tensor_metadata_json\": str(target_tensor_meta_json),\n",
    "        \"source_train_dir\": str(source_train_dir),\n",
    "        \"source_val_dir\": str(source_val_dir),\n",
    "        \"source_test_dir\": str(source_test_dir),\n",
    "        \"source_vocab_json\": str(source_vocab_json),\n",
    "        \"dataloader_config\": str(cfg_path_repo),\n",
    "        \"sanity_metrics\": str(sanity_path_repo),\n",
    "        \"session_gap_thresholds\": str(gaps_path_repo),\n",
    "    },\n",
    "    \"protocol\": {\n",
    "        \"K_LIST\": K_LIST,\n",
    "        \"MAX_PREFIX_LEN\": MAX_PREFIX_LEN,\n",
    "        \"PAD_ID_TARGET\": PAD_ID_TARGET,\n",
    "        \"PAD_ID_SOURCE\": PAD_ID_SOURCE,\n",
    "        \"UNK_ID_TARGET\": UNK_ID_TARGET,\n",
    "        \"UNK_ID_SOURCE\": UNK_ID_SOURCE,\n",
    "        \"vocab_size_target\": int(vocab_size_target),\n",
    "        \"vocab_size_source\": int(vocab_size_source),\n",
    "        \"source_long_session_policy\": {\n",
    "            \"enabled\": CAP_ENABLED,\n",
    "            \"cap_session_len\": CAP_SESSION_LEN,\n",
    "            \"cap_strategy\": CAP_STRATEGY,\n",
    "        },\n",
    "        \"popularity_definition\": {\n",
    "            \"target\": \"count next-item labels in target TRAIN split\",\n",
    "            \"source\": \"count all item occurrences in source TRAIN session sequences (mapped via source_vocab)\",\n",
    "        },\n",
    "        \"pad_excluded_from_ranking\": True,\n",
    "    },\n",
    "    \"source_mapping_diagnostics\": {\n",
    "        \"seq_col\": seq_col,\n",
    "        \"source_vocab_keys\": list(source_vocab.keys()),\n",
    "        \"source_token_to_id_size\": int(len(source_token_to_id)),\n",
    "        \"source_id_range\": {\"min_id\": int(min_id) if min_id is not None else None, \"max_id\": int(max_id) if max_id is not None else None},\n",
    "        \"unk_mapped_in_train_counts\": int(n_unk_mapped),\n",
    "    },\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"target\": {\n",
    "        \"mostpop_top_items@20\": top_items_target[:20],\n",
    "        \"val\": t_val,\n",
    "        \"test\": t_test,\n",
    "    },\n",
    "    \"source\": {\n",
    "        \"mostpop_top_items@20\": top_items_source[:20],\n",
    "        \"val\": s_val,\n",
    "        \"test\": s_test,\n",
    "        \"train_counts_summary\": {\n",
    "            \"sessions_counted\": int(n_sessions),\n",
    "            \"events_counted\": int(n_events),\n",
    "            \"unk_mapped\": int(n_unk_mapped),\n",
    "            \"seq_col\": seq_col,\n",
    "            \"n_train_shards\": int(len(train_shards)),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "save_json(run_meta, REPORT_DIR / \"run_meta.json\")\n",
    "save_json(results, REPORT_DIR / \"results.json\")\n",
    "\n",
    "# Save top items CSVs (top MAX_K)\n",
    "pd.DataFrame({\n",
    "    \"rank\": np.arange(1, MAX_K + 1),\n",
    "    \"item_id\": top_items_target,\n",
    "    \"train_label_count\": [int(counts[i]) for i in top_items_target],\n",
    "}).to_csv(REPORT_DIR / \"target_mostpop_top20.csv\", index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"rank\": np.arange(1, MAX_K + 1),\n",
    "    \"item_id\": top_items_source,\n",
    "    \"train_event_count\": [int(src_counts[i]) for i in top_items_source],\n",
    "}).to_csv(REPORT_DIR / \"source_mostpop_top20.csv\", index=False)\n",
    "\n",
    "print(\"[07-08] ✅ Wrote report files under:\", REPORT_DIR)\n",
    "\n",
    "# Update meta.json (append-only record)\n",
    "meta_path = REPO_ROOT / \"meta.json\"\n",
    "if meta_path.exists():\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "else:\n",
    "    meta = {\"artifacts\": {}}\n",
    "\n",
    "meta.setdefault(\"artifacts\", {})\n",
    "meta[\"artifacts\"].setdefault(\"mostpop_baseline\", {})\n",
    "meta[\"artifacts\"][\"mostpop_baseline\"][RUN_TAG] = {\n",
    "    \"target_run_tag\": TARGET_TAG,\n",
    "    \"source_run_tag\": SOURCE_TAG,\n",
    "    \"report_dir\": str(REPORT_DIR),\n",
    "    \"results_json\": str(REPORT_DIR / \"results.json\"),\n",
    "    \"run_meta_json\": str(REPORT_DIR / \"run_meta.json\"),\n",
    "}\n",
    "meta[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "save_json(meta, meta_path)\n",
    "print(\"[07-08] ✅ Updated meta.json:\", meta_path)\n",
    "\n",
    "print(\"\\n[07-08] CHECKPOINT G\")\n",
    "print(\"Paste: report_dir path + confirm meta.json updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c07e2",
   "metadata": {},
   "source": [
    "Footer summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d190630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 07 MostPop Baseline Summary ==========\n",
      "RUN_TAG: 20260102_133019\n",
      "--- TARGET ---\n",
      "VAL : {'HR@5': 0.07407407407407407, 'HR@10': 0.13227513227513227, 'HR@20': 0.1746031746031746, 'MRR@5': 0.05026455026455026, 'MRR@10': 0.058704963466868224, 'MRR@20': 0.06130174366942758, 'NDCG@5': 0.05620747419730002, 'NDCG@10': 0.07570020394614688, 'NDCG@20': 0.08597813493276235, '_n_examples': 189}\n",
      "TEST: {'HR@5': 0.075, 'HR@10': 0.125, 'HR@20': 0.185, 'MRR@5': 0.045, 'MRR@10': 0.05177579365079365, 'MRR@20': 0.0553870221840423, 'NDCG@5': 0.05258000942002037, 'NDCG@10': 0.06885437485718231, 'NDCG@20': 0.08332921918090552, '_n_examples': 200}\n",
      "--- SOURCE ---\n",
      "VAL : {'HR@5': 0.11741418178221939, 'HR@10': 0.18701743950367639, 'HR@20': 0.27324388679120887, 'MRR@5': 0.06704973488080263, 'MRR@10': 0.07616040761226384, 'MRR@20': 0.08207514133001786, 'NDCG@5': 0.0794066625164993, 'NDCG@10': 0.10173664076730178, 'NDCG@20': 0.1234496055743828, '_n_pairs': 13802285, '_n_sessions_seen': 833042, '_n_unk_labels': 9}\n",
      "TEST: {'HR@5': 0.11623230488663819, 'HR@10': 0.18684090886912577, 'HR@20': 0.27260656388896704, 'MRR@5': 0.06676961370437104, 'MRR@10': 0.07602570969994939, 'MRR@20': 0.08191117415357692, 'NDCG@5': 0.07890570917467557, 'NDCG@10': 0.1015724898141176, 'NDCG@20': 0.12317230266785058, '_n_pairs': 13936616, '_n_sessions_seen': 835233, '_n_unk_labels': 19}\n",
      "Report dir: C:\\mooc-coldstart-session-meta\\reports\\07_mostpop_baseline\\20260102_133019\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07-09] Footer summary\n",
    "\n",
    "print(\"========== 07 MostPop Baseline Summary ==========\")\n",
    "print(\"RUN_TAG:\", RUN_TAG)\n",
    "print(\"--- TARGET ---\")\n",
    "print(\"VAL :\", t_val)\n",
    "print(\"TEST:\", t_test)\n",
    "print(\"--- SOURCE ---\")\n",
    "print(\"VAL :\", s_val)\n",
    "print(\"TEST:\", s_test)\n",
    "print(\"Report dir:\", REPORT_DIR)\n",
    "print(\"===============================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
