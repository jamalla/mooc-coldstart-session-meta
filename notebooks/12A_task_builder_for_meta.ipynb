{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3963a8",
   "metadata": {},
   "source": [
    "Imports + versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abcacac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-00] torch: 2.9.1+cpu\n",
      "[12A-00] pandas: 2.3.3\n",
      "[12A-00] numpy: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-00] Imports + versions\n",
    "import os, json, time, math, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"[12A-00] torch:\", torch.__version__)\n",
    "print(\"[12A-00] pandas:\", pd.__version__)\n",
    "print(\"[12A-00] numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a2aae",
   "metadata": {},
   "source": [
    "Repo root (portable) + load single source-of-truth JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddf79b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[12A-01] RUN_TAG: 20260104_141727\n",
      "[12A-01] Expect config: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[12A-01] Expect sanity: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\sanity_metrics_20251229_163357_20251229_232834.json\n",
      "[12A-01] Expect gaps  : C:\\mooc-coldstart-session-meta\\data\\processed\\normalized_events\\session_gap_thresholds.json\n",
      "[12A-01] target: gap_minutes from primary_threshold_seconds=1800 -> 30m | label=30m\n",
      "[12A-01] source: gap_minutes from primary_threshold_seconds=600 -> 10m | label=10m\n",
      "[12A-01] ✅ Session gaps confirmed.\n",
      "\n",
      "[12A-01] CHECKPOINT A\n",
      "Paste: inferred gaps lines + confirm asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-01] Repo root (portable) + load single source-of-truth JSONs\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    # fallback: git root\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not locate repo root (PROJECT_STATE.md or .git).\")\n",
    "\n",
    "REPO_ROOT = Path(os.getenv(\"MOOC_REPO_ROOT\", \"\")).expanduser().resolve() if os.getenv(\"MOOC_REPO_ROOT\") else find_repo_root(Path.cwd().resolve())\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[12A-01] REPO_ROOT:\", str(REPO_ROOT))\n",
    "print(\"[12A-01] RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "CFG_PATH = REPO_ROOT / \"data/processed/supervised/dataloader_config_20251229_163357_20251229_232834.json\"\n",
    "SANITY_PATH = REPO_ROOT / \"data/processed/supervised/sanity_metrics_20251229_163357_20251229_232834.json\"\n",
    "GAPS_PATH = REPO_ROOT / \"data/processed/normalized_events/session_gap_thresholds.json\"\n",
    "\n",
    "print(\"[12A-01] Expect config:\", CFG_PATH)\n",
    "print(\"[12A-01] Expect sanity:\", SANITY_PATH)\n",
    "print(\"[12A-01] Expect gaps  :\", GAPS_PATH)\n",
    "\n",
    "assert CFG_PATH.exists(), f\"Missing: {CFG_PATH}\"\n",
    "assert SANITY_PATH.exists(), f\"Missing: {SANITY_PATH}\"\n",
    "assert GAPS_PATH.exists(), f\"Missing: {GAPS_PATH}\"\n",
    "\n",
    "DL_CFG = json.loads(CFG_PATH.read_text(encoding=\"utf-8\"))\n",
    "SANITY = json.loads(SANITY_PATH.read_text(encoding=\"utf-8\"))\n",
    "GAPS = json.loads(GAPS_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def infer_gap_minutes(d: dict, name: str) -> int:\n",
    "    if \"gap_minutes\" in d:  # legacy\n",
    "        return int(d[\"gap_minutes\"])\n",
    "    if \"primary_threshold_seconds\" in d:\n",
    "        sec = int(d[\"primary_threshold_seconds\"])\n",
    "        return sec // 60\n",
    "    raise KeyError(f\"[12A-01] {name}: cannot infer gap minutes. keys={list(d.keys())}\")\n",
    "\n",
    "gap_target_m = infer_gap_minutes(GAPS[\"target\"], \"target\")\n",
    "gap_source_m = infer_gap_minutes(GAPS[\"source\"], \"source\")\n",
    "print(f\"[12A-01] target: gap_minutes from primary_threshold_seconds={GAPS['target'].get('primary_threshold_seconds')} -> {gap_target_m}m | label={GAPS['target'].get('primary_threshold_label')}\")\n",
    "print(f\"[12A-01] source: gap_minutes from primary_threshold_seconds={GAPS['source'].get('primary_threshold_seconds')} -> {gap_source_m}m | label={GAPS['source'].get('primary_threshold_label')}\")\n",
    "\n",
    "assert gap_target_m == 30, f\"target gap mismatch: got {gap_target_m}m\"\n",
    "assert gap_source_m == 10, f\"source gap mismatch: got {gap_source_m}m\"\n",
    "print(\"[12A-01] ✅ Session gaps confirmed.\")\n",
    "\n",
    "print(\"\\n[12A-01] CHECKPOINT A\")\n",
    "print(\"Paste: inferred gaps lines + confirm asserts passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf94ed",
   "metadata": {},
   "source": [
    "Load SOURCE vocab + list shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31060f3f",
   "metadata": {},
   "source": [
    "**CRITICAL CHECK**: Probe unique domains across shards (verify multi-task assumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95f1d9",
   "metadata": {},
   "source": [
    "Stable hash sampling + session→pairs (left-pad to MAX_PREFIX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "633a7814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-02] shards: train= 1024 val= 1024 test= 1024\n",
      "[12A-02] source_vocab keys: ['run_tag_source', 'built_from', 'vocab_size', 'pad_id', 'unk_id', 'item2id']\n",
      "[12A-02] VOCAB_SIZE_SOURCE: 1620\n",
      "[12A-02] PAD/UNK: 0 1\n",
      "\n",
      "[12A-02] CHECKPOINT B\n",
      "Confirm: shards are 1024 each + vocab_size/pad/unk printed.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-02] Source artifacts + vocab\n",
    "SRC_ROOT = REPO_ROOT / \"data/processed/session_sequences/source_sessions_20251229_232834\"\n",
    "SRC_TRAIN_DIR = SRC_ROOT / \"train\"\n",
    "SRC_VAL_DIR   = SRC_ROOT / \"val\"\n",
    "SRC_TEST_DIR  = SRC_ROOT / \"test\"\n",
    "SRC_VOCAB_PATH = SRC_ROOT / \"source_vocab_items_20251229_232834.json\"\n",
    "\n",
    "for p in [SRC_TRAIN_DIR, SRC_VAL_DIR, SRC_TEST_DIR, SRC_VOCAB_PATH]:\n",
    "    assert p.exists(), f\"Missing: {p}\"\n",
    "\n",
    "def list_parquet_files(d: Path):\n",
    "    files = sorted([p for p in d.glob(\"*.parquet\")])\n",
    "    return files\n",
    "\n",
    "train_files = list_parquet_files(SRC_TRAIN_DIR)\n",
    "val_files   = list_parquet_files(SRC_VAL_DIR)\n",
    "test_files  = list_parquet_files(SRC_TEST_DIR)\n",
    "\n",
    "print(\"[12A-02] shards: train=\", len(train_files), \"val=\", len(val_files), \"test=\", len(test_files))\n",
    "assert len(train_files) == 1024 and len(val_files) == 1024 and len(test_files) == 1024, \"Shard count drift (expected 1024 each).\"\n",
    "\n",
    "source_vocab = json.loads(SRC_VOCAB_PATH.read_text(encoding=\"utf-8\"))\n",
    "print(\"[12A-02] source_vocab keys:\", list(source_vocab.keys()))\n",
    "VOCAB_SIZE_SOURCE = int(source_vocab[\"vocab_size\"])\n",
    "PAD_ID_SOURCE = int(source_vocab.get(\"pad_id\", 0))\n",
    "UNK_ID_SOURCE = int(source_vocab.get(\"unk_id\", 1))\n",
    "item2id = source_vocab[\"item2id\"]\n",
    "\n",
    "assert len(item2id) == VOCAB_SIZE_SOURCE, f\"item2id size mismatch: {len(item2id)} vs vocab_size={VOCAB_SIZE_SOURCE}\"\n",
    "\n",
    "print(\"[12A-02] VOCAB_SIZE_SOURCE:\", VOCAB_SIZE_SOURCE)\n",
    "print(\"[12A-02] PAD/UNK:\", PAD_ID_SOURCE, UNK_ID_SOURCE)\n",
    "\n",
    "print(\"\\n[12A-02] CHECKPOINT B\")\n",
    "print(\"Confirm: shards are 1024 each + vocab_size/pad/unk printed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9242411",
   "metadata": {},
   "source": [
    "Normalize protocol fields from dataloader_config (Notebook 06 compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66f1f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-03] protocol keys: ['max_prefix_len', 'source_vocab_mode', 'source_pair_rule', 'source_long_session_policy', 'dataloader', 'seeds']\n",
      "[12A-03] source_long_session_policy keys: ['enabled', 'cap_session_len', 'cap_strategy']\n",
      "[12A-03] ✅ PROTO: {'K_LIST': [5, 10, 20], 'MAX_PREFIX_LEN': 20, 'CAP_ENABLED': True, 'CAP_SESSION_LEN': 200, 'CAP_STRATEGY': 'take_last'}\n",
      "[12A-03] ✅ PROTO asserts passed (matches Notebook 06).\n",
      "\n",
      "[12A-03] CHECKPOINT C\n",
      "Paste: protocol keys + source_long_session_policy keys + PROTO.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-03] Normalize protocol fields from dataloader_config (Notebook 06 compatibility)\n",
    "PROTO_RAW = DL_CFG.get(\"protocol\", {})\n",
    "\n",
    "# protocol keys in your config are nested; normalize to canonical fields used everywhere.\n",
    "def normalize_proto(d: dict) -> dict:\n",
    "    # fixed per repo decisions\n",
    "    max_prefix_len = int(d.get(\"max_prefix_len\", 20))\n",
    "    long_pol = d.get(\"source_long_session_policy\", {})\n",
    "    cap_enabled = bool(long_pol.get(\"enabled\", True))\n",
    "    cap_session_len = int(long_pol.get(\"cap_session_len\", 200))\n",
    "    cap_strategy = str(long_pol.get(\"cap_strategy\", \"take_last\"))\n",
    "\n",
    "    # K_LIST is fixed by protocol in Notebook 06 (even if not stored under protocol)\n",
    "    K_LIST = [5, 10, 20]\n",
    "\n",
    "    return {\n",
    "        \"K_LIST\": K_LIST,\n",
    "        \"MAX_PREFIX_LEN\": max_prefix_len,\n",
    "        \"CAP_ENABLED\": cap_enabled,\n",
    "        \"CAP_SESSION_LEN\": cap_session_len,\n",
    "        \"CAP_STRATEGY\": cap_strategy,\n",
    "    }\n",
    "\n",
    "PROTO = normalize_proto(PROTO_RAW)\n",
    "print(\"[12A-03] protocol keys:\", list(PROTO_RAW.keys()))\n",
    "print(\"[12A-03] source_long_session_policy keys:\", list(PROTO_RAW.get(\"source_long_session_policy\", {}).keys()))\n",
    "print(\"[12A-03] ✅ PROTO:\", PROTO)\n",
    "\n",
    "# Hard asserts to prevent drift\n",
    "assert PROTO[\"K_LIST\"] == [5, 10, 20]\n",
    "assert int(PROTO[\"MAX_PREFIX_LEN\"]) == 20\n",
    "assert bool(PROTO[\"CAP_ENABLED\"]) is True\n",
    "assert int(PROTO[\"CAP_SESSION_LEN\"]) == 200\n",
    "assert str(PROTO[\"CAP_STRATEGY\"]) == \"take_last\"\n",
    "print(\"[12A-03] ✅ PROTO asserts passed (matches Notebook 06).\")\n",
    "\n",
    "print(\"\\n[12A-03] CHECKPOINT C\")\n",
    "print(\"Paste: protocol keys + source_long_session_policy keys + PROTO.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f3986",
   "metadata": {},
   "source": [
    "Streaming episode builder (task = domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed9a4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-04] Built source_token_to_id size=1,620\n",
      "[12A-04] PAD/UNK: {'PAD_ID_SOURCE': 0, 'UNK_ID_SOURCE': 1}\n",
      "[12A-04] stable_mod probe: 9 9 (should match)\n",
      "[12A-04] ✅ Streaming pair generator (task_key) ready\n",
      "\n",
      "[12A-04] CHECKPOINT D\n",
      "Next run [12A-05] again (it should work now).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-04] Utilities + streaming pair generator WITH task_key derivation (multi-task inside SOURCE)\n",
    "# FIX: no pd.read_parquet(..., nrows=1). Use pyarrow schema to check optional columns.\n",
    "\n",
    "import hashlib\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "MAX_LEN = int(PROTO[\"MAX_PREFIX_LEN\"])\n",
    "CAP_ENABLED = bool(PROTO[\"CAP_ENABLED\"])\n",
    "CAP_SESSION_LEN = int(PROTO[\"CAP_SESSION_LEN\"])\n",
    "CAP_STRATEGY = str(PROTO[\"CAP_STRATEGY\"])\n",
    "assert CAP_STRATEGY == \"take_last\"\n",
    "\n",
    "# Ensure mapping exists\n",
    "if \"item2id\" not in globals():\n",
    "    raise AssertionError(\"[12A-04] item2id not found; run [12A-02] first.\")\n",
    "source_token_to_id = item2id\n",
    "\n",
    "print(f\"[12A-04] Built source_token_to_id size={len(source_token_to_id):,}\")\n",
    "print(\"[12A-04] PAD/UNK:\", {\"PAD_ID_SOURCE\": PAD_ID_SOURCE, \"UNK_ID_SOURCE\": UNK_ID_SOURCE})\n",
    "\n",
    "def stable_hash64(s: str) -> int:\n",
    "    h = hashlib.blake2b(s.encode(\"utf-8\"), digest_size=8).digest()\n",
    "    return int.from_bytes(h, \"little\", signed=False)\n",
    "\n",
    "def stable_mod(value, mod: int) -> int:\n",
    "    return stable_hash64(str(value)) % mod\n",
    "\n",
    "def map_seq_tokens_to_ids(seq) -> tuple[np.ndarray, int]:\n",
    "    out = np.empty(len(seq), dtype=np.int64)\n",
    "    unk = 0\n",
    "    for i, tok in enumerate(seq):\n",
    "        tid = source_token_to_id.get(str(tok), UNK_ID_SOURCE)\n",
    "        out[i] = tid\n",
    "        if tid == UNK_ID_SOURCE:\n",
    "            unk += 1\n",
    "    return out, unk\n",
    "\n",
    "def session_to_one_pair(seq_ids: np.ndarray):\n",
    "    if CAP_ENABLED and len(seq_ids) > CAP_SESSION_LEN and CAP_STRATEGY == \"take_last\":\n",
    "        seq_ids = seq_ids[-CAP_SESSION_LEN:]\n",
    "    if len(seq_ids) < 2:\n",
    "        return None\n",
    "\n",
    "    label = int(seq_ids[-1])\n",
    "    prefix = seq_ids[:-1]\n",
    "    if len(prefix) > MAX_LEN:\n",
    "        prefix = prefix[-MAX_LEN:]\n",
    "\n",
    "    x = np.zeros((MAX_LEN,), dtype=np.int64)\n",
    "    m = np.zeros((MAX_LEN,), dtype=np.int64)\n",
    "    plen = len(prefix)\n",
    "    x[-plen:] = prefix\n",
    "    m[-plen:] = 1\n",
    "    return x, m, label\n",
    "\n",
    "def reservoir_add(buf: list, item, cap: int | None, rng: np.random.Generator, seen_count: int):\n",
    "    if cap is None:\n",
    "        buf.append(item)\n",
    "        return\n",
    "    if len(buf) < cap:\n",
    "        buf.append(item)\n",
    "        return\n",
    "    j = int(rng.integers(0, seen_count + 1))\n",
    "    if j < cap:\n",
    "        buf[j] = item\n",
    "\n",
    "def detect_seq_col(cols: list[str]) -> str:\n",
    "    for c in [\"items\", \"item_seq\", \"sequence\", \"seq\", \"item_ids\"]:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    raise KeyError(f\"Could not detect sequence column. cols={cols}\")\n",
    "\n",
    "# ---- NEW: task_key derivation (multi-task inside one dataset) ----\n",
    "def task_key_from_row(session_length: int, start_ts=None, mode: str = \"len_bin\") -> str:\n",
    "    L = int(session_length)\n",
    "    if mode == \"len_bin\":\n",
    "        if L <= 2:   return \"len_01_02\"\n",
    "        if L <= 5:   return \"len_03_05\"\n",
    "        if L <= 10:  return \"len_06_10\"\n",
    "        if L <= 20:  return \"len_11_20\"\n",
    "        return \"len_21_plus\"\n",
    "\n",
    "    if mode == \"time_month\":\n",
    "        try:\n",
    "            ts = pd.to_datetime(start_ts, unit=\"s\", utc=True) if isinstance(start_ts, (int, np.integer)) else pd.to_datetime(start_ts, utc=True)\n",
    "            return f\"month_{ts.year:04d}_{ts.month:02d}\"\n",
    "        except Exception:\n",
    "            return task_key_from_row(L, None, mode=\"len_bin\")\n",
    "\n",
    "    raise ValueError(f\"Unknown task_key mode: {mode}\")\n",
    "\n",
    "def parquet_has_col(fp, col: str) -> bool:\n",
    "    # Fast schema check (no full read)\n",
    "    schema = pq.read_schema(fp)\n",
    "    return col in schema.names\n",
    "\n",
    "def iter_pairs_from_files(\n",
    "    files,\n",
    "    sample_mod: int,\n",
    "    sample_rem: int,\n",
    "    seed: int,\n",
    "    task_key_mode: str = \"len_bin\",\n",
    "    max_files=None,\n",
    "    log_every_files: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream (task_key, x, m, y) pairs.\n",
    "    - sampling uses stable hash of session_id (works even when session_id isn't int)\n",
    "    - filters out UNK labels\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_files = 0\n",
    "    sessions_seen = 0\n",
    "    yielded = 0\n",
    "    short = 0\n",
    "    unk_labels = 0\n",
    "    unk_tokens_total = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for fp in files:\n",
    "        n_files += 1\n",
    "        if max_files is not None and n_files > max_files:\n",
    "            break\n",
    "\n",
    "        # Decide optional cols from schema\n",
    "        has_start_ts = parquet_has_col(fp, \"start_ts\")\n",
    "\n",
    "        cols = [\"session_id\", \"session_length\", \"items\"]\n",
    "        if has_start_ts:\n",
    "            cols.append(\"start_ts\")\n",
    "\n",
    "        df = pd.read_parquet(fp, columns=cols)\n",
    "\n",
    "        seq_col = detect_seq_col(list(df.columns))\n",
    "\n",
    "        for row in df.itertuples(index=False):\n",
    "            sessions_seen += 1\n",
    "            sid = getattr(row, \"session_id\")\n",
    "\n",
    "            if sample_mod and sample_mod > 1:\n",
    "                if stable_mod(sid, sample_mod) != sample_rem:\n",
    "                    continue\n",
    "\n",
    "            seq = getattr(row, seq_col)\n",
    "            if seq is None:\n",
    "                short += 1\n",
    "                continue\n",
    "\n",
    "            seq_ids, unk_tok = map_seq_tokens_to_ids(seq)\n",
    "            unk_tokens_total += unk_tok\n",
    "\n",
    "            pair = session_to_one_pair(seq_ids)\n",
    "            if pair is None:\n",
    "                short += 1\n",
    "                continue\n",
    "\n",
    "            x, m, y = pair\n",
    "            if y == UNK_ID_SOURCE:\n",
    "                unk_labels += 1\n",
    "                continue\n",
    "\n",
    "            sess_len = getattr(row, \"session_length\")\n",
    "            st = getattr(row, \"start_ts\", None) if has_start_ts else None\n",
    "            task_key = task_key_from_row(sess_len, st, mode=task_key_mode)\n",
    "\n",
    "            yielded += 1\n",
    "            yield task_key, x, m, y\n",
    "\n",
    "        if (n_files % log_every_files) == 0:\n",
    "            print(f\"[12A-04] scanned_files={n_files}/{len(files)} sessions_seen={sessions_seen:,} \"\n",
    "                  f\"yielded={yielded:,} short={short:,} unk_labels={unk_labels:,} unk_tokens_total={unk_tokens_total:,} \"\n",
    "                  f\"elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "    print(f\"[12A-04] DONE files={n_files} sessions_seen={sessions_seen:,} yielded={yielded:,} short={short:,} \"\n",
    "          f\"unk_labels={unk_labels:,} unk_tokens_total={unk_tokens_total:,} elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "print(\"[12A-04] stable_mod probe:\", stable_mod(\"3160332::21\", 10), stable_mod(\"3160332::21\", 10), \"(should match)\")\n",
    "print(\"[12A-04] ✅ Streaming pair generator (task_key) ready\")\n",
    "print(\"\\n[12A-04] CHECKPOINT D\")\n",
    "print(\"Next run [12A-05] again (it should work now).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8cde9",
   "metadata": {},
   "source": [
    " Probe 2 episodes (shape sanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0690734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-05] TASK_CFG: {'n_support': 20, 'n_query': 20, 'sample_mod': 10, 'sample_rem': 0, 'max_files': 50, 'max_pairs_per_task_buffer': 50000, 'seed': 42, 'task_key_mode': 'len_bin', 'require_multi_task': True, 'min_tasks_required': 3}\n",
      "[12A-05] sample 0: task=len_21_plus x_nonzero=20 label=197\n",
      " x[:10]= [197, 197, 197, 197, 197, 197, 197, 197, 197, 197]\n",
      " m[:10]= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[12A-05] sample 1: task=len_21_plus x_nonzero=20 label=344\n",
      " x[:10]= [344, 344, 344, 344, 344, 344, 344, 344, 344, 344]\n",
      " m[:10]= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[12A-05] sample 2: task=len_06_10 x_nonzero=9 label=179\n",
      " x[:10]= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " m[:10]= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[12A-04] DONE files=51 sessions_seen=325,575 yielded=32,540 short=0 unk_labels=0 unk_tokens_total=0 elapsed=3.3s\n",
      "[12A-05] task_unique=5 | pairs_scanned=32,540\n",
      "[12A-05] top10 tasks: [('len_21_plus', 7920), ('len_03_05', 7541), ('len_06_10', 6939), ('len_11_20', 6686), ('len_01_02', 3454)]\n",
      "\n",
      "[12A-05] CHECKPOINT E\n",
      "Paste: 3 probe samples + task_unique + top10 tasks.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-05] Probe generator output + task diversity diagnostics (replaces domain)\n",
    "\n",
    "TASK_CFG = {\n",
    "    \"n_support\": 20,\n",
    "    \"n_query\": 20,\n",
    "    \"sample_mod\": 10,\n",
    "    \"sample_rem\": 0,\n",
    "    \"max_files\": 50,  # diagnostic scan\n",
    "    \"max_pairs_per_task_buffer\": 50_000,\n",
    "    \"seed\": 42,\n",
    "    \"task_key_mode\": \"len_bin\",   # <<< multi-task inside SOURCE\n",
    "    \"require_multi_task\": True,\n",
    "    \"min_tasks_required\": 3,\n",
    "}\n",
    "\n",
    "print(\"[12A-05] TASK_CFG:\", TASK_CFG)\n",
    "assert TASK_CFG[\"n_support\"] + TASK_CFG[\"n_query\"] <= TASK_CFG[\"max_pairs_per_task_buffer\"], \\\n",
    "    \"Invalid: n_support+n_query exceeds max_pairs_per_task_buffer.\"\n",
    "\n",
    "# Probe 3 samples\n",
    "gen = iter_pairs_from_files(\n",
    "    train_files,\n",
    "    sample_mod=TASK_CFG[\"sample_mod\"],\n",
    "    sample_rem=TASK_CFG[\"sample_rem\"],\n",
    "    seed=TASK_CFG[\"seed\"],\n",
    "    task_key_mode=TASK_CFG[\"task_key_mode\"],\n",
    "    max_files=2\n",
    ")\n",
    "for j in range(3):\n",
    "    task_key, x, m, y = next(gen)\n",
    "    print(f\"[12A-05] sample {j}: task={task_key} x_nonzero={int(m.sum())} label={y}\")\n",
    "    print(\" x[:10]=\", x[:10].tolist())\n",
    "    print(\" m[:10]=\", m[:10].tolist())\n",
    "\n",
    "# Task diversity scan\n",
    "task_counts = {}\n",
    "pairs_scanned = 0\n",
    "for task_key, *_ in iter_pairs_from_files(\n",
    "    train_files,\n",
    "    sample_mod=TASK_CFG[\"sample_mod\"],\n",
    "    sample_rem=TASK_CFG[\"sample_rem\"],\n",
    "    seed=TASK_CFG[\"seed\"],\n",
    "    task_key_mode=TASK_CFG[\"task_key_mode\"],\n",
    "    max_files=TASK_CFG[\"max_files\"]\n",
    "):\n",
    "    task_counts[task_key] = task_counts.get(task_key, 0) + 1\n",
    "    pairs_scanned += 1\n",
    "\n",
    "tasks_sorted = sorted(task_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"[12A-05] task_unique={len(task_counts)} | pairs_scanned={pairs_scanned:,}\")\n",
    "print(\"[12A-05] top10 tasks:\", tasks_sorted[:10])\n",
    "\n",
    "if TASK_CFG[\"require_multi_task\"]:\n",
    "    assert len(task_counts) >= TASK_CFG[\"min_tasks_required\"], (\n",
    "        f\"[12A-05] Too few tasks detected: {len(task_counts)}. \"\n",
    "        \"Meta-learning would be weak. Switch task_key_mode or adjust bins.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n[12A-05] CHECKPOINT E\")\n",
    "print(\"Paste: 3 probe samples + task_unique + top10 tasks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85273c2b",
   "metadata": {},
   "source": [
    "Save task-builder config + stats snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3f347ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-04] DONE files=51 sessions_seen=325,575 yielded=32,540 short=0 unk_labels=0 unk_tokens_total=0 elapsed=2.5s\n",
      "[12A-06] DONE build buffers | pairs_seen=32,540 kept=32,540 tasks=5 elapsed=2.5s\n",
      "[12A-06] tasks_ok: 5 | tasks_bad: 0 | need per task: 40\n",
      "\n",
      "[12A-06] CHECKPOINT F\n",
      "Paste: DONE build buffers line + tasks_ok/tasks_bad + need.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-06] Build per-task buffers (reservoir sampled) for episode sampling\n",
    "\n",
    "rng = np.random.default_rng(TASK_CFG[\"seed\"])\n",
    "\n",
    "buffers = {}        # task_key -> list of (x,m,y)\n",
    "seen_per_task = {}  # task_key -> count for reservoir\n",
    "cap = int(TASK_CFG[\"max_pairs_per_task_buffer\"])\n",
    "\n",
    "pairs_total_seen = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for task_key, x, m, y in iter_pairs_from_files(\n",
    "    train_files,\n",
    "    sample_mod=TASK_CFG[\"sample_mod\"],\n",
    "    sample_rem=TASK_CFG[\"sample_rem\"],\n",
    "    seed=TASK_CFG[\"seed\"],\n",
    "    task_key_mode=TASK_CFG[\"task_key_mode\"],\n",
    "    max_files=TASK_CFG[\"max_files\"]\n",
    "):\n",
    "    pairs_total_seen += 1\n",
    "    if task_key not in buffers:\n",
    "        buffers[task_key] = []\n",
    "        seen_per_task[task_key] = 0\n",
    "    seen_per_task[task_key] += 1\n",
    "\n",
    "    reservoir_add(buffers[task_key], (x, m, y), cap=cap, rng=rng, seen_count=seen_per_task[task_key])\n",
    "\n",
    "    if pairs_total_seen % 50_000 == 0:\n",
    "        kept = sum(len(v) for v in buffers.values())\n",
    "        print(f\"[12A-06] pairs_seen={pairs_total_seen:,} kept={kept:,} tasks={len(buffers)} elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "kept = sum(len(v) for v in buffers.values())\n",
    "print(f\"[12A-06] DONE build buffers | pairs_seen={pairs_total_seen:,} kept={kept:,} tasks={len(buffers)} elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "need = TASK_CFG[\"n_support\"] + TASK_CFG[\"n_query\"]\n",
    "ok = {k: len(v) for k, v in buffers.items() if len(v) >= need}\n",
    "bad = {k: len(v) for k, v in buffers.items() if len(v) < need}\n",
    "\n",
    "print(\"[12A-06] tasks_ok:\", len(ok), \"| tasks_bad:\", len(bad), \"| need per task:\", need)\n",
    "if len(bad) > 0:\n",
    "    print(\"[12A-06] BAD tasks:\", sorted(bad.items(), key=lambda x: x[1])[:10])\n",
    "\n",
    "assert len(ok) >= TASK_CFG[\"min_tasks_required\"], \"[12A-06] Not enough tasks with sufficient pairs for episodes.\"\n",
    "\n",
    "print(\"\\n[12A-06] CHECKPOINT F\")\n",
    "print(\"Paste: DONE build buffers line + tasks_ok/tasks_bad + need.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b347e86",
   "metadata": {},
   "source": [
    "Episode sampler with max_episodes control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a5040a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-07] ✅ Episode sampler ready\n",
      "\n",
      "[12A-07] CHECKPOINT G\n",
      "Next run [12A-08] to print a few episodes + task keys.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-07] Episode sampler with max_episodes control\n",
    "\n",
    "def sample_episode(buf: list, n_support: int, n_query: int, rng: np.random.Generator):\n",
    "    idx = rng.choice(len(buf), size=(n_support + n_query), replace=False)\n",
    "    sup = [buf[i] for i in idx[:n_support]]\n",
    "    qry = [buf[i] for i in idx[n_support:]]\n",
    "\n",
    "    def stack(pairs):\n",
    "        xs = np.stack([p[0] for p in pairs], axis=0)\n",
    "        ms = np.stack([p[1] for p in pairs], axis=0)\n",
    "        ys = np.asarray([p[2] for p in pairs], dtype=np.int64)\n",
    "        return xs, ms, ys\n",
    "\n",
    "    return stack(sup), stack(qry)\n",
    "\n",
    "def iter_episodes(buffers: dict, n_support: int, n_query: int, seed: int, max_episodes: int = 100):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    keys = sorted([k for k, v in buffers.items() if len(v) >= (n_support + n_query)])\n",
    "    for ep in range(max_episodes):\n",
    "        task_key = keys[int(rng.integers(0, len(keys)))]\n",
    "        (sx, sm, sy), (qx, qm, qy) = sample_episode(buffers[task_key], n_support, n_query, rng)\n",
    "        yield {\n",
    "            \"episode_id\": ep,\n",
    "            \"task_key\": task_key,\n",
    "            \"support\": {\"x\": sx, \"m\": sm, \"y\": sy},\n",
    "            \"query\": {\"x\": qx, \"m\": qm, \"y\": qy},\n",
    "        }\n",
    "\n",
    "print(\"[12A-07] ✅ Episode sampler ready\")\n",
    "print(\"\\n[12A-07] CHECKPOINT G\")\n",
    "print(\"Next run [12A-08] to print a few episodes + task keys.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ad582",
   "metadata": {},
   "source": [
    "Probe episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5951ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-08] episode 0 task=len_01_02\n",
      "  support x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 1.0\n",
      "  query   x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 1.0\n",
      "[12A-08] episode 1 task=len_21_plus\n",
      "  support x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 20.0\n",
      "  query   x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 20.0\n",
      "[12A-08] episode 2 task=len_21_plus\n",
      "  support x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 20.0\n",
      "  query   x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 20.0\n",
      "[12A-08] episode 3 task=len_01_02\n",
      "  support x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 1.0\n",
      "  query   x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 1.0\n",
      "[12A-08] episode 4 task=len_03_05\n",
      "  support x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 3.05\n",
      "  query   x (20, 20) m (20, 20) y (20,) | x_nonzero_mean 2.85\n",
      "[12A-08] tasks_seen_in_probe: ['len_01_02', 'len_21_plus', 'len_21_plus', 'len_01_02', 'len_03_05']\n",
      "\n",
      "[12A-08] CHECKPOINT H\n",
      "Paste: episodes output + tasks_seen_in_probe.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-08] Probe episodes\n",
    "\n",
    "MAX_EPISODES_PROBE = 5\n",
    "eps = iter_episodes(\n",
    "    buffers=buffers,\n",
    "    n_support=TASK_CFG[\"n_support\"],\n",
    "    n_query=TASK_CFG[\"n_query\"],\n",
    "    seed=TASK_CFG[\"seed\"],\n",
    "    max_episodes=MAX_EPISODES_PROBE\n",
    ")\n",
    "\n",
    "seen = []\n",
    "for ep in eps:\n",
    "    tk = ep[\"task_key\"]\n",
    "    seen.append(tk)\n",
    "    sm = ep[\"support\"][\"m\"]\n",
    "    qm = ep[\"query\"][\"m\"]\n",
    "    print(f\"[12A-08] episode {ep['episode_id']} task={tk}\")\n",
    "    print(\"  support x\", ep[\"support\"][\"x\"].shape, \"m\", sm.shape, \"y\", ep[\"support\"][\"y\"].shape,\n",
    "          \"| x_nonzero_mean\", float(sm.sum(axis=1).mean()))\n",
    "    print(\"  query   x\", ep[\"query\"][\"x\"].shape, \"m\", qm.shape, \"y\", ep[\"query\"][\"y\"].shape,\n",
    "          \"| x_nonzero_mean\", float(qm.sum(axis=1).mean()))\n",
    "\n",
    "print(\"[12A-08] tasks_seen_in_probe:\", seen)\n",
    "\n",
    "assert len(set(seen)) >= 2, \"[12A-08] probe did not show multiple tasks; increase MAX_EPISODES_PROBE or check bins.\"\n",
    "print(\"\\n[12A-08] CHECKPOINT H\")\n",
    "print(\"Paste: episodes output + tasks_seen_in_probe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d1a15",
   "metadata": {},
   "source": [
    "Write report artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e6b8890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-09] ✅ Wrote: C:\\mooc-coldstart-session-meta\\reports\\12A_task_builder_for_meta\\20260104_141727\\meta_task_config.json\n",
      "[12A-09] ✅ Wrote: C:\\mooc-coldstart-session-meta\\reports\\12A_task_builder_for_meta\\20260104_141727\\task_coverage.json\n",
      "\n",
      "[12A-09] CHECKPOINT I\n",
      "Paste: both written paths.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-09] Write report artifacts\n",
    "\n",
    "REPORT_DIR = REPO_ROOT / \"reports/12A_task_builder_for_meta\" / RUN_TAG\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "meta_task_config = {\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"protocol\": PROTO,\n",
    "    \"session_gaps\": {\n",
    "        \"target_gap_minutes\": gap_target_m,\n",
    "        \"source_gap_minutes\": gap_source_m,\n",
    "        \"target_gap_label\": GAPS[\"target\"].get(\"primary_threshold_label\"),\n",
    "        \"source_gap_label\": GAPS[\"source\"].get(\"primary_threshold_label\"),\n",
    "    },\n",
    "    \"task_cfg\": TASK_CFG,\n",
    "    \"task_definition\": {\n",
    "        \"note\": \"SOURCE parquet has domain_unique=1 (always 'source'). Tasks are derived from real fields.\",\n",
    "        \"task_key_mode\": TASK_CFG[\"task_key_mode\"],\n",
    "        \"len_bin_definition\": [\"len_01_02\",\"len_03_05\",\"len_06_10\",\"len_11_20\",\"len_21_plus\"],\n",
    "    },\n",
    "    \"source\": {\n",
    "        \"run_tag_source\": source_vocab.get(\"run_tag_source\"),\n",
    "        \"vocab_size\": VOCAB_SIZE_SOURCE,\n",
    "        \"pad_id\": PAD_ID_SOURCE,\n",
    "        \"unk_id\": UNK_ID_SOURCE,\n",
    "        \"shards\": {\"train\": len(train_files), \"val\": len(val_files), \"test\": len(test_files)},\n",
    "        \"paths\": {\n",
    "            \"train_dir\": str(SRC_TRAIN_DIR),\n",
    "            \"val_dir\": str(SRC_VAL_DIR),\n",
    "            \"test_dir\": str(SRC_TEST_DIR),\n",
    "            \"vocab\": str(SRC_VOCAB_PATH),\n",
    "        },\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"UNK labels filtered out at pair construction.\",\n",
    "        \"Reservoir sampling used for buffer cap to avoid recency bias.\",\n",
    "        \"Episodes are sampled across task_key groups (multi-task within SOURCE).\",\n",
    "        \"Long-session cap uses take_last (ablate later in Notebook 13).\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "task_coverage = {\n",
    "    \"pairs_total_seen\": int(pairs_total_seen),\n",
    "    \"pairs_total_kept\": int(sum(len(v) for v in buffers.values())),\n",
    "    \"tasks_total\": int(len(buffers)),\n",
    "    \"need_per_task\": int(TASK_CFG[\"n_support\"] + TASK_CFG[\"n_query\"]),\n",
    "    \"pairs_kept_per_task\": {k: len(v) for k, v in buffers.items()},\n",
    "    \"pairs_seen_per_task\": {k: int(seen_per_task.get(k, 0)) for k in buffers.keys()},\n",
    "}\n",
    "\n",
    "META_CFG_PATH = REPORT_DIR / \"meta_task_config.json\"\n",
    "COV_PATH = REPORT_DIR / \"task_coverage.json\"\n",
    "\n",
    "META_CFG_PATH.write_text(json.dumps(meta_task_config, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "COV_PATH.write_text(json.dumps(task_coverage, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"[12A-09] ✅ Wrote:\", str(META_CFG_PATH))\n",
    "print(\"[12A-09] ✅ Wrote:\", str(COV_PATH))\n",
    "\n",
    "print(\"\\n[12A-09] CHECKPOINT I\")\n",
    "print(\"Paste: both written paths.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de924fe3",
   "metadata": {},
   "source": [
    "Update repo meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c8c6822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12A-10] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[12A-10] CHECKPOINT J\n",
      "Confirm meta.json updated.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12A-10] Update repo meta.json\n",
    "\n",
    "META_JSON = REPO_ROOT / \"meta.json\"\n",
    "meta = json.loads(META_JSON.read_text(encoding=\"utf-8\")) if META_JSON.exists() else {}\n",
    "meta.setdefault(\"runs\", [])\n",
    "\n",
    "meta[\"runs\"].append({\n",
    "    \"kind\": \"12A_task_builder_for_meta\",\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"report_dir\": str(REPORT_DIR),\n",
    "    \"artifacts\": {\n",
    "        \"meta_task_config\": str(META_CFG_PATH),\n",
    "        \"task_coverage\": str(COV_PATH),\n",
    "    }\n",
    "})\n",
    "\n",
    "META_JSON.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(\"[12A-10] ✅ Updated meta.json:\", str(META_JSON))\n",
    "\n",
    "print(\"\\n[12A-10] CHECKPOINT J\")\n",
    "print(\"Confirm meta.json updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
