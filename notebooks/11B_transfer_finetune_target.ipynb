{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c99e08",
   "metadata": {},
   "source": [
    "Imports + versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fb9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-00] torch: 2.9.1+cpu\n",
      "[11B-00] pandas: 2.3.3\n",
      "[11B-00] numpy: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-00] Imports + versions\n",
    "import os, json, time, math, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(\"[11B-00] torch:\", torch.__version__)\n",
    "print(\"[11B-00] pandas:\", pd.__version__)\n",
    "print(\"[11B-00] numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409eafe6",
   "metadata": {},
   "source": [
    "REPO_ROOT + load configs (single source of truth = repo artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1011e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[11B-01] RUN_TAG: 20260104_114435\n",
      "[11B-01] Expect config: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[11B-01] Expect sanity: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\sanity_metrics_20251229_163357_20251229_232834.json\n",
      "[11B-01] Expect gaps: C:\\mooc-coldstart-session-meta\\data\\processed\\normalized_events\\session_gap_thresholds.json\n",
      "[11B-01] Loaded dataloader_config keys: ['target', 'source', 'protocol']\n",
      "[11B-01] Loaded sanity_metrics keys: ['run_tag_target', 'run_tag_source', 'created_at', 'target', 'source', 'notes']\n",
      "[11B-01] Loaded session_gap_thresholds keys: ['generated_from_run_tag', 'generated_at', 'target', 'source', 'decision_notes']\n",
      "[11B-01] target: gap_minutes from primary_threshold_seconds=1800 -> 30m | label=30m\n",
      "[11B-01] source: gap_minutes from primary_threshold_seconds=600 -> 10m | label=10m\n",
      "[11B-01] ✅ Session gaps confirmed: target=30m, source=10m\n",
      "[11B-01] ✅ PROTO: {'K_LIST': [5, 10, 20], 'MAX_PREFIX_LEN': 20, 'CAP_ENABLED': True, 'CAP_SESSION_LEN': 200, 'CAP_STRATEGY': 'take_last'}\n",
      "\n",
      "[11B-01] CHECKPOINT A\n",
      "Paste: inferred gaps + PROTO dict\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-01] REPO_ROOT + load configs (single source of truth = repo artifacts)\n",
    "REPO_ROOT = Path(r\"C:\\mooc-coldstart-session-meta\").resolve()\n",
    "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "cfg_path   = REPO_ROOT / \"data\" / \"processed\" / \"supervised\" / \"dataloader_config_20251229_163357_20251229_232834.json\"\n",
    "san_path   = REPO_ROOT / \"data\" / \"processed\" / \"supervised\" / \"sanity_metrics_20251229_163357_20251229_232834.json\"\n",
    "gaps_path  = REPO_ROOT / \"data\" / \"processed\" / \"normalized_events\" / \"session_gap_thresholds.json\"\n",
    "\n",
    "print(\"[11B-01] REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"[11B-01] RUN_TAG:\", RUN_TAG)\n",
    "print(\"[11B-01] Expect config:\", cfg_path)\n",
    "print(\"[11B-01] Expect sanity:\", san_path)\n",
    "print(\"[11B-01] Expect gaps:\", gaps_path)\n",
    "\n",
    "assert cfg_path.exists(), f\"Missing: {cfg_path}\"\n",
    "assert san_path.exists(), f\"Missing: {san_path}\"\n",
    "assert gaps_path.exists(), f\"Missing: {gaps_path}\"\n",
    "\n",
    "DL_CFG = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "SANITY = json.loads(san_path.read_text(encoding=\"utf-8\"))\n",
    "GAPS   = json.loads(gaps_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"[11B-01] Loaded dataloader_config keys:\", list(DL_CFG.keys()))\n",
    "print(\"[11B-01] Loaded sanity_metrics keys:\", list(SANITY.keys()))\n",
    "print(\"[11B-01] Loaded session_gap_thresholds keys:\", list(GAPS.keys()))\n",
    "\n",
    "def infer_gap_minutes(d: dict, name: str) -> int:\n",
    "    if \"gap_minutes\" in d:\n",
    "        return int(d[\"gap_minutes\"])\n",
    "    if \"primary_threshold_seconds\" in d:\n",
    "        m = int(round(int(d[\"primary_threshold_seconds\"]) / 60))\n",
    "        lbl = d.get(\"primary_threshold_label\", None)\n",
    "        print(f\"[11B-01] {name}: gap_minutes from primary_threshold_seconds={d['primary_threshold_seconds']} -> {m}m | label={lbl}\")\n",
    "        return m\n",
    "    raise KeyError(f\"[11B-01] {name}: cannot infer gap minutes. keys={list(d.keys())}\")\n",
    "\n",
    "gap_target_m = infer_gap_minutes(GAPS[\"target\"], \"target\")\n",
    "gap_source_m = infer_gap_minutes(GAPS[\"source\"], \"source\")\n",
    "assert gap_target_m == 30, f\"target gap mismatch: got {gap_target_m}m\"\n",
    "assert gap_source_m == 10, f\"source gap mismatch: got {gap_source_m}m\"\n",
    "print(\"[11B-01] ✅ Session gaps confirmed: target=30m, source=10m\")\n",
    "\n",
    "PROTO_RAW = DL_CFG[\"protocol\"]\n",
    "\n",
    "# Normalize protocol to match Notebook 06 constants\n",
    "PROTO = {\n",
    "    \"K_LIST\": [5, 10, 20],\n",
    "    \"MAX_PREFIX_LEN\": int(PROTO_RAW[\"max_prefix_len\"]),\n",
    "    \"CAP_ENABLED\": bool(PROTO_RAW[\"source_long_session_policy\"][\"enabled\"]),\n",
    "    \"CAP_SESSION_LEN\": int(PROTO_RAW[\"source_long_session_policy\"][\"cap_session_len\"]),\n",
    "    \"CAP_STRATEGY\": str(PROTO_RAW[\"source_long_session_policy\"][\"cap_strategy\"]),\n",
    "}\n",
    "assert PROTO[\"MAX_PREFIX_LEN\"] == 20, \"Protocol drift: MAX_PREFIX_LEN != 20\"\n",
    "assert PROTO[\"CAP_ENABLED\"] is True, \"Protocol drift: CAP_ENABLED != True\"\n",
    "assert PROTO[\"CAP_SESSION_LEN\"] == 200, \"Protocol drift: CAP_SESSION_LEN != 200\"\n",
    "assert PROTO[\"CAP_STRATEGY\"] == \"take_last\", \"Protocol drift: CAP_STRATEGY != take_last\"\n",
    "\n",
    "print(\"[11B-01] ✅ PROTO:\", PROTO)\n",
    "\n",
    "print(\"\\n[11B-01] CHECKPOINT A\")\n",
    "print(\"Paste: inferred gaps + PROTO dict\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702a69f",
   "metadata": {},
   "source": [
    "Paths: TARGET tensors + vocab, SOURCE pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64dd3a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-02] Expect: C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[11B-02] Expect: C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[11B-02] Expect: C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[11B-02] Expect: C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_vocab_items_20251229_163357.json\n",
      "[11B-02] Expect: C:\\mooc-coldstart-session-meta\\reports\\11A_transfer_pretrain_source\\20260103_220933\\model_pretrained_source.pt\n",
      "[11B-02] ✅ All required artifacts exist\n",
      "\n",
      "[11B-02] CHECKPOINT B\n",
      "Confirm the 5 artifact paths exist (pt/json + pretrained checkpoint).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-02] Paths: TARGET tensors + vocab, SOURCE pretrained checkpoint\n",
    "target_run_tag = \"20251229_163357\"\n",
    "src_pretrain_run_tag = \"20260103_220933\"\n",
    "\n",
    "target_train_pt = REPO_ROOT / \"data\" / \"processed\" / \"tensor_target\" / f\"target_tensor_train_{target_run_tag}.pt\"\n",
    "target_val_pt   = REPO_ROOT / \"data\" / \"processed\" / \"tensor_target\" / f\"target_tensor_val_{target_run_tag}.pt\"\n",
    "target_test_pt  = REPO_ROOT / \"data\" / \"processed\" / \"tensor_target\" / f\"target_tensor_test_{target_run_tag}.pt\"\n",
    "target_vocab_js = REPO_ROOT / \"data\" / \"processed\" / \"tensor_target\" / f\"target_vocab_items_{target_run_tag}.json\"\n",
    "\n",
    "src_ckpt_pt = REPO_ROOT / \"reports\" / \"11A_transfer_pretrain_source\" / src_pretrain_run_tag / \"model_pretrained_source.pt\"\n",
    "\n",
    "for p in [target_train_pt, target_val_pt, target_test_pt, target_vocab_js, src_ckpt_pt]:\n",
    "    print(\"[11B-02] Expect:\", p)\n",
    "    assert p.exists(), f\"Missing artifact: {p}\"\n",
    "\n",
    "print(\"[11B-02] ✅ All required artifacts exist\")\n",
    "\n",
    "print(\"\\n[11B-02] CHECKPOINT B\")\n",
    "print(\"Confirm the 5 artifact paths exist (pt/json + pretrained checkpoint).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c36d5",
   "metadata": {},
   "source": [
    "Load TARGET tensors + vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401b47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-03] TARGET: vocab_size from max(vocab['vocab'])+1 = 747 (token->id)\n",
      "[11B-03] VOCAB_SIZE_TARGET: 747\n",
      "[11B-03] PAD_ID_TARGET: 0 | UNK_ID_TARGET: 1\n",
      "[11B-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_train_20251229_163357.pt\n",
      "[11B-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_val_20251229_163357.pt\n",
      "[11B-03] torch.load OK (weights_only=False): C:\\mooc-coldstart-session-meta\\data\\processed\\tensor_target\\target_tensor_test_20251229_163357.pt\n",
      "[11B-03] TARGET train shapes: (1944, 20) (1944, 20) (1944,)\n",
      "[11B-03] TARGET val shapes  : (189, 20) (189,)\n",
      "[11B-03] TARGET test shapes : (200, 20) (200,)\n",
      "\n",
      "[11B-03] CHECKPOINT C\n",
      "Paste: VOCAB_SIZE_TARGET + PAD/UNK + the three shape lines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-03] Load TARGET tensors + vocab\n",
    "target_vocab = json.loads(target_vocab_js.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def infer_vocab_size(vocab: dict, name: str) -> int:\n",
    "    if \"vocab_size\" in vocab:\n",
    "        return int(vocab[\"vocab_size\"])\n",
    "    if \"vocab\" in vocab and isinstance(vocab[\"vocab\"], dict):\n",
    "        # token->id map\n",
    "        vs = int(max(vocab[\"vocab\"].values())) + 1\n",
    "        print(f\"[11B-03] {name}: vocab_size from max(vocab['vocab'])+1 = {vs} (token->id)\")\n",
    "        return vs\n",
    "    raise KeyError(f\"[11B-03] {name}: cannot infer vocab_size. keys={list(vocab.keys())}\")\n",
    "\n",
    "VOCAB_SIZE_TARGET = infer_vocab_size(target_vocab, \"TARGET\")\n",
    "PAD_ID_TARGET = int(target_vocab.get(\"pad_id\", 0))\n",
    "UNK_ID_TARGET = int(target_vocab.get(\"unk_id\", 1))\n",
    "\n",
    "print(\"[11B-03] VOCAB_SIZE_TARGET:\", VOCAB_SIZE_TARGET)\n",
    "print(\"[11B-03] PAD_ID_TARGET:\", PAD_ID_TARGET, \"| UNK_ID_TARGET:\", UNK_ID_TARGET)\n",
    "\n",
    "def load_pt(path: Path):\n",
    "    obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    print(f\"[11B-03] torch.load OK (weights_only=False): {path}\")\n",
    "    return obj\n",
    "\n",
    "tr = load_pt(target_train_pt)\n",
    "va = load_pt(target_val_pt)\n",
    "te = load_pt(target_test_pt)\n",
    "\n",
    "# Required keys from 05B tensor target\n",
    "need_keys = [\"input_ids\", \"attn_mask\", \"labels\"]\n",
    "for k in need_keys:\n",
    "    assert k in tr and k in va and k in te, f\"Missing key '{k}' in tensors.\"\n",
    "\n",
    "print(\"[11B-03] TARGET train shapes:\", tuple(tr[\"input_ids\"].shape), tuple(tr[\"attn_mask\"].shape), tuple(tr[\"labels\"].shape))\n",
    "print(\"[11B-03] TARGET val shapes  :\", tuple(va[\"input_ids\"].shape), tuple(va[\"labels\"].shape))\n",
    "print(\"[11B-03] TARGET test shapes :\", tuple(te[\"input_ids\"].shape), tuple(te[\"labels\"].shape))\n",
    "\n",
    "assert tr[\"input_ids\"].shape[1] == PROTO[\"MAX_PREFIX_LEN\"] == 20, \"seq_len mismatch vs protocol\"\n",
    "\n",
    "print(\"\\n[11B-03] CHECKPOINT C\")\n",
    "print(\"Paste: VOCAB_SIZE_TARGET + PAD/UNK + the three shape lines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d375f1",
   "metadata": {},
   "source": [
    "Metrics (reuse Notebook 06 protocol exactly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a865c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-04] ✅ Metric functions ready\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-04] Metrics (reuse Notebook 06 protocol exactly)\n",
    "K_LIST = PROTO[\"K_LIST\"]\n",
    "\n",
    "def metrics_from_ranks(ranks: np.ndarray, K_LIST):\n",
    "    # ranks: 1..inf ; inf means miss\n",
    "    out = {}\n",
    "    n = len(ranks)\n",
    "    for k in K_LIST:\n",
    "        hit = (ranks <= k).astype(np.float64)\n",
    "        out[f\"HR@{k}\"] = float(hit.mean()) if n else 0.0\n",
    "        rr = np.where(ranks <= k, 1.0 / ranks, 0.0)\n",
    "        out[f\"MRR@{k}\"] = float(rr.mean()) if n else 0.0\n",
    "        nd = np.where(ranks <= k, 1.0 / np.log2(ranks + 1.0), 0.0)\n",
    "        out[f\"NDCG@{k}\"] = float(nd.mean()) if n else 0.0\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_target_model(model: nn.Module, input_ids: torch.Tensor, attn_mask: torch.Tensor, labels: torch.Tensor,\n",
    "                      pad_id: int, K_LIST):\n",
    "    model.eval()\n",
    "    bs = 512\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, input_ids.size(0), bs):\n",
    "        x = input_ids[i:i+bs]\n",
    "        m = attn_mask[i:i+bs]\n",
    "        y = labels[i:i+bs]\n",
    "\n",
    "        lengths = m.sum(dim=1).clamp(min=1).long()\n",
    "        logits = model(x, lengths)  # [B, V]\n",
    "        logits[:, pad_id] = -1e9    # PAD excluded from ranking\n",
    "\n",
    "        topk = max(K_LIST)\n",
    "        recs = torch.topk(logits, k=topk, dim=1).indices  # [B, topk]\n",
    "\n",
    "        y_np = y.cpu().numpy()\n",
    "        recs_np = recs.cpu().numpy()\n",
    "\n",
    "        for b in range(recs_np.shape[0]):\n",
    "            yt = int(y_np[b])\n",
    "            if yt == pad_id:\n",
    "                continue\n",
    "            pos = np.where(recs_np[b] == yt)[0]\n",
    "            if pos.size == 0:\n",
    "                ranks.append(np.inf)\n",
    "            else:\n",
    "                ranks.append(float(pos[0] + 1))\n",
    "\n",
    "    ranks = np.asarray(ranks, dtype=np.float64)\n",
    "    out = metrics_from_ranks(ranks, K_LIST)\n",
    "    out[\"_n_examples\"] = int(np.isfinite(ranks).shape[0])\n",
    "    return out\n",
    "\n",
    "print(\"[11B-04] ✅ Metric functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d4e4f",
   "metadata": {},
   "source": [
    "Model: GRU4RecDropout (same shape as 11A pretrain; transfer GRU weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbe7159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-05] ✅ GRU4RecDropout defined\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-05] Model: GRU4RecDropout (same shape as 11A pretrain; transfer GRU weights)\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class GRU4RecDropout(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, dropout: float, pad_id: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        # input_ids: [B, T]\n",
    "        emb = self.drop(self.emb(input_ids))  # [B, T, E]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out_packed, h = self.gru(packed)      # h: [1, B, H]\n",
    "        last = h.squeeze(0)                   # [B, H]\n",
    "        logits = self.fc(last)                # [B, V]\n",
    "        return logits\n",
    "\n",
    "print(\"[11B-05] ✅ GRU4RecDropout defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38e8a1",
   "metadata": {},
   "source": [
    "Load SOURCE pretrained checkpoint (robust extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e603eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-06] Loaded pretrained checkpoint: C:\\mooc-coldstart-session-meta\\reports\\11A_transfer_pretrain_source\\20260103_220933\\model_pretrained_source.pt\n",
      "[11B-06] state_dict source key: state_dict\n",
      "[11B-06] #keys in src_sd: 7\n",
      "[11B-06] sample keys: ['emb.weight', 'gru.weight_ih_l0', 'gru.weight_hh_l0', 'gru.bias_ih_l0', 'gru.bias_hh_l0', 'out.weight', 'out.bias']\n",
      "\n",
      "[11B-06] CHECKPOINT D\n",
      "Paste: src_sd_key + #keys + sample keys (first ~8).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-06] Load SOURCE pretrained checkpoint (robust extraction)\n",
    "def extract_state_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        for k in [\"state_dict\", \"model_state_dict\", \"model\"]:\n",
    "            if k in obj and isinstance(obj[k], dict):\n",
    "                return obj[k], k\n",
    "        # sometimes it's already a state dict\n",
    "        if all(isinstance(v, torch.Tensor) for v in obj.values()):\n",
    "            return obj, \"root\"\n",
    "    raise TypeError(f\"Unrecognized checkpoint format: type={type(obj)} keys={list(obj.keys()) if isinstance(obj, dict) else None}\")\n",
    "\n",
    "ckpt_obj = torch.load(src_ckpt_pt, map_location=\"cpu\", weights_only=False)\n",
    "src_sd, src_sd_key = extract_state_dict(ckpt_obj)\n",
    "\n",
    "print(\"[11B-06] Loaded pretrained checkpoint:\", src_ckpt_pt)\n",
    "print(\"[11B-06] state_dict source key:\", src_sd_key)\n",
    "print(\"[11B-06] #keys in src_sd:\", len(src_sd))\n",
    "print(\"[11B-06] sample keys:\", list(src_sd.keys())[:8])\n",
    "\n",
    "print(\"\\n[11B-06] CHECKPOINT D\")\n",
    "print(\"Paste: src_sd_key + #keys + sample keys (first ~8).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d67a54",
   "metadata": {},
   "source": [
    "Init TARGET model + transfer GRU weights (+ optional PAD/UNK embedding rows only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd8648b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-07] Transfer summary:\n",
      "  copied_keys: 4\n",
      "  skipped_missing: 2\n",
      "  skipped_shape: 1\n",
      "  pad/unk emb rows copied: True\n",
      "  skipped_shape sample: [('emb.weight', (1620, 64), (747, 64))]\n",
      "\n",
      "[11B-07] CHECKPOINT E\n",
      "Paste: copied_keys/skipped counts + whether pad/unk emb rows copied.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-07] Init TARGET model + transfer GRU weights (+ optional PAD/UNK embedding rows only)\n",
    "FINETUNE_SEED = 42\n",
    "set_seed(FINETUNE_SEED)\n",
    "\n",
    "FT_CFG = {\n",
    "    \"emb_dim\": 64,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.3,\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"max_epochs\": 50,\n",
    "    \"early_stop_metric\": \"HR@20\",\n",
    "    \"patience\": 7,\n",
    "    \"min_delta\": 1e-4,\n",
    "    \"seed\": FINETUNE_SEED,\n",
    "    \"transfer_mode\": \"copy_gru_only_plus_pad_unk_emb_rows\",\n",
    "    \"src_pretrain_run_tag\": src_pretrain_run_tag,\n",
    "    \"src_checkpoint\": str(src_ckpt_pt),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = GRU4RecDropout(\n",
    "    vocab_size=VOCAB_SIZE_TARGET,\n",
    "    emb_dim=FT_CFG[\"emb_dim\"],\n",
    "    hidden_dim=FT_CFG[\"hidden_dim\"],\n",
    "    dropout=FT_CFG[\"dropout\"],\n",
    "    pad_id=PAD_ID_TARGET,\n",
    ").to(device)\n",
    "\n",
    "# Build a source-shaped model just to validate key compatibility (optional)\n",
    "# But we can directly copy matching keys by name + shape.\n",
    "tgt_sd = model.state_dict()\n",
    "\n",
    "copied, skipped_shape, skipped_missing = [], [], []\n",
    "\n",
    "for k, v in src_sd.items():\n",
    "    if k not in tgt_sd:\n",
    "        skipped_missing.append(k)\n",
    "        continue\n",
    "    if tgt_sd[k].shape != v.shape:\n",
    "        skipped_shape.append((k, tuple(v.shape), tuple(tgt_sd[k].shape)))\n",
    "        continue\n",
    "    # default: copy everything that matches (this will include GRU weights if names align)\n",
    "    tgt_sd[k].copy_(v)\n",
    "    copied.append(k)\n",
    "\n",
    "# If embedding/output names differ between notebooks, we force a safer transfer:\n",
    "# copy GRU weights by contains(\".gru.\") and ignore emb/fc unless matched already\n",
    "# (If they were matched above, it's still safe only if same shape.)\n",
    "# Also: explicitly re-init output head (always) because vocab differs.\n",
    "with torch.no_grad():\n",
    "    model.fc.reset_parameters()\n",
    "\n",
    "# Optional: PAD/UNK rows from src embedding if matching key exists and shape allows\n",
    "padunk_copied = False\n",
    "if \"emb.weight\" in src_sd and \"emb.weight\" in tgt_sd:\n",
    "    if src_sd[\"emb.weight\"].shape[1] == tgt_sd[\"emb.weight\"].shape[1] and src_sd[\"emb.weight\"].shape[0] >= 2 and tgt_sd[\"emb.weight\"].shape[0] >= 2:\n",
    "        tgt_sd[\"emb.weight\"][:2].copy_(src_sd[\"emb.weight\"][:2])\n",
    "        padunk_copied = True\n",
    "\n",
    "model.load_state_dict(tgt_sd)\n",
    "\n",
    "print(\"[11B-07] Transfer summary:\")\n",
    "print(\"  copied_keys:\", len(copied))\n",
    "print(\"  skipped_missing:\", len(skipped_missing))\n",
    "print(\"  skipped_shape:\", len(skipped_shape))\n",
    "print(\"  pad/unk emb rows copied:\", padunk_copied)\n",
    "if skipped_shape[:5]:\n",
    "    print(\"  skipped_shape sample:\", skipped_shape[:5])\n",
    "\n",
    "print(\"\\n[11B-07] CHECKPOINT E\")\n",
    "print(\"Paste: copied_keys/skipped counts + whether pad/unk emb rows copied.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80845dd0",
   "metadata": {},
   "source": [
    "Build TARGET dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d20b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-08] ✅ DataLoaders ready | train_batches: 8\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-08] Build TARGET dataloaders\n",
    "train_ds = TensorDataset(tr[\"input_ids\"].long(), tr[\"attn_mask\"].long(), tr[\"labels\"].long())\n",
    "val_ds   = TensorDataset(va[\"input_ids\"].long(), va[\"attn_mask\"].long(), va[\"labels\"].long())\n",
    "test_ds  = TensorDataset(te[\"input_ids\"].long(), te[\"attn_mask\"].long(), te[\"labels\"].long())\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=FT_CFG[\"batch_size\"], shuffle=True, drop_last=False)\n",
    "val_tensors  = (va[\"input_ids\"].long(), va[\"attn_mask\"].long(), va[\"labels\"].long())\n",
    "test_tensors = (te[\"input_ids\"].long(), te[\"attn_mask\"].long(), te[\"labels\"].long())\n",
    "\n",
    "def make_lengths(attn_mask: torch.Tensor) -> torch.Tensor:\n",
    "    return attn_mask.sum(dim=1).clamp(min=1).long()\n",
    "\n",
    "print(\"[11B-08] ✅ DataLoaders ready | train_batches:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949dd82",
   "metadata": {},
   "source": [
    "Fine-tune on TARGET with early stopping on HR@20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313ead19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-09] epoch=01 loss=6.6204 elapsed=0.5s | VAL: {'HR@5': 0.015873015873015872, 'MRR@5': 0.008994708994708995, 'NDCG@5': 0.010676098205322749, 'HR@10': 0.026455026455026454, 'MRR@10': 0.010338456370202401, 'NDCG@10': 0.014032517935467271, 'HR@20': 0.037037037037037035, 'MRR@20': 0.010963088939279416, 'NDCG@20': 0.016572515054434803, '_n_examples': 189}\n",
      "[11B-09] epoch=02 loss=6.5098 elapsed=0.8s | VAL: {'HR@5': 0.021164021164021163, 'MRR@5': 0.014109347442680775, 'NDCG@5': 0.015873015873015872, 'HR@10': 0.026455026455026454, 'MRR@10': 0.014638447971781305, 'NDCG@10': 0.017402459398507344, 'HR@20': 0.07407407407407407, 'MRR@20': 0.018135819670907386, 'NDCG@20': 0.029675130697266714, '_n_examples': 189}\n",
      "[11B-09] epoch=03 loss=6.4069 elapsed=1.2s | VAL: {'HR@5': 0.021164021164021163, 'MRR@5': 0.014109347442680775, 'NDCG@5': 0.015873015873015872, 'HR@10': 0.05291005291005291, 'MRR@10': 0.018060804568741075, 'NDCG@10': 0.02586080022338621, 'HR@20': 0.10052910052910052, 'MRR@20': 0.02117894791244833, 'NDCG@20': 0.0376749078112314, '_n_examples': 189}\n",
      "[11B-09] epoch=04 loss=6.3127 elapsed=1.5s | VAL: {'HR@5': 0.021164021164021163, 'MRR@5': 0.021164021164021163, 'NDCG@5': 0.021164021164021163, 'HR@10': 0.07936507936507936, 'MRR@10': 0.028569328966154363, 'NDCG@10': 0.039636629922482444, 'HR@20': 0.12169312169312169, 'MRR@20': 0.03158304805130202, 'NDCG@20': 0.05042569884152263, '_n_examples': 189}\n",
      "[11B-09] epoch=05 loss=6.2201 elapsed=1.8s | VAL: {'HR@5': 0.021164021164021163, 'MRR@5': 0.015873015873015872, 'NDCG@5': 0.017258515910809076, 'HR@10': 0.08994708994708994, 'MRR@10': 0.02506508776350046, 'NDCG@10': 0.03949443296573201, 'HR@20': 0.15873015873015872, 'MRR@20': 0.029593261783418548, 'NDCG@20': 0.056562267763337185, '_n_examples': 189}\n",
      "[11B-09] epoch=06 loss=6.1192 elapsed=2.1s | VAL: {'HR@5': 0.047619047619047616, 'MRR@5': 0.021164021164021163, 'NDCG@5': 0.027492717160400124, 'HR@10': 0.08994708994708994, 'MRR@10': 0.026774166456706137, 'NDCG@10': 0.04114703111853348, 'HR@20': 0.164021164021164, 'MRR@20': 0.03190485680042906, 'NDCG@20': 0.05984975957024525, '_n_examples': 189}\n",
      "[11B-09] epoch=07 loss=6.0207 elapsed=2.4s | VAL: {'HR@5': 0.047619047619047616, 'MRR@5': 0.021340388007054675, 'NDCG@5': 0.027934614116131826, 'HR@10': 0.10052910052910052, 'MRR@10': 0.028819181993785168, 'NDCG@10': 0.045461174913232795, 'HR@20': 0.18518518518518517, 'MRR@20': 0.034315491806031884, 'NDCG@20': 0.06639399680661259, '_n_examples': 189}\n",
      "[11B-09] epoch=08 loss=5.9319 elapsed=2.8s | VAL: {'HR@5': 0.0582010582010582, 'MRR@5': 0.023721340388007054, 'NDCG@5': 0.0322601663135283, 'HR@10': 0.12169312169312169, 'MRR@10': 0.03224993701184177, 'NDCG@10': 0.05285120402890055, 'HR@20': 0.18518518518518517, 'MRR@20': 0.03620869600401764, 'NDCG@20': 0.06835404142574586, '_n_examples': 189}\n",
      "[11B-09] epoch=09 loss=5.8518 elapsed=3.1s | VAL: {'HR@5': 0.07407407407407407, 'MRR@5': 0.02522045855379189, 'NDCG@5': 0.037278468529821376, 'HR@10': 0.12169312169312169, 'MRR@10': 0.03133450911228689, 'NDCG@10': 0.05243930820709551, 'HR@20': 0.18518518518518517, 'MRR@20': 0.035441737548720695, 'NDCG@20': 0.06812754868499371, '_n_examples': 189}\n",
      "[11B-09] epoch=10 loss=5.7781 elapsed=3.6s | VAL: {'HR@5': 0.07936507936507936, 'MRR@5': 0.04514991181657848, 'NDCG@5': 0.053615227797957375, 'HR@10': 0.12169312169312169, 'MRR@10': 0.05057949105568153, 'NDCG@10': 0.06707486769253146, 'HR@20': 0.18518518518518517, 'MRR@20': 0.05507283396553139, 'NDCG@20': 0.0832359256891058, '_n_examples': 189}\n",
      "[11B-09] epoch=11 loss=5.6924 elapsed=4.1s | VAL: {'HR@5': 0.08994708994708994, 'MRR@5': 0.04506172839506172, 'NDCG@5': 0.05599744893912837, 'HR@10': 0.1164021164021164, 'MRR@10': 0.04821953472747123, 'NDCG@10': 0.06417642415059092, 'HR@20': 0.18518518518518517, 'MRR@20': 0.0532798885217923, 'NDCG@20': 0.0819058501127215, '_n_examples': 189}\n",
      "[11B-09] epoch=12 loss=5.6245 elapsed=4.6s | VAL: {'HR@5': 0.08465608465608465, 'MRR@5': 0.0437389770723104, 'NDCG@5': 0.0537187369916501, 'HR@10': 0.1164021164021164, 'MRR@10': 0.04777861761988746, 'NDCG@10': 0.06378240631479531, 'HR@20': 0.18518518518518517, 'MRR@20': 0.05293928533031191, 'NDCG@20': 0.08165414597553809, '_n_examples': 189}\n",
      "[11B-09] epoch=13 loss=5.5489 elapsed=5.0s | VAL: {'HR@5': 0.07936507936507936, 'MRR@5': 0.04691358024691358, 'NDCG@5': 0.055041559212902734, 'HR@10': 0.1164021164021164, 'MRR@10': 0.05130595448055765, 'NDCG@10': 0.06646934479830209, 'HR@20': 0.19047619047619047, 'MRR@20': 0.056787135011494926, 'NDCG@20': 0.08561414438480601, '_n_examples': 189}\n",
      "[11B-09] epoch=14 loss=5.4709 elapsed=5.7s | VAL: {'HR@5': 0.06878306878306878, 'MRR@5': 0.046296296296296294, 'NDCG@5': 0.0519760096232157, 'HR@10': 0.12698412698412698, 'MRR@10': 0.05337826488620139, 'NDCG@10': 0.07010594499394271, 'HR@20': 0.20105820105820105, 'MRR@20': 0.05799168906692618, 'NDCG@20': 0.08816282865768477, '_n_examples': 189}\n",
      "[11B-09] epoch=15 loss=5.3980 elapsed=6.5s | VAL: {'HR@5': 0.07407407407407407, 'MRR@5': 0.046472663139329795, 'NDCG@5': 0.0533300998542373, 'HR@10': 0.12698412698412698, 'MRR@10': 0.05315780633240951, 'NDCG@10': 0.07007027450618102, 'HR@20': 0.1746031746031746, 'MRR@20': 0.05605045987005788, 'NDCG@20': 0.08158556656713188, '_n_examples': 189}\n",
      "[11B-09] epoch=16 loss=5.3273 elapsed=7.0s | VAL: {'HR@5': 0.06878306878306878, 'MRR@5': 0.04514991181657848, 'NDCG@5': 0.05105138790675903, 'HR@10': 0.12169312169312169, 'MRR@10': 0.052187788695725205, 'NDCG@10': 0.06814681314489393, 'HR@20': 0.164021164021164, 'MRR@20': 0.054947907451716, 'NDCG@20': 0.07860886946651999, '_n_examples': 189}\n",
      "[11B-09] epoch=17 loss=5.2564 elapsed=7.4s | VAL: {'HR@5': 0.07407407407407407, 'MRR@5': 0.04603174603174603, 'NDCG@5': 0.052963309156212925, 'HR@10': 0.1164021164021164, 'MRR@10': 0.051599899218946844, 'NDCG@10': 0.06658128898285574, 'HR@20': 0.15873015873015872, 'MRR@20': 0.054542551490804475, 'NDCG@20': 0.07727447799000586, '_n_examples': 189}\n",
      "[11B-09] epoch=18 loss=5.1873 elapsed=7.9s | VAL: {'HR@5': 0.07407407407407407, 'MRR@5': 0.04514991181657848, 'NDCG@5': 0.05222972776016417, 'HR@10': 0.1164021164021164, 'MRR@10': 0.05090702947845805, 'NDCG@10': 0.06603679178307793, 'HR@20': 0.15873015873015872, 'MRR@20': 0.05356607105280259, 'NDCG@20': 0.0763959956019564, '_n_examples': 189}\n",
      "[11B-09] epoch=19 loss=5.1284 elapsed=8.4s | VAL: {'HR@5': 0.07407407407407407, 'MRR@5': 0.04576719576719577, 'NDCG@5': 0.05269060608150071, 'HR@10': 0.1164021164021164, 'MRR@10': 0.051488620139413785, 'NDCG@10': 0.06646643578059695, 'HR@20': 0.15343915343915343, 'MRR@20': 0.0538765948724669, 'NDCG@20': 0.07560306816434624, '_n_examples': 189}\n",
      "[11B-09] epoch=20 loss=5.0583 elapsed=9.0s | VAL: {'HR@5': 0.07407407407407407, 'MRR@5': 0.04603174603174603, 'NDCG@5': 0.05292247777906078, 'HR@10': 0.12169312169312169, 'MRR@10': 0.05228227093306458, 'NDCG@10': 0.06822775100364849, 'HR@20': 0.15873015873015872, 'MRR@20': 0.05462354532456258, 'NDCG@20': 0.0772994897190678, '_n_examples': 189}\n",
      "[11B-09] epoch=21 loss=5.0019 elapsed=9.6s | VAL: {'HR@5': 0.07407407407407407, 'MRR@5': 0.04550264550264551, 'NDCG@5': 0.05245873438394066, 'HR@10': 0.12698412698412698, 'MRR@10': 0.05250272948685648, 'NDCG@10': 0.06950901891350286, 'HR@20': 0.15873015873015872, 'MRR@20': 0.05444317014418739, 'NDCG@20': 0.07719698322043801, '_n_examples': 189}\n",
      "[11B-09] ✅ Early stop at epoch=21 (best epoch=14, best HR@20=0.201058)\n",
      "[11B-09] ✅ Restored best weights from epoch=14 with best HR@20=0.201058\n",
      "\n",
      "[11B-09] CHECKPOINT F\n",
      "Paste: best_epoch + best_metric + last 2 epoch lines.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-09] Fine-tune on TARGET with early stopping on HR@20\n",
    "opt = torch.optim.Adam(model.parameters(), lr=FT_CFG[\"lr\"], weight_decay=FT_CFG[\"weight_decay\"])\n",
    "\n",
    "best_metric = -1.0\n",
    "best_epoch = -1\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(1, FT_CFG[\"max_epochs\"] + 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for (x, m, y) in train_loader:\n",
    "        x = x.to(device)\n",
    "        m = m.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        lengths = make_lengths(m)\n",
    "        logits = model(x, lengths)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=PAD_ID_TARGET)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), FT_CFG[\"grad_clip\"])\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(float(loss.item()))\n",
    "\n",
    "    val_res = eval_target_model(model, val_tensors[0], val_tensors[1], val_tensors[2], PAD_ID_TARGET, K_LIST)\n",
    "    metric = float(val_res[FT_CFG[\"early_stop_metric\"]])\n",
    "    mean_loss = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[11B-09] epoch={epoch:02d} loss={mean_loss:.4f} elapsed={dt:.1f}s | VAL: {val_res}\")\n",
    "\n",
    "    if metric > best_metric + FT_CFG[\"min_delta\"]:\n",
    "        best_metric = metric\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    if bad_epochs >= FT_CFG[\"patience\"]:\n",
    "        print(f\"[11B-09] ✅ Early stop at epoch={epoch} (best epoch={best_epoch}, best {FT_CFG['early_stop_metric']}={best_metric:.6f})\")\n",
    "        break\n",
    "\n",
    "assert best_state is not None, \"No best_state captured (unexpected).\"\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"[11B-09] ✅ Restored best weights from epoch={best_epoch} with best {FT_CFG['early_stop_metric']}={best_metric:.6f}\")\n",
    "\n",
    "print(\"\\n[11B-09] CHECKPOINT F\")\n",
    "print(\"Paste: best_epoch + best_metric + last 2 epoch lines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505050db",
   "metadata": {},
   "source": [
    "Final eval on TARGET VAL/TEST (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e35dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-10] TARGET VAL (transfer-finetune): {'HR@5': 0.06878306878306878, 'MRR@5': 0.046296296296296294, 'NDCG@5': 0.0519760096232157, 'HR@10': 0.12698412698412698, 'MRR@10': 0.05337826488620139, 'NDCG@10': 0.07010594499394271, 'HR@20': 0.20105820105820105, 'MRR@20': 0.05799168906692618, 'NDCG@20': 0.08816282865768477, '_n_examples': 189}\n",
      "[11B-10] TARGET TEST (transfer-finetune): {'HR@5': 0.09, 'MRR@5': 0.055, 'NDCG@5': 0.06377071188430579, 'HR@10': 0.15, 'MRR@10': 0.06272619047619048, 'NDCG@10': 0.0828935307213819, 'HR@20': 0.23, 'MRR@20': 0.06820425152333047, 'NDCG@20': 0.1030238813429633, '_n_examples': 200}\n",
      "\n",
      "[11B-10] CHECKPOINT G\n",
      "Paste VAL/TEST metrics dicts before writing reports.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-10] Final eval on TARGET VAL/TEST (best model)\n",
    "val_final  = eval_target_model(model, val_tensors[0], val_tensors[1], val_tensors[2], PAD_ID_TARGET, K_LIST)\n",
    "test_final = eval_target_model(model, test_tensors[0], test_tensors[1], test_tensors[2], PAD_ID_TARGET, K_LIST)\n",
    "\n",
    "print(\"[11B-10] TARGET VAL (transfer-finetune):\", val_final)\n",
    "print(\"[11B-10] TARGET TEST (transfer-finetune):\", test_final)\n",
    "\n",
    "print(\"\\n[11B-10] CHECKPOINT G\")\n",
    "print(\"Paste VAL/TEST metrics dicts before writing reports.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7db27",
   "metadata": {},
   "source": [
    "Write reports + update meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ece87f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11B-11] ✅ Saved model: C:\\mooc-coldstart-session-meta\\reports\\11B_transfer_finetune_target\\20260104_114435\\model_transfer_finetuned_target.pt\n",
      "[11B-11] ✅ Wrote report files under: C:\\mooc-coldstart-session-meta\\reports\\11B_transfer_finetune_target\\20260104_114435\n",
      "[11B-11] ✅ Updated meta.json: C:\\mooc-coldstart-session-meta\\meta.json\n",
      "\n",
      "[11B-11] CHECKPOINT H\n",
      "Paste: report_dir path + confirm meta.json updated.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11B-11] Write reports + update meta.json\n",
    "report_dir = REPO_ROOT / \"reports\" / \"11B_transfer_finetune_target\" / RUN_TAG\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = report_dir / \"model_transfer_finetuned_target.pt\"\n",
    "torch.save({\"state_dict\": model.state_dict(), \"cfg\": FT_CFG}, model_path)\n",
    "\n",
    "# Save metrics + cfg\n",
    "(report_dir / \"metrics_target_val.json\").write_text(json.dumps(val_final, indent=2), encoding=\"utf-8\")\n",
    "(report_dir / \"metrics_target_test.json\").write_text(json.dumps(test_final, indent=2), encoding=\"utf-8\")\n",
    "(report_dir / \"finetune_cfg.json\").write_text(json.dumps(FT_CFG, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Update meta.json (append-only)\n",
    "meta_path = REPO_ROOT / \"meta.json\"\n",
    "meta = {}\n",
    "if meta_path.exists():\n",
    "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "runs = meta.get(\"runs\", [])\n",
    "runs.append({\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"notebook\": \"11B_transfer_finetune_target.ipynb\",\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"artifacts\": {\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"model\": str(model_path),\n",
    "        \"val_metrics\": str(report_dir / \"metrics_target_val.json\"),\n",
    "        \"test_metrics\": str(report_dir / \"metrics_target_test.json\"),\n",
    "        \"cfg\": str(report_dir / \"finetune_cfg.json\"),\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"target_run_tag\": target_run_tag,\n",
    "        \"source_pretrain_run_tag\": src_pretrain_run_tag,\n",
    "        \"source_checkpoint\": str(src_ckpt_pt),\n",
    "        \"dataloader_config\": str(cfg_path),\n",
    "        \"sanity_metrics\": str(san_path),\n",
    "        \"session_gap_thresholds\": str(gaps_path),\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"target_val\": val_final,\n",
    "        \"target_test\": test_final,\n",
    "    }\n",
    "})\n",
    "meta[\"runs\"] = runs\n",
    "meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"[11B-11] ✅ Saved model:\", model_path)\n",
    "print(\"[11B-11] ✅ Wrote report files under:\", report_dir)\n",
    "print(\"[11B-11] ✅ Updated meta.json:\", meta_path)\n",
    "\n",
    "print(\"\\n[11B-11] CHECKPOINT H\")\n",
    "print(\"Paste: report_dir path + confirm meta.json updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
