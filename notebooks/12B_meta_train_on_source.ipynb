{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6700e9a",
   "metadata": {},
   "source": [
    "Imports + versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39a286b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-00] torch: 2.9.1+cpu\n",
      "[12B-00] pandas: 2.3.3\n",
      "[12B-00] numpy: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-00] Imports + versions\n",
    "import os, json, time, math, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"[12B-00] torch:\", torch.__version__)\n",
    "print(\"[12B-00] pandas:\", pd.__version__)\n",
    "print(\"[12B-00] numpy:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8c245",
   "metadata": {},
   "source": [
    "Repo root (portable) + load single source-of-truth JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cc54c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-01] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[12B-01] RUN_TAG: 20260104_165117\n",
      "[12B-01] Expect config: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\dataloader_config_20251229_163357_20251229_232834.json\n",
      "[12B-01] Expect sanity: C:\\mooc-coldstart-session-meta\\data\\processed\\supervised\\sanity_metrics_20251229_163357_20251229_232834.json\n",
      "[12B-01] Expect gaps  : C:\\mooc-coldstart-session-meta\\data\\processed\\normalized_events\\session_gap_thresholds.json\n",
      "[12B-01] target: gap_minutes from primary_threshold_seconds=1800 -> 30m | label=30m\n",
      "[12B-01] source: gap_minutes from primary_threshold_seconds=600 -> 10m | label=10m\n",
      "[12B-01] ✅ Session gaps confirmed.\n",
      "\n",
      "[12B-01] CHECKPOINT A\n",
      "Paste: inferred gaps lines + confirm asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-01] Repo root (portable) + load single source-of-truth JSONs\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    # fallback: git root\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not locate repo root (PROJECT_STATE.md or .git).\")\n",
    "\n",
    "REPO_ROOT = Path(os.getenv(\"MOOC_REPO_ROOT\", \"\")).expanduser().resolve() if os.getenv(\"MOOC_REPO_ROOT\") else find_repo_root(Path.cwd().resolve())\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(\"[12B-01] REPO_ROOT:\", str(REPO_ROOT))\n",
    "print(\"[12B-01] RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "CFG_PATH = REPO_ROOT / \"data/processed/supervised/dataloader_config_20251229_163357_20251229_232834.json\"\n",
    "SANITY_PATH = REPO_ROOT / \"data/processed/supervised/sanity_metrics_20251229_163357_20251229_232834.json\"\n",
    "GAPS_PATH = REPO_ROOT / \"data/processed/normalized_events/session_gap_thresholds.json\"\n",
    "\n",
    "print(\"[12B-01] Expect config:\", CFG_PATH)\n",
    "print(\"[12B-01] Expect sanity:\", SANITY_PATH)\n",
    "print(\"[12B-01] Expect gaps  :\", GAPS_PATH)\n",
    "\n",
    "assert CFG_PATH.exists(), f\"Missing: {CFG_PATH}\"\n",
    "assert SANITY_PATH.exists(), f\"Missing: {SANITY_PATH}\"\n",
    "assert GAPS_PATH.exists(), f\"Missing: {GAPS_PATH}\"\n",
    "\n",
    "DL_CFG = json.loads(CFG_PATH.read_text(encoding=\"utf-8\"))\n",
    "SANITY = json.loads(SANITY_PATH.read_text(encoding=\"utf-8\"))\n",
    "GAPS = json.loads(GAPS_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def infer_gap_minutes(d: dict, name: str) -> int:\n",
    "    if \"gap_minutes\" in d:  # legacy\n",
    "        return int(d[\"gap_minutes\"])\n",
    "    if \"primary_threshold_seconds\" in d:\n",
    "        sec = int(d[\"primary_threshold_seconds\"])\n",
    "        return sec // 60\n",
    "    raise KeyError(f\"[12A-01] {name}: cannot infer gap minutes. keys={list(d.keys())}\")\n",
    "\n",
    "gap_target_m = infer_gap_minutes(GAPS[\"target\"], \"target\")\n",
    "gap_source_m = infer_gap_minutes(GAPS[\"source\"], \"source\")\n",
    "print(f\"[12B-01] target: gap_minutes from primary_threshold_seconds={GAPS['target'].get('primary_threshold_seconds')} -> {gap_target_m}m | label={GAPS['target'].get('primary_threshold_label')}\")\n",
    "print(f\"[12B-01] source: gap_minutes from primary_threshold_seconds={GAPS['source'].get('primary_threshold_seconds')} -> {gap_source_m}m | label={GAPS['source'].get('primary_threshold_label')}\")\n",
    "\n",
    "assert gap_target_m == 30, f\"target gap mismatch: got {gap_target_m}m\"\n",
    "assert gap_source_m == 10, f\"source gap mismatch: got {gap_source_m}m\"\n",
    "print(\"[12B-01] ✅ Session gaps confirmed.\")\n",
    "\n",
    "print(\"\\n[12B-01] CHECKPOINT A\")\n",
    "print(\"Paste: inferred gaps lines + confirm asserts passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86cbb5",
   "metadata": {},
   "source": [
    "Load 12A task artifacts (auto-pick latest run dir) + basic asserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4c9b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-02] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[12B-02] Using TASK_RUN_DIR: C:\\mooc-coldstart-session-meta\\reports\\12A_task_builder_for_meta\\20260104_141727\n",
      "[12B-02] TASK_META keys: ['run_tag', 'created_at', 'protocol', 'session_gaps', 'task_cfg', 'task_definition', 'source', 'notes']\n",
      "[12B-02] TASK_COV keys : ['pairs_total_seen', 'pairs_total_kept', 'tasks_total', 'need_per_task', 'pairs_kept_per_task', 'pairs_seen_per_task']\n",
      "[12B-02] TASK_CFG keys : ['n_support', 'n_query', 'sample_mod', 'sample_rem', 'max_files', 'max_pairs_per_task_buffer', 'seed', 'task_key_mode', 'require_multi_task', 'min_tasks_required']\n",
      "[12B-02] TASK_CFG: {'n_support': 20, 'n_query': 20, 'sample_mod': 10, 'sample_rem': 0, 'max_files': 50, 'max_pairs_per_task_buffer': 50000, 'seed': 42, 'task_key_mode': 'len_bin', 'require_multi_task': True, 'min_tasks_required': 3}\n",
      "[12B-02] task_unique: 5 | top10: [('len_21_plus', 7920), ('len_03_05', 7541), ('len_06_10', 6939), ('len_11_20', 6686), ('len_01_02', 3454)]\n",
      "[12B-02] need_per_task: 40 | n_support+n_query: 40\n",
      "\n",
      "[12B-02] CHECKPOINT B\n",
      "Paste: TASK_RUN_DIR + TASK_CFG + TASK_COV keys + task_unique/top10 + need_per_task line.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-02] Load TASK_META + TASK_COV robustly and assert we can proceed\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "REPO_ROOT = REPO_ROOT  # from 12B-01\n",
    "TASK_RUN_DIR = Path(r\"C:\\mooc-coldstart-session-meta\\reports\\12A_task_builder_for_meta\\20260104_141727\")\n",
    "\n",
    "meta_path = TASK_RUN_DIR / \"meta_task_config.json\"\n",
    "cov_path  = TASK_RUN_DIR / \"task_coverage.json\"\n",
    "assert meta_path.exists(), f\"[12B-02] Missing: {meta_path}\"\n",
    "assert cov_path.exists(),  f\"[12B-02] Missing: {cov_path}\"\n",
    "\n",
    "TASK_META = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "TASK_COV  = json.loads(cov_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"[12B-02] REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"[12B-02] Using TASK_RUN_DIR:\", TASK_RUN_DIR)\n",
    "print(\"[12B-02] TASK_META keys:\", list(TASK_META.keys()))\n",
    "print(\"[12B-02] TASK_COV keys :\", list(TASK_COV.keys()))\n",
    "\n",
    "# IMPORTANT: real cfg is nested\n",
    "TASK_CFG = TASK_META[\"task_cfg\"]\n",
    "print(\"[12B-02] TASK_CFG keys :\", list(TASK_CFG.keys()))\n",
    "print(\"[12B-02] TASK_CFG:\", TASK_CFG)\n",
    "\n",
    "# derive coverage in a defensible way (no guessing)\n",
    "pairs_kept_per_task = TASK_COV.get(\"pairs_kept_per_task\", {}) or {}\n",
    "task_unique = int(TASK_COV.get(\"tasks_total\", len(pairs_kept_per_task))) if (TASK_COV.get(\"tasks_total\") is not None or pairs_kept_per_task) else None\n",
    "top10 = sorted(pairs_kept_per_task.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "\n",
    "need_per_task = int(TASK_COV.get(\"need_per_task\", TASK_CFG[\"n_support\"] + TASK_CFG[\"n_query\"]))\n",
    "print(\"[12B-02] task_unique:\", task_unique, \"| top10:\", top10)\n",
    "print(\"[12B-02] need_per_task:\", need_per_task, \"| n_support+n_query:\", int(TASK_CFG[\"n_support\"]) + int(TASK_CFG[\"n_query\"]))\n",
    "\n",
    "# hard sanity\n",
    "assert int(TASK_CFG[\"n_support\"]) > 0 and int(TASK_CFG[\"n_query\"]) > 0\n",
    "assert int(TASK_CFG[\"n_support\"]) + int(TASK_CFG[\"n_query\"]) <= int(TASK_CFG[\"max_pairs_per_task_buffer\"]), \\\n",
    "    \"[12B-02] n_support+n_query must be <= max_pairs_per_task_buffer\"\n",
    "\n",
    "min_required = int(TASK_CFG.get(\"min_tasks_required\", 1))\n",
    "assert task_unique is not None and task_unique >= min_required, \\\n",
    "    f\"[12B-02] Not enough tasks: task_unique={task_unique}, min_required={min_required}\"\n",
    "\n",
    "print(\"\\n[12B-02] CHECKPOINT B\")\n",
    "print(\"Paste: TASK_RUN_DIR + TASK_CFG + TASK_COV keys + task_unique/top10 + need_per_task line.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a2898",
   "metadata": {},
   "source": [
    "Load source shard paths + vocab + normalize protocol (same logic as 12A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24797414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-03] ✅ PROTO: {'K_LIST': [5, 10, 20], 'MAX_PREFIX_LEN': 20, 'CAP_ENABLED': True, 'CAP_SESSION_LEN': 200, 'CAP_STRATEGY': 'take_last'}\n",
      "[12B-03] TRAIN_DIR: C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\train\n",
      "[12B-03] VAL_DIR  : C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\val\n",
      "[12B-03] TEST_DIR : C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\test\n",
      "[12B-03] VOCAB    : C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\source_vocab_items_20251229_232834.json\n",
      "[12B-03] Source shards: train= 1024 val= 1024 test= 1024\n",
      "[12B-03] VOCAB_SIZE_SOURCE: 1620\n",
      "[12B-03] PAD/UNK: 0 1\n",
      "[12B-03] seq_col: items | first seq type: <class 'numpy.ndarray'> | first elem type: <class 'str'>\n",
      "\n",
      "[12B-03] CHECKPOINT C\n",
      "Paste: shard counts + VOCAB_SIZE_SOURCE/PAD/UNK + seq_col + first elem type.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-03] Resolve SOURCE shard dirs + vocab + protocol (robust: prefer TASK_META paths)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- PROTO: prefer TASK_META (already normalized + asserted in 12B-02) ----\n",
    "assert \"protocol\" in TASK_META, f\"[12B-03] TASK_META missing protocol. Keys={list(TASK_META.keys())}\"\n",
    "PROTO = TASK_META[\"protocol\"]\n",
    "print(\"[12B-03] ✅ PROTO:\", PROTO)\n",
    "\n",
    "# hard asserts (must match earlier notebooks)\n",
    "assert PROTO[\"MAX_PREFIX_LEN\"] == 20\n",
    "assert PROTO[\"CAP_ENABLED\"] is True\n",
    "assert PROTO[\"CAP_SESSION_LEN\"] == 200\n",
    "assert PROTO[\"CAP_STRATEGY\"] == \"take_last\"\n",
    "assert PROTO[\"K_LIST\"] == [5, 10, 20]\n",
    "\n",
    "K_LIST = list(map(int, PROTO[\"K_LIST\"]))\n",
    "MAX_LEN = int(PROTO[\"MAX_PREFIX_LEN\"])\n",
    "CAP_ENABLED = bool(PROTO[\"CAP_ENABLED\"])\n",
    "CAP_SESSION_LEN = int(PROTO[\"CAP_SESSION_LEN\"])\n",
    "CAP_STRATEGY = str(PROTO[\"CAP_STRATEGY\"])\n",
    "\n",
    "# ---- SOURCE paths: prefer TASK_META (12A wrote them) ----\n",
    "assert \"source\" in TASK_META and \"paths\" in TASK_META[\"source\"], (\n",
    "    f\"[12B-03] TASK_META missing source.paths. \"\n",
    "    f\"Keys={list(TASK_META.get('source', {}).keys())}\"\n",
    ")\n",
    "SRC_PATHS = TASK_META[\"source\"][\"paths\"]\n",
    "\n",
    "TRAIN_DIR = Path(SRC_PATHS[\"train_dir\"])\n",
    "VAL_DIR   = Path(SRC_PATHS[\"val_dir\"])\n",
    "TEST_DIR  = Path(SRC_PATHS[\"test_dir\"])\n",
    "SRC_VOCAB_PATH = Path(SRC_PATHS[\"vocab\"])\n",
    "\n",
    "print(\"[12B-03] TRAIN_DIR:\", TRAIN_DIR)\n",
    "print(\"[12B-03] VAL_DIR  :\", VAL_DIR)\n",
    "print(\"[12B-03] TEST_DIR :\", TEST_DIR)\n",
    "print(\"[12B-03] VOCAB    :\", SRC_VOCAB_PATH)\n",
    "\n",
    "assert TRAIN_DIR.exists(), f\"[12B-03] Missing TRAIN_DIR: {TRAIN_DIR}\"\n",
    "assert VAL_DIR.exists(),   f\"[12B-03] Missing VAL_DIR: {VAL_DIR}\"\n",
    "assert TEST_DIR.exists(),  f\"[12B-03] Missing TEST_DIR: {TEST_DIR}\"\n",
    "assert SRC_VOCAB_PATH.exists(), f\"[12B-03] Missing SRC_VOCAB_PATH: {SRC_VOCAB_PATH}\"\n",
    "\n",
    "train_files = sorted(TRAIN_DIR.glob(\"*.parquet\"))\n",
    "val_files   = sorted(VAL_DIR.glob(\"*.parquet\"))\n",
    "test_files  = sorted(TEST_DIR.glob(\"*.parquet\"))\n",
    "print(\"[12B-03] Source shards:\", \"train=\", len(train_files), \"val=\", len(val_files), \"test=\", len(test_files))\n",
    "assert len(train_files) == 1024 and len(val_files) == 1024 and len(test_files) == 1024\n",
    "\n",
    "source_vocab = json.loads(SRC_VOCAB_PATH.read_text(encoding=\"utf-8\"))\n",
    "VOCAB_SIZE_SOURCE = int(source_vocab[\"vocab_size\"])\n",
    "PAD_ID_SOURCE = int(source_vocab.get(\"pad_id\", 0))\n",
    "UNK_ID_SOURCE = int(source_vocab.get(\"unk_id\", 1))\n",
    "\n",
    "print(\"[12B-03] VOCAB_SIZE_SOURCE:\", VOCAB_SIZE_SOURCE)\n",
    "print(\"[12B-03] PAD/UNK:\", PAD_ID_SOURCE, UNK_ID_SOURCE)\n",
    "\n",
    "# detect seq_col (parquet schema must match 11A/12A)\n",
    "probe = pd.read_parquet(train_files[0], columns=None)\n",
    "cols = list(probe.columns)\n",
    "assert \"items\" in cols, f\"[12B-03] Expected 'items' col. Got: {cols}\"\n",
    "SEQ_COL = \"items\"\n",
    "\n",
    "first_seq = probe[SEQ_COL].iloc[0]\n",
    "first_elem = first_seq[0] if len(first_seq) > 0 else None\n",
    "print(\"[12B-03] seq_col:\", SEQ_COL, \"| first seq type:\", type(first_seq), \"| first elem type:\", type(first_elem))\n",
    "\n",
    "print(\"\\n[12B-03] CHECKPOINT C\")\n",
    "print(\"Paste: shard counts + VOCAB_SIZE_SOURCE/PAD/UNK + seq_col + first elem type.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393a998",
   "metadata": {},
   "source": [
    "Pair builder + task_key builder + reservoir buffer (no recency bias)\n",
    "- Uses pyarrow row-group streaming (fast, avoids full shard load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d99ea07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-04] ✅ Pair+task generator ready\n",
      "[12B-04] stable_mod probe: 3 3 (should match)\n",
      "\n",
      "[12B-04] CHECKPOINT D\n",
      "Proceed to build task buffers.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-04] Pair builder + task_key builder + reservoir buffer (no recency bias)\n",
    "# Uses pyarrow row-group streaming (fast, avoids full shard load)\n",
    "\n",
    "import hashlib\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "MAX_LEN = int(PROTO[\"MAX_PREFIX_LEN\"])\n",
    "CAP_ENABLED = bool(PROTO[\"CAP_ENABLED\"])\n",
    "CAP_SESSION_LEN = int(PROTO[\"CAP_SESSION_LEN\"])\n",
    "CAP_STRATEGY = str(PROTO[\"CAP_STRATEGY\"])\n",
    "\n",
    "source_token_to_id = dict(source_vocab[\"item2id\"])\n",
    "assert len(source_token_to_id) == VOCAB_SIZE_SOURCE\n",
    "\n",
    "def stable_mod(value, mod: int) -> int:\n",
    "    h = hashlib.md5(str(value).encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16) % mod\n",
    "\n",
    "def session_len_to_task_key(L: int) -> str:\n",
    "    # same bins you printed in 12A\n",
    "    if L <= 2: return \"len_01_02\"\n",
    "    if L <= 5: return \"len_03_05\"\n",
    "    if L <= 10: return \"len_06_10\"\n",
    "    if L <= 20: return \"len_11_20\"\n",
    "    return \"len_21_plus\"\n",
    "\n",
    "def session_to_one_pair(seq_tokens):\n",
    "    # seq_tokens: list[str] or np.ndarray of str\n",
    "    if isinstance(seq_tokens, np.ndarray):\n",
    "        seq_tokens = seq_tokens.tolist()\n",
    "    if len(seq_tokens) < 2:\n",
    "        return None\n",
    "\n",
    "    if CAP_ENABLED and len(seq_tokens) > CAP_SESSION_LEN and CAP_STRATEGY == \"take_last\":\n",
    "        seq_tokens = seq_tokens[-CAP_SESSION_LEN:]\n",
    "\n",
    "    # label is last token\n",
    "    y_tok = seq_tokens[-1]\n",
    "    y = int(source_token_to_id.get(y_tok, UNK_ID_SOURCE))\n",
    "    if y == UNK_ID_SOURCE:\n",
    "        return None  # filter UNK labels (panel concern)\n",
    "\n",
    "    prefix = seq_tokens[:-1]\n",
    "    if len(prefix) > MAX_LEN:\n",
    "        prefix = prefix[-MAX_LEN:]  # take last MAX_LEN\n",
    "\n",
    "    # left-pad (recent at end), mask tracks non-pad\n",
    "    x = np.full((MAX_LEN,), PAD_ID_SOURCE, dtype=np.int64)\n",
    "    m = np.zeros((MAX_LEN,), dtype=np.int64)\n",
    "\n",
    "    ids = [int(source_token_to_id.get(t, UNK_ID_SOURCE)) for t in prefix]\n",
    "    # map UNK tokens inside prefix to UNK_ID_SOURCE (allowed)\n",
    "    ids = [i if i >= 0 else UNK_ID_SOURCE for i in ids]\n",
    "\n",
    "    start = MAX_LEN - len(ids)\n",
    "    x[start:] = np.asarray(ids, dtype=np.int64)\n",
    "    m[start:] = 1\n",
    "    return x, m, y\n",
    "\n",
    "class ReservoirBuffer:\n",
    "    \"\"\"\n",
    "    Uniform reservoir sampling => avoids 'take last 50k' recency bias.\n",
    "    Keeps at most cap items; each incoming item has chance cap/seen to replace a random stored item.\n",
    "    \"\"\"\n",
    "    def __init__(self, cap: int, seed: int):\n",
    "        self.cap = int(cap)\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.items = []\n",
    "        self.seen = 0\n",
    "\n",
    "    def add(self, item):\n",
    "        self.seen += 1\n",
    "        if len(self.items) < self.cap:\n",
    "            self.items.append(item)\n",
    "        else:\n",
    "            j = self.rng.integers(0, self.seen)\n",
    "            if j < self.cap:\n",
    "                self.items[int(j)] = item\n",
    "\n",
    "def iter_pairs_from_files(files, sample_mod: int, sample_rem: int, max_files=None, seed=42):\n",
    "    \"\"\"\n",
    "    Yields: (task_key, x, m, y)\n",
    "    Streams row groups via pyarrow for speed + low memory.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    n_files = 0\n",
    "    sessions_seen = 0\n",
    "    yielded = 0\n",
    "\n",
    "    for fp in files:\n",
    "        n_files += 1\n",
    "        if max_files is not None and n_files > int(max_files):\n",
    "            break\n",
    "\n",
    "        pf = pq.ParquetFile(fp)\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            table = pf.read_row_group(rg, columns=[\"session_id\", \"session_length\", SEQ_COL])\n",
    "            df = table.to_pandas()\n",
    "\n",
    "            for sid, slen, seq in zip(df[\"session_id\"], df[\"session_length\"], df[SEQ_COL]):\n",
    "                sessions_seen += 1\n",
    "\n",
    "                if sample_mod and int(sample_mod) > 1:\n",
    "                    if stable_mod(sid, int(sample_mod)) != int(sample_rem):\n",
    "                        continue\n",
    "\n",
    "                pair = session_to_one_pair(seq)\n",
    "                if pair is None:\n",
    "                    continue\n",
    "\n",
    "                x, m, y = pair\n",
    "                task_key = session_len_to_task_key(int(slen))\n",
    "                yielded += 1\n",
    "                yield task_key, x, m, y\n",
    "\n",
    "    # no prints here (caller prints)\n",
    "\n",
    "print(\"[12B-04] ✅ Pair+task generator ready\")\n",
    "print(\"[12B-04] stable_mod probe:\", stable_mod(\"3160332::21\", 10), stable_mod(\"3160332::21\", 10), \"(should match)\")\n",
    "\n",
    "print(\"\\n[12B-04] CHECKPOINT D\")\n",
    "print(\"Proceed to build task buffers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805e882",
   "metadata": {},
   "source": [
    "Build task buffers (train + val) with coverage checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37b58f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-05] NEED_PER_TASK: 40 | BUF_CAP: 50000\n",
      "[12B-05] TRAIN: task_unique=5 | top10: [('len_21_plus', 7968), ('len_03_05', 7722), ('len_11_20', 6794), ('len_06_10', 6749), ('len_01_02', 3431)]\n",
      "[12B-05] TRAIN: tasks_ok=5 tasks_bad=0 need=40\n",
      "[12B-05] VAL: task_unique=5 | top10: [('len_03_05', 979), ('len_21_plus', 953), ('len_11_20', 875), ('len_06_10', 833), ('len_01_02', 442)]\n",
      "[12B-05] VAL: tasks_ok=5 tasks_bad=0 need=40\n",
      "\n",
      "[12B-05] CHECKPOINT E\n",
      "Paste: TRAIN/VAL task_unique+top10 + tasks_ok/tasks_bad.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-05] Build task buffers (train + val) with coverage checks\n",
    "\n",
    "TASKS_MIN = int(TASK_CFG.get(\"min_tasks_required\", 3))\n",
    "REQUIRE_MULTI_TASK = bool(TASK_CFG.get(\"require_multi_task\", True))\n",
    "N_SUPPORT = int(TASK_CFG[\"n_support\"])\n",
    "N_QUERY   = int(TASK_CFG[\"n_query\"])\n",
    "NEED_PER_TASK = N_SUPPORT + N_QUERY\n",
    "BUF_CAP = int(TASK_CFG[\"max_pairs_per_task_buffer\"])\n",
    "\n",
    "SAMPLE_MOD = int(TASK_CFG[\"sample_mod\"])\n",
    "SAMPLE_REM = int(TASK_CFG[\"sample_rem\"])\n",
    "MAX_FILES  = TASK_CFG.get(\"max_files\", None)\n",
    "\n",
    "print(\"[12B-05] NEED_PER_TASK:\", NEED_PER_TASK, \"| BUF_CAP:\", BUF_CAP)\n",
    "assert NEED_PER_TASK <= BUF_CAP, \"[12B-05] n_support+n_query must be <= buffer cap\"\n",
    "\n",
    "def build_task_buffers(files, seed: int, max_files=None):\n",
    "    buffers = {}  # task_key -> ReservoirBuffer\n",
    "    counts = {}   # task_key -> kept\n",
    "    seen_pairs = 0\n",
    "\n",
    "    gen = iter_pairs_from_files(\n",
    "        files,\n",
    "        sample_mod=SAMPLE_MOD,\n",
    "        sample_rem=SAMPLE_REM,\n",
    "        max_files=max_files,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    for task_key, x, m, y in gen:\n",
    "        seen_pairs += 1\n",
    "        if task_key not in buffers:\n",
    "            buffers[task_key] = ReservoirBuffer(cap=BUF_CAP, seed=seed + stable_mod(task_key, 9973))\n",
    "            counts[task_key] = 0\n",
    "\n",
    "        buffers[task_key].add((x, m, y))\n",
    "        # count kept is min(cap, seen) but we approximate by current len\n",
    "        counts[task_key] = len(buffers[task_key].items)\n",
    "\n",
    "        # optional: stop early if all tasks have enough\n",
    "        if len(counts) >= 5 and all(v >= NEED_PER_TASK for v in counts.values()):\n",
    "            # do NOT break: we want more diversity unless you want speed\n",
    "            pass\n",
    "\n",
    "    return buffers, counts, seen_pairs\n",
    "\n",
    "# build train buffers (bigger) and val buffers (smaller max_files to keep CPU reasonable)\n",
    "train_buffers, train_counts, train_seen = build_task_buffers(train_files, seed=42, max_files=MAX_FILES)\n",
    "val_buffers,   val_counts,   val_seen   = build_task_buffers(val_files,   seed=1337, max_files=MAX_FILES)\n",
    "\n",
    "def summarize_counts(name, counts):\n",
    "    items = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    print(f\"[12B-05] {name}: task_unique={len(items)} | top10:\", items[:10])\n",
    "    ok = [k for k,v in items if v >= NEED_PER_TASK]\n",
    "    bad = [k for k,v in items if v < NEED_PER_TASK]\n",
    "    print(f\"[12B-05] {name}: tasks_ok={len(ok)} tasks_bad={len(bad)} need={NEED_PER_TASK}\")\n",
    "    return ok, bad\n",
    "\n",
    "train_ok, train_bad = summarize_counts(\"TRAIN\", train_counts)\n",
    "val_ok,   val_bad   = summarize_counts(\"VAL\", val_counts)\n",
    "\n",
    "if REQUIRE_MULTI_TASK:\n",
    "    assert len(train_ok) >= TASKS_MIN, f\"[12B-05] Not enough tasks_ok in TRAIN: {len(train_ok)} < {TASKS_MIN}\"\n",
    "    assert len(val_ok)   >= max(2, TASKS_MIN), f\"[12B-05] Not enough tasks_ok in VAL: {len(val_ok)}\"\n",
    "\n",
    "print(\"\\n[12B-05] CHECKPOINT E\")\n",
    "print(\"Paste: TRAIN/VAL task_unique+top10 + tasks_ok/tasks_bad.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8522310",
   "metadata": {},
   "source": [
    "Episode sampler (support/query) + quick probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40ac1ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-06] episode 0 task=len_06_10 | support_nonzero_mean=6.55 | query_nonzero_mean=7.30\n",
      "  support shapes: (20, 20) (20, 20) (20,)\n",
      "  query   shapes: (20, 20) (20, 20) (20,)\n",
      "[12B-06] episode 1 task=len_03_05 | support_nonzero_mean=3.15 | query_nonzero_mean=2.85\n",
      "  support shapes: (20, 20) (20, 20) (20,)\n",
      "  query   shapes: (20, 20) (20, 20) (20,)\n",
      "[12B-06] episode 2 task=len_11_20 | support_nonzero_mean=15.20 | query_nonzero_mean=12.60\n",
      "  support shapes: (20, 20) (20, 20) (20,)\n",
      "  query   shapes: (20, 20) (20, 20) (20,)\n",
      "\n",
      "[12B-06] CHECKPOINT F\n",
      "Paste: the 3 episode lines (task + nonzero means + shapes).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-06] Episode sampler (support/query) + quick probe\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def sample_episode(buffers, ok_tasks, n_support, n_query, seed):\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    task = rng.choice(ok_tasks)\n",
    "    pool = buffers[task].items\n",
    "    # choose without replacement for support+query\n",
    "    idx = rng.choice(len(pool), size=(n_support + n_query), replace=False)\n",
    "    sup = [pool[i] for i in idx[:n_support]]\n",
    "    qry = [pool[i] for i in idx[n_support:]]\n",
    "\n",
    "    def stack(batch):\n",
    "        x = torch.tensor(np.stack([b[0] for b in batch], axis=0), dtype=torch.long, device=device)\n",
    "        m = torch.tensor(np.stack([b[1] for b in batch], axis=0), dtype=torch.long, device=device)\n",
    "        y = torch.tensor(np.array([b[2] for b in batch], dtype=np.int64), dtype=torch.long, device=device)\n",
    "        return x, m, y\n",
    "\n",
    "    return task, stack(sup), stack(qry)\n",
    "\n",
    "# probe 3 episodes\n",
    "for i in range(3):\n",
    "    task, (sx, sm, sy), (qx, qm, qy) = sample_episode(train_buffers, train_ok, N_SUPPORT, N_QUERY, seed=100+i)\n",
    "    print(f\"[12B-06] episode {i} task={task} | support_nonzero_mean={float(sm.sum(1).float().mean()):.2f} | query_nonzero_mean={float(qm.sum(1).float().mean()):.2f}\")\n",
    "    print(\"  support shapes:\", tuple(sx.shape), tuple(sm.shape), tuple(sy.shape))\n",
    "    print(\"  query   shapes:\", tuple(qx.shape), tuple(qm.shape), tuple(qy.shape))\n",
    "\n",
    "print(\"\\n[12B-06] CHECKPOINT F\")\n",
    "print(\"Paste: the 3 episode lines (task + nonzero means + shapes).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae648dd0",
   "metadata": {},
   "source": [
    "Model: GRU4RecDropout (same family as 11A/11B) for SOURCE vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f84e1031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-07] ✅ GRU4RecDropout ready\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-07] Model: GRU4RecDropout (same family as 11A/11B) for SOURCE vocab\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class GRU4RecDropout(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, pad_id: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=self.pad_id)\n",
    "        self.drop = nn.Dropout(float(dropout))\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        # input_ids: [B, T] left-padded. lengths: [B] counts of non-pad\n",
    "        emb = self.drop(self.emb(input_ids))  # [B,T,E]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h = self.gru(packed)  # h: [1,B,H]\n",
    "        logits = self.out(h.squeeze(0))  # [B,V]\n",
    "        return logits\n",
    "\n",
    "def make_lengths(attn_mask: torch.Tensor) -> torch.Tensor:\n",
    "    # attn_mask: [B,T] 0/1\n",
    "    lengths = attn_mask.sum(dim=1).clamp(min=1)\n",
    "    return lengths\n",
    "\n",
    "print(\"[12B-07] ✅ GRU4RecDropout ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee36909",
   "metadata": {},
   "source": [
    "Metrics (HR/MRR/NDCG @ K) on logits (PAD excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c5033e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-08] ✅ Metrics ready (PAD excluded).\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-08] Metrics (HR/MRR/NDCG @ K) on logits (PAD excluded)\n",
    "\n",
    "K_LIST = list(map(int, PROTO[\"K_LIST\"]))\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics_from_logits(logits: torch.Tensor, labels: torch.Tensor):\n",
    "    # logits: [B,V], labels: [B]\n",
    "    # exclude PAD from ranking\n",
    "    logits = logits.clone()\n",
    "    logits[:, PAD_ID_SOURCE] = -1e9\n",
    "\n",
    "    res = {}\n",
    "    B = labels.shape[0]\n",
    "    for k in K_LIST:\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices  # [B,k]\n",
    "        hits = (topk == labels.unsqueeze(1)).any(dim=1).float()  # [B]\n",
    "        hr = hits.mean().item()\n",
    "\n",
    "        # ranks\n",
    "        # if hit: rank = position+1 else inf\n",
    "        match = (topk == labels.unsqueeze(1))\n",
    "        # get first position if exists\n",
    "        pos = torch.where(match.any(dim=1), match.float().argmax(dim=1) + 1, torch.full((B,), 10**9, device=logits.device))\n",
    "        rr = torch.where(pos < 10**8, 1.0 / pos.float(), torch.zeros_like(pos, dtype=torch.float))\n",
    "        mrr = rr.mean().item()\n",
    "\n",
    "        ndcg = torch.where(pos < 10**8, 1.0 / torch.log2(pos.float() + 1.0), torch.zeros_like(pos, dtype=torch.float)).mean().item()\n",
    "\n",
    "        res[f\"HR@{k}\"] = hr\n",
    "        res[f\"MRR@{k}\"] = mrr\n",
    "        res[f\"NDCG@{k}\"] = ndcg\n",
    "\n",
    "    res[\"_n_examples\"] = int(B)\n",
    "    return res\n",
    "\n",
    "print(\"[12B-08] ✅ Metrics ready (PAD excluded).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d72696",
   "metadata": {},
   "source": [
    "Reptile meta-train loop on SOURCE tasks (CPU-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "967bb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12B-09] REPO_ROOT: C:\\mooc-coldstart-session-meta\n",
      "[12B-09] ✅ PROTO: {'K_LIST': [5, 10, 20], 'MAX_PREFIX_LEN': 20, 'CAP_ENABLED': True, 'CAP_SESSION_LEN': 200, 'CAP_STRATEGY': 'take_last'}\n",
      "[12B-09] Source dirs from seq_dir: train=C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\train | val=C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\val | test=C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\test\n",
      "[12B-09] Source shards: train= 1024 val= 1024 test= 1024\n",
      "[12B-09] Source vocab: vocab_json=C:\\mooc-coldstart-session-meta\\data\\processed\\session_sequences\\source_sessions_20251229_232834\\source_vocab_items_20251229_232834.json\n",
      "[12B-09] VOCAB_SIZE_SOURCE: 1620 | PAD/UNK: 0 1\n",
      "[12B-09] scanned_files=10 sessions_seen=64,811 yielded=6,475 short=0 unk_labels=0 elapsed=0.5s\n",
      "[12B-09] scanned_files=20 sessions_seen=130,232 yielded=12,974 short=0 unk_labels=0 elapsed=1.1s\n",
      "[12B-09] scanned_files=30 sessions_seen=195,010 yielded=19,405 short=0 unk_labels=0 elapsed=1.6s\n",
      "[12B-09] scanned_files=40 sessions_seen=260,266 yielded=26,054 short=0 unk_labels=0 elapsed=2.0s\n",
      "[12B-09] scanned_files=50 sessions_seen=325,575 yielded=32,540 short=0 unk_labels=0 elapsed=2.5s\n",
      "[12B-09] DONE files=51 sessions_seen=325,575 yielded=32,540 short=0 unk_labels=0 unk_tokens_total=0 elapsed=2.5s\n",
      "[12B-09] [train] buffers built | total_seen=32,540 | tasks_ok=5 | tasks_bad=0\n",
      "[12B-09] scanned_files=10 sessions_seen=8,206 yielded=841 short=0 unk_labels=0 elapsed=0.1s\n",
      "[12B-09] scanned_files=20 sessions_seen=16,470 yielded=1,668 short=0 unk_labels=0 elapsed=0.1s\n",
      "[12B-09] scanned_files=30 sessions_seen=24,748 yielded=2,463 short=0 unk_labels=0 elapsed=0.2s\n",
      "[12B-09] scanned_files=40 sessions_seen=32,915 yielded=3,290 short=0 unk_labels=0 elapsed=0.3s\n",
      "[12B-09] scanned_files=50 sessions_seen=40,753 yielded=4,065 short=0 unk_labels=0 elapsed=0.4s\n",
      "[12B-09] DONE files=51 sessions_seen=40,753 yielded=4,065 short=0 unk_labels=0 unk_tokens_total=0 elapsed=0.4s\n",
      "[12B-09] [val] buffers built | total_seen=4,065 | tasks_ok=5 | tasks_bad=0\n",
      "[12B-09] tasks(train) top: [('len_21_plus', 7520), ('len_03_05', 6840), ('len_06_10', 6163), ('len_11_20', 6080), ('len_01_02', 5937)]\n",
      "[12B-09] tasks(val)   top: [('len_21_plus', 949), ('len_03_05', 855), ('len_06_10', 797), ('len_11_20', 752), ('len_01_02', 712)]\n",
      "[12B-09] META_CFG: {'emb_dim': 64, 'hidden_dim': 128, 'dropout': 0.3, 'meta_lr': 0.0005, 'inner_lr': 0.01, 'inner_steps': 1, 'meta_steps': 2000, 'meta_batch_tasks': 4, 'grad_clip': 1.0, 'seed': 42, 'log_every': 100, 'eval_every': 250, 'val_episodes': 50}\n",
      "[12B-09] Starting meta-train on SOURCE tasks...\n",
      "[12B-09] step=100/2000 meta_query_loss~7.3759 elapsed=9.5s\n",
      "[12B-09] step=200/2000 meta_query_loss~7.4080 elapsed=20.3s\n",
      "[12B-09] EVAL step=250 | VAL(meta-adapt) HR@20=0.018000 | full={'HR@5': 0.004000000059604645, 'HR@10': 0.007000000104308128, 'HR@20': 0.018000000268220902, 'MRR@5': 0.0010000000149011613, 'MRR@10': 0.0013916666898876428, 'MRR@20': 0.0022064537089318036, 'NDCG@5': 0.0017227062582969666, 'NDCG@10': 0.0026834431663155557, 'NDCG@20': 0.00552954463288188, '_n_episodes': 50}\n",
      "[12B-09] step=300/2000 meta_query_loss~7.4184 elapsed=32.0s\n",
      "[12B-09] step=400/2000 meta_query_loss~7.4075 elapsed=43.3s\n",
      "[12B-09] step=500/2000 meta_query_loss~7.4211 elapsed=53.3s\n",
      "[12B-09] EVAL step=500 | VAL(meta-adapt) HR@20=0.009000 | full={'HR@5': 0.0010000000149011613, 'HR@10': 0.0020000000298023225, 'HR@20': 0.009000000134110451, 'MRR@5': 0.0002500000037252903, 'MRR@10': 0.00035000000149011613, 'MRR@20': 0.0009214286040514708, 'NDCG@5': 0.00043067656457424164, 'NDCG@10': 0.0007197413966059684, 'NDCG@20': 0.002597128413617611, '_n_episodes': 50}\n",
      "[12B-09] step=600/2000 meta_query_loss~7.4278 elapsed=63.9s\n",
      "[12B-09] step=700/2000 meta_query_loss~7.3463 elapsed=73.1s\n",
      "[12B-09] EVAL step=750 | VAL(meta-adapt) HR@20=0.016000 | full={'HR@5': 0.004000000059604645, 'HR@10': 0.010000000149011612, 'HR@20': 0.01600000023841858, 'MRR@5': 0.0010000000149011613, 'MRR@10': 0.0017000000178813935, 'MRR@20': 0.002175490236841142, 'NDCG@5': 0.0017227062582969666, 'NDCG@10': 0.00356269545853138, 'NDCG@20': 0.005153698734939098, '_n_episodes': 50}\n",
      "[12B-09] step=800/2000 meta_query_loss~7.3918 elapsed=84.5s\n",
      "[12B-09] step=900/2000 meta_query_loss~7.4266 elapsed=94.6s\n",
      "[12B-09] step=1000/2000 meta_query_loss~7.4383 elapsed=103.3s\n",
      "[12B-09] EVAL step=1000 | VAL(meta-adapt) HR@20=0.013000 | full={'HR@5': 0.0, 'HR@10': 0.00800000011920929, 'HR@20': 0.013000000193715095, 'MRR@5': 0.0, 'MRR@10': 0.0009111111145466566, 'MRR@20': 0.001321367546916008, 'NDCG@5': 0.0, 'NDCG@10': 0.0024300840497016906, 'NDCG@20': 0.003773686233907938, '_n_episodes': 50}\n",
      "[12B-09] step=1100/2000 meta_query_loss~7.4131 elapsed=113.4s\n",
      "[12B-09] step=1200/2000 meta_query_loss~7.3598 elapsed=123.5s\n",
      "[12B-09] EVAL step=1250 | VAL(meta-adapt) HR@20=0.016000 | full={'HR@5': 0.0020000000298023225, 'HR@10': 0.00800000011920929, 'HR@20': 0.01600000023841858, 'MRR@5': 0.0007500000111758709, 'MRR@10': 0.0015067460667341948, 'MRR@20': 0.0021077264845371247, 'NDCG@5': 0.0010616063326597215, 'NDCG@10': 0.002957736626267433, 'NDCG@20': 0.005038552358746529, '_n_episodes': 50}\n",
      "[12B-09] step=1300/2000 meta_query_loss~7.4173 elapsed=135.3s\n",
      "[12B-09] step=1400/2000 meta_query_loss~7.3831 elapsed=146.6s\n",
      "[12B-09] step=1500/2000 meta_query_loss~7.3980 elapsed=157.0s\n",
      "[12B-09] EVAL step=1500 | VAL(meta-adapt) HR@20=0.012000 | full={'HR@5': 0.0030000000447034836, 'HR@10': 0.007000000104308128, 'HR@20': 0.012000000178813934, 'MRR@5': 0.0007500000111758709, 'MRR@10': 0.0012400793936103583, 'MRR@20': 0.0016359127452597023, 'NDCG@5': 0.001292029693722725, 'NDCG@10': 0.0025428879633545877, 'NDCG@20': 0.0038684911653399465, '_n_episodes': 50}\n",
      "[12B-09] step=1600/2000 meta_query_loss~7.4086 elapsed=168.0s\n",
      "[12B-09] step=1700/2000 meta_query_loss~7.4005 elapsed=178.0s\n",
      "[12B-09] EVAL step=1750 | VAL(meta-adapt) HR@20=0.007000 | full={'HR@5': 0.0020000000298023225, 'HR@10': 0.005000000074505806, 'HR@20': 0.007000000104308128, 'MRR@5': 0.0005000000074505806, 'MRR@10': 0.000875000013038516, 'MRR@20': 0.0010074786515906454, 'NDCG@5': 0.0008613531291484833, 'NDCG@10': 0.0018077477812767028, 'NDCG@20': 0.0023058062233030797, '_n_episodes': 50}\n",
      "[12B-09] step=1800/2000 meta_query_loss~7.3924 elapsed=189.3s\n",
      "[12B-09] step=1900/2000 meta_query_loss~7.3586 elapsed=198.8s\n",
      "[12B-09] step=2000/2000 meta_query_loss~7.4090 elapsed=207.3s\n",
      "[12B-09] EVAL step=2000 | VAL(meta-adapt) HR@20=0.016000 | full={'HR@5': 0.0030000000447034836, 'HR@10': 0.005000000074505806, 'HR@20': 0.016000000312924385, 'MRR@5': 0.0007500000111758709, 'MRR@10': 0.0010416666883975268, 'MRR@20': 0.0018509804457426072, 'NDCG@5': 0.001292029693722725, 'NDCG@10': 0.0019637017697095873, 'NDCG@20': 0.004804056789726019, '_n_episodes': 50}\n",
      "[12B-09] ✅ Meta-train done. best_step=250 best_val_HR@20=0.018000\n",
      "[12B-09] ✅ Saved: C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\\meta_model_source.pt\n",
      "[12B-09] ✅ Wrote report: C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\\report.json\n",
      "[12B-09] REPORT_DIR: C:\\mooc-coldstart-session-meta\\reports\\12B_meta_train_on_source\\20260104_165117\n",
      "\n",
      "[12B-09] CHECKPOINT Z\n",
      "Paste: REPORT_DIR + best_step/best_val_hr20 + final_val_meta_adapt.\n"
     ]
    }
   ],
   "source": [
    "# [CELL 12B-09] Build task buffers (train/val) + meta-train loop (Reptile-ish) + checkpoints\n",
    "# Assumes:\n",
    "#   - DL_CFG loaded in [12B-01]\n",
    "#   - TASK_META/TASK_CFG/TASK_COV loaded in [12B-02]\n",
    "# This cell is self-healing for protocol/paths/vocab.\n",
    "\n",
    "import time, math, json, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09A) Resolve protocol + source paths + vocab robustly\n",
    "# ----------------------------\n",
    "t0 = time.time()\n",
    "REPO_ROOT = REPO_ROOT if \"REPO_ROOT\" in globals() else Path.cwd().resolve()\n",
    "print(\"[12B-09] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "assert \"DL_CFG\" in globals() and isinstance(DL_CFG, dict), \"[12B-09] DL_CFG not found; run [12B-01]\"\n",
    "assert \"TASK_META\" in globals() and isinstance(TASK_META, dict), \"[12B-09] TASK_META not found; run [12B-02]\"\n",
    "assert \"TASK_CFG\" in globals() and isinstance(TASK_CFG, dict), \"[12B-09] TASK_CFG not found; run [12B-02]\"\n",
    "\n",
    "def pick_first(d: dict, keys: list[str], name: str):\n",
    "    for k in keys:\n",
    "        if k in d and d[k]:\n",
    "            return d[k], k\n",
    "    raise KeyError(f\"[12B-09] Missing '{name}'. Tried {keys}. Available keys={list(d.keys())}\")\n",
    "\n",
    "# Protocol: prefer TASK_META['protocol'] (frozen) then DL_CFG['protocol']\n",
    "PROTO = None\n",
    "if \"protocol\" in TASK_META and isinstance(TASK_META[\"protocol\"], dict):\n",
    "    PROTO = TASK_META[\"protocol\"]\n",
    "elif \"protocol\" in DL_CFG and isinstance(DL_CFG[\"protocol\"], dict):\n",
    "    PROTO_RAW = DL_CFG[\"protocol\"]\n",
    "    max_prefix_len, _ = pick_first(PROTO_RAW, [\"MAX_PREFIX_LEN\", \"max_prefix_len\", \"max_len\", \"seq_len\"], \"MAX_PREFIX_LEN\")\n",
    "    slsp = PROTO_RAW.get(\"source_long_session_policy\", {}) or {}\n",
    "    cap_enabled = bool(slsp.get(\"enabled\", True))\n",
    "    cap_session_len = int(slsp.get(\"cap_session_len\", 200))\n",
    "    cap_strategy = str(slsp.get(\"cap_strategy\", \"take_last\"))\n",
    "    k_list = PROTO_RAW.get(\"K_LIST\") or PROTO_RAW.get(\"k_list\") or [5, 10, 20]\n",
    "    PROTO = {\n",
    "        \"K_LIST\": list(map(int, k_list)),\n",
    "        \"MAX_PREFIX_LEN\": int(max_prefix_len),\n",
    "        \"CAP_ENABLED\": bool(cap_enabled),\n",
    "        \"CAP_SESSION_LEN\": int(cap_session_len),\n",
    "        \"CAP_STRATEGY\": cap_strategy,\n",
    "    }\n",
    "else:\n",
    "    raise KeyError(\"[12B-09] Could not find protocol in TASK_META or DL_CFG\")\n",
    "\n",
    "print(\"[12B-09] ✅ PROTO:\", PROTO)\n",
    "assert PROTO[\"MAX_PREFIX_LEN\"] == 20, \"[12B-09] Protocol drift: MAX_PREFIX_LEN != 20\"\n",
    "assert PROTO[\"CAP_ENABLED\"] is True, \"[12B-09] Protocol drift: CAP_ENABLED != True\"\n",
    "assert PROTO[\"CAP_SESSION_LEN\"] == 200, \"[12B-09] Protocol drift: CAP_SESSION_LEN != 200\"\n",
    "assert PROTO[\"CAP_STRATEGY\"] == \"take_last\", \"[12B-09] Protocol drift: CAP_STRATEGY != take_last\"\n",
    "assert PROTO[\"K_LIST\"] == [5, 10, 20], \"[12B-09] Protocol drift: K_LIST != [5,10,20]\"\n",
    "\n",
    "MAX_LEN = int(PROTO[\"MAX_PREFIX_LEN\"])\n",
    "CAP_ENABLED = bool(PROTO[\"CAP_ENABLED\"])\n",
    "CAP_SESSION_LEN = int(PROTO[\"CAP_SESSION_LEN\"])\n",
    "CAP_STRATEGY = str(PROTO[\"CAP_STRATEGY\"])\n",
    "K_LIST = list(map(int, PROTO[\"K_LIST\"]))\n",
    "\n",
    "# Source paths: your schema has seq_dir + *_glob + vocab_json\n",
    "SRC = DL_CFG[\"source\"]\n",
    "seq_dir_raw, _ = pick_first(SRC, [\"seq_dir\"], \"source seq_dir\")\n",
    "seq_dir = Path(seq_dir_raw)\n",
    "\n",
    "TRAIN_DIR = seq_dir / \"train\"\n",
    "VAL_DIR   = seq_dir / \"val\"\n",
    "TEST_DIR  = seq_dir / \"test\"\n",
    "print(f\"[12B-09] Source dirs from seq_dir: train={TRAIN_DIR} | val={VAL_DIR} | test={TEST_DIR}\")\n",
    "assert TRAIN_DIR.exists() and VAL_DIR.exists() and TEST_DIR.exists(), \"[12B-09] Missing one of train/val/test dirs\"\n",
    "\n",
    "train_files = sorted(TRAIN_DIR.glob(\"*.parquet\"))\n",
    "val_files   = sorted(VAL_DIR.glob(\"*.parquet\"))\n",
    "test_files  = sorted(TEST_DIR.glob(\"*.parquet\"))\n",
    "print(\"[12B-09] Source shards:\", \"train=\", len(train_files), \"val=\", len(val_files), \"test=\", len(test_files))\n",
    "assert len(train_files) == 1024 and len(val_files) == 1024 and len(test_files) == 1024, \"[12B-09] shard count mismatch\"\n",
    "\n",
    "vocab_raw, vocab_key = pick_first(SRC, [\"vocab_json\", \"vocab_items_path\", \"vocab\"], \"source vocab path\")\n",
    "SRC_VOCAB_PATH = Path(vocab_raw)\n",
    "print(f\"[12B-09] Source vocab: {vocab_key}={SRC_VOCAB_PATH}\")\n",
    "assert SRC_VOCAB_PATH.exists(), f\"[12B-09] Missing source vocab: {SRC_VOCAB_PATH}\"\n",
    "\n",
    "source_vocab = json.loads(SRC_VOCAB_PATH.read_text(encoding=\"utf-8\"))\n",
    "VOCAB_SIZE_SOURCE = int(source_vocab[\"vocab_size\"])\n",
    "PAD_ID_SOURCE = int(source_vocab.get(\"pad_id\", 0))\n",
    "UNK_ID_SOURCE = int(source_vocab.get(\"unk_id\", 1))\n",
    "source_token_to_id = source_vocab[\"item2id\"]\n",
    "print(\"[12B-09] VOCAB_SIZE_SOURCE:\", VOCAB_SIZE_SOURCE, \"| PAD/UNK:\", PAD_ID_SOURCE, UNK_ID_SOURCE)\n",
    "assert isinstance(source_token_to_id, dict) and len(source_token_to_id) == VOCAB_SIZE_SOURCE, \"[12B-09] item2id invalid\"\n",
    "\n",
    "# Detect seq col (expected)\n",
    "probe_df = pd.read_parquet(train_files[0])\n",
    "assert \"items\" in probe_df.columns, f\"[12B-09] Expected 'items' col. Got: {list(probe_df.columns)}\"\n",
    "SEQ_COL = \"items\"\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09B) Pair construction helpers\n",
    "# ----------------------------\n",
    "def cap_session_tokens(seq_tokens: list[str]) -> list[str]:\n",
    "    if not CAP_ENABLED:\n",
    "        return seq_tokens\n",
    "    if len(seq_tokens) <= CAP_SESSION_LEN:\n",
    "        return seq_tokens\n",
    "    if CAP_STRATEGY == \"take_last\":\n",
    "        return seq_tokens[-CAP_SESSION_LEN:]\n",
    "    raise ValueError(f\"[12B-09] Unknown CAP_STRATEGY={CAP_STRATEGY}\")\n",
    "\n",
    "def session_to_one_pair(seq_tokens: list[str]):\n",
    "    seq_tokens = cap_session_tokens(seq_tokens)\n",
    "    if len(seq_tokens) < 2:\n",
    "        return None\n",
    "\n",
    "    prefix_tokens = seq_tokens[:-1]\n",
    "    label_token = seq_tokens[-1]\n",
    "\n",
    "    x_ids = [source_token_to_id.get(tok, UNK_ID_SOURCE) for tok in prefix_tokens]\n",
    "    y_id = source_token_to_id.get(label_token, UNK_ID_SOURCE)\n",
    "\n",
    "    # filter UNK labels\n",
    "    if y_id == UNK_ID_SOURCE:\n",
    "        return None\n",
    "\n",
    "    # cap to MAX_LEN (take last)\n",
    "    if len(x_ids) > MAX_LEN:\n",
    "        x_ids = x_ids[-MAX_LEN:]\n",
    "\n",
    "    m = [1] * len(x_ids)\n",
    "\n",
    "    # right pad\n",
    "    if len(x_ids) < MAX_LEN:\n",
    "        pad_n = MAX_LEN - len(x_ids)\n",
    "        x_ids = x_ids + [PAD_ID_SOURCE] * pad_n\n",
    "        m = m + [0] * pad_n\n",
    "\n",
    "    return (np.array(x_ids, dtype=np.int64), np.array(m, dtype=np.int64), int(y_id), int(len(prefix_tokens)))\n",
    "\n",
    "def stable_mod(value, mod: int) -> int:\n",
    "    if mod <= 1:\n",
    "        return 0\n",
    "    if isinstance(value, (int, np.integer)):\n",
    "        return int(value) % mod\n",
    "    s = str(value).encode(\"utf-8\")\n",
    "    h = hashlib.blake2b(s, digest_size=8).digest()\n",
    "    as_int = int.from_bytes(h, byteorder=\"little\", signed=False)\n",
    "    return as_int % mod\n",
    "\n",
    "LEN_BINS = [\n",
    "    (\"len_01_02\", 1, 2),\n",
    "    (\"len_03_05\", 3, 5),\n",
    "    (\"len_06_10\", 6, 10),\n",
    "    (\"len_11_20\", 11, 20),\n",
    "    (\"len_21_plus\", 21, 10**9),\n",
    "]\n",
    "def task_key_from_len(prefix_len: int) -> str:\n",
    "    for name, lo, hi in LEN_BINS:\n",
    "        if lo <= prefix_len <= hi:\n",
    "            return name\n",
    "    return \"len_21_plus\"\n",
    "\n",
    "def iter_pairs_from_files(files, sample_mod: int, sample_rem: int, seed: int, max_files: int | None = None):\n",
    "    n_files = 0\n",
    "    n_sessions = 0\n",
    "    yielded = 0\n",
    "    short = 0\n",
    "    unk_labels = 0\n",
    "    unk_tokens_total = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for fp in files:\n",
    "        n_files += 1\n",
    "        if max_files is not None and n_files > max_files:\n",
    "            break\n",
    "\n",
    "        df = pd.read_parquet(fp, columns=[\"session_id\", \"session_length\", SEQ_COL])\n",
    "        for sid, slen, seq in zip(df[\"session_id\"].tolist(), df[\"session_length\"].tolist(), df[SEQ_COL].tolist()):\n",
    "            n_sessions += 1\n",
    "\n",
    "            if sample_mod > 1 and stable_mod(sid, sample_mod) != sample_rem:\n",
    "                continue\n",
    "\n",
    "            pair = session_to_one_pair(list(seq))\n",
    "            if pair is None:\n",
    "                short += 1\n",
    "                continue\n",
    "\n",
    "            x, m, y, prefix_len = pair\n",
    "            unk_tokens_total += int((x == UNK_ID_SOURCE).sum())\n",
    "            if y == UNK_ID_SOURCE:\n",
    "                unk_labels += 1\n",
    "                continue\n",
    "\n",
    "            task_key = task_key_from_len(prefix_len)\n",
    "            yielded += 1\n",
    "            yield task_key, x, m, y\n",
    "\n",
    "        if n_files % 10 == 0:\n",
    "            elapsed = time.time() - t_start\n",
    "            print(f\"[12B-09] scanned_files={n_files} sessions_seen={n_sessions:,} yielded={yielded:,} short={short:,} unk_labels={unk_labels:,} elapsed={elapsed:.1f}s\")\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    print(f\"[12B-09] DONE files={n_files} sessions_seen={n_sessions:,} yielded={yielded:,} short={short:,} unk_labels={unk_labels:,} unk_tokens_total={unk_tokens_total:,} elapsed={elapsed:.1f}s\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09C) Reservoir sampling per task buffer\n",
    "# ----------------------------\n",
    "MAX_BUF = int(TASK_CFG[\"max_pairs_per_task_buffer\"])\n",
    "NEED_PER_TASK = int(TASK_CFG[\"n_support\"]) + int(TASK_CFG[\"n_query\"])\n",
    "assert NEED_PER_TASK <= MAX_BUF, \"[12B-09] n_support+n_query must be <= max_pairs_per_task_buffer\"\n",
    "\n",
    "def reservoir_push(buf, item, rng: np.random.Generator, seen_count: int, cap: int):\n",
    "    if len(buf) < cap:\n",
    "        buf.append(item)\n",
    "        return\n",
    "    j = rng.integers(0, seen_count)\n",
    "    if j < cap:\n",
    "        buf[int(j)] = item\n",
    "\n",
    "def build_task_buffers(files, split_name: str, seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    buffers = {}\n",
    "    seen_per_task = {}\n",
    "    total_seen = 0\n",
    "\n",
    "    gen = iter_pairs_from_files(\n",
    "        files,\n",
    "        sample_mod=int(TASK_CFG[\"sample_mod\"]),\n",
    "        sample_rem=int(TASK_CFG[\"sample_rem\"]),\n",
    "        seed=seed,\n",
    "        max_files=int(TASK_CFG[\"max_files\"]) if TASK_CFG.get(\"max_files\") is not None else None\n",
    "    )\n",
    "\n",
    "    for task_key, x, m, y in gen:\n",
    "        total_seen += 1\n",
    "        if task_key not in buffers:\n",
    "            buffers[task_key] = []\n",
    "            seen_per_task[task_key] = 0\n",
    "        seen_per_task[task_key] += 1\n",
    "        reservoir_push(buffers[task_key], (x, m, y), rng, seen_per_task[task_key], MAX_BUF)\n",
    "\n",
    "    ok = {k: v for k, v in buffers.items() if len(v) >= NEED_PER_TASK}\n",
    "    bad = {k: len(v) for k, v in buffers.items() if len(v) < NEED_PER_TASK}\n",
    "\n",
    "    print(f\"[12B-09] [{split_name}] buffers built | total_seen={total_seen:,} | tasks_ok={len(ok)} | tasks_bad={len(bad)}\")\n",
    "    return ok, {\"pairs_total_seen\": total_seen, \"tasks_ok\": len(ok), \"tasks_bad\": len(bad), \"bad\": bad}\n",
    "\n",
    "BUFFERS_TRAIN, TRAIN_STATS = build_task_buffers(train_files, \"train\", seed=int(TASK_CFG[\"seed\"]) + 0)\n",
    "BUFFERS_VAL,   VAL_STATS   = build_task_buffers(val_files,   \"val\",   seed=int(TASK_CFG[\"seed\"]) + 1)\n",
    "\n",
    "min_tasks_required = int(TASK_CFG.get(\"min_tasks_required\", 1))\n",
    "if bool(TASK_CFG.get(\"require_multi_task\", False)):\n",
    "    assert len(BUFFERS_TRAIN) >= min_tasks_required, f\"[12B-09] Not enough tasks in train buffers: {len(BUFFERS_TRAIN)} < {min_tasks_required}\"\n",
    "    assert len(BUFFERS_VAL)   >= min_tasks_required, f\"[12B-09] Not enough tasks in val buffers: {len(BUFFERS_VAL)} < {min_tasks_required}\"\n",
    "\n",
    "print(\"[12B-09] tasks(train) top:\", sorted([(k, len(v)) for k, v in BUFFERS_TRAIN.items()], key=lambda x: -x[1])[:5])\n",
    "print(\"[12B-09] tasks(val)   top:\", sorted([(k, len(v)) for k, v in BUFFERS_VAL.items()], key=lambda x: -x[1])[:5])\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09D) Episode sampler\n",
    "# ----------------------------\n",
    "def sample_episode(buffers: dict, rng: np.random.Generator, n_support: int, n_query: int):\n",
    "    task_key = rng.choice(list(buffers.keys()))\n",
    "    pool = buffers[task_key]\n",
    "    idx = rng.choice(len(pool), size=(n_support + n_query), replace=False)\n",
    "    sup = [pool[i] for i in idx[:n_support]]\n",
    "    qry = [pool[i] for i in idx[n_support:]]\n",
    "    def stack(examples):\n",
    "        x = torch.tensor(np.stack([e[0] for e in examples], axis=0), dtype=torch.long)\n",
    "        m = torch.tensor(np.stack([e[1] for e in examples], axis=0), dtype=torch.long)\n",
    "        y = torch.tensor([e[2] for e in examples], dtype=torch.long)\n",
    "        return x, m, y\n",
    "    return task_key, stack(sup), stack(qry)\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09E) Model\n",
    "# ----------------------------\n",
    "class GRU4RecDropout(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, pad_id: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        emb = self.emb(input_ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h = self.gru(packed)\n",
    "        h_last = self.drop(h[-1])\n",
    "        return self.out(h_last)\n",
    "\n",
    "def make_lengths(attn_mask: torch.Tensor) -> torch.Tensor:\n",
    "    return attn_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09F) Metrics\n",
    "# ----------------------------\n",
    "def topk_metrics(logits: torch.Tensor, labels: torch.Tensor, ks=(5,10,20)):\n",
    "    res = {}\n",
    "    with torch.no_grad():\n",
    "        max_k = max(ks)\n",
    "        topk = torch.topk(logits, k=max_k, dim=1).indices\n",
    "        for k in ks:\n",
    "            hit = (topk[:, :k] == labels.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "            res[f\"HR@{k}\"] = hit\n",
    "        for k in ks:\n",
    "            preds = topk[:, :k]\n",
    "            eq = (preds == labels.unsqueeze(1))\n",
    "            ranks = torch.where(eq.any(dim=1), eq.float().argmax(dim=1) + 1, torch.zeros_like(labels))\n",
    "            rr = torch.where(ranks > 0, 1.0 / ranks.float(), torch.zeros_like(ranks).float()).mean().item()\n",
    "            res[f\"MRR@{k}\"] = rr\n",
    "            ndcg = torch.where(ranks > 0, 1.0 / torch.log2(ranks.float() + 1.0), torch.zeros_like(ranks).float()).mean().item()\n",
    "            res[f\"NDCG@{k}\"] = ndcg\n",
    "    return res\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09G) Meta-train (Reptile-ish)\n",
    "# ----------------------------\n",
    "META_CFG = {\n",
    "    \"emb_dim\": 64,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.3,\n",
    "    \"meta_lr\": 5e-4,\n",
    "    \"inner_lr\": 1e-2,\n",
    "    \"inner_steps\": 1,\n",
    "    \"meta_steps\": 2000,\n",
    "    \"meta_batch_tasks\": 4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"seed\": 42,\n",
    "    \"log_every\": 100,\n",
    "    \"eval_every\": 250,\n",
    "    \"val_episodes\": 50,\n",
    "}\n",
    "print(\"[12B-09] META_CFG:\", META_CFG)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.manual_seed(int(META_CFG[\"seed\"]))\n",
    "rng = np.random.default_rng(int(META_CFG[\"seed\"]))\n",
    "\n",
    "model = GRU4RecDropout(\n",
    "    vocab_size=VOCAB_SIZE_SOURCE,\n",
    "    emb_dim=int(META_CFG[\"emb_dim\"]),\n",
    "    hidden_dim=int(META_CFG[\"hidden_dim\"]),\n",
    "    pad_id=PAD_ID_SOURCE,\n",
    "    dropout=float(META_CFG[\"dropout\"]),\n",
    ").to(device)\n",
    "\n",
    "meta_opt = torch.optim.Adam(model.parameters(), lr=float(META_CFG[\"meta_lr\"]), weight_decay=0.0)\n",
    "\n",
    "def clone_state_dict(sd):\n",
    "    return {k: v.clone() for k, v in sd.items()}\n",
    "\n",
    "def load_state_dict_(m: nn.Module, sd: dict):\n",
    "    m.load_state_dict(sd, strict=True)\n",
    "\n",
    "# ✅ CRITICAL FIX: ensure adaptation runs with grads enabled even if caller is in no_grad\n",
    "def adapt_one_task(base_sd: dict, support_batch, inner_lr: float, inner_steps: int):\n",
    "    with torch.enable_grad():\n",
    "        fast = GRU4RecDropout(\n",
    "            vocab_size=VOCAB_SIZE_SOURCE,\n",
    "            emb_dim=int(META_CFG[\"emb_dim\"]),\n",
    "            hidden_dim=int(META_CFG[\"hidden_dim\"]),\n",
    "            pad_id=PAD_ID_SOURCE,\n",
    "            dropout=float(META_CFG[\"dropout\"]),\n",
    "        ).to(device)\n",
    "        load_state_dict_(fast, base_sd)\n",
    "        fast.train()\n",
    "\n",
    "        x_s, m_s, y_s = support_batch\n",
    "        x_s = x_s.to(device)\n",
    "        m_s = m_s.to(device)\n",
    "        y_s = y_s.to(device)\n",
    "        lengths = make_lengths(m_s)\n",
    "\n",
    "        opt = torch.optim.SGD(fast.parameters(), lr=inner_lr)\n",
    "        for _ in range(inner_steps):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = fast(x_s, lengths)\n",
    "            loss = F.cross_entropy(logits, y_s, ignore_index=PAD_ID_SOURCE)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(fast.parameters(), float(META_CFG[\"grad_clip\"]))\n",
    "            opt.step()\n",
    "\n",
    "        return clone_state_dict(fast.state_dict())\n",
    "\n",
    "# ✅ CRITICAL FIX: DO NOT decorate this with @torch.no_grad\n",
    "def eval_meta(model: nn.Module, buffers: dict, n_episodes: int = 50, seed: int = 123):\n",
    "    rng_eval = np.random.default_rng(seed)\n",
    "    model.eval()\n",
    "\n",
    "    agg = {f\"HR@{k}\": 0.0 for k in K_LIST}\n",
    "    agg |= {f\"MRR@{k}\": 0.0 for k in K_LIST}\n",
    "    agg |= {f\"NDCG@{k}\": 0.0 for k in K_LIST}\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        _, sup, qry = sample_episode(buffers, rng_eval, int(TASK_CFG[\"n_support\"]), int(TASK_CFG[\"n_query\"]))\n",
    "        base_sd = clone_state_dict(model.state_dict())\n",
    "\n",
    "        # adaptation MUST have grads\n",
    "        fast_sd = adapt_one_task(base_sd, sup, inner_lr=float(META_CFG[\"inner_lr\"]), inner_steps=int(META_CFG[\"inner_steps\"]))\n",
    "\n",
    "        # query eval no grads\n",
    "        with torch.no_grad():\n",
    "            fast = GRU4RecDropout(\n",
    "                vocab_size=VOCAB_SIZE_SOURCE,\n",
    "                emb_dim=int(META_CFG[\"emb_dim\"]),\n",
    "                hidden_dim=int(META_CFG[\"hidden_dim\"]),\n",
    "                pad_id=PAD_ID_SOURCE,\n",
    "                dropout=float(META_CFG[\"dropout\"]),\n",
    "            ).to(device)\n",
    "            load_state_dict_(fast, fast_sd)\n",
    "            fast.eval()\n",
    "\n",
    "            x_q, m_q, y_q = qry\n",
    "            x_q = x_q.to(device)\n",
    "            m_q = m_q.to(device)\n",
    "            y_q = y_q.to(device)\n",
    "            lengths = make_lengths(m_q)\n",
    "            logits = fast(x_q, lengths)\n",
    "\n",
    "            res = topk_metrics(logits, y_q, ks=K_LIST)\n",
    "            for k in res:\n",
    "                agg[k] += res[k]\n",
    "\n",
    "    for k in agg:\n",
    "        agg[k] /= float(n_episodes)\n",
    "    agg[\"_n_episodes\"] = n_episodes\n",
    "    return agg\n",
    "\n",
    "print(\"[12B-09] Starting meta-train on SOURCE tasks...\")\n",
    "start = time.time()\n",
    "best_val = -1.0\n",
    "best_step = 0\n",
    "best_sd = None\n",
    "\n",
    "for step in range(1, int(META_CFG[\"meta_steps\"]) + 1):\n",
    "    model.train()\n",
    "    base_sd = clone_state_dict(model.state_dict())\n",
    "    deltas = {k: torch.zeros_like(v).detach() for k, v in base_sd.items()}\n",
    "    meta_q_loss = 0.0\n",
    "\n",
    "    for _ in range(int(META_CFG[\"meta_batch_tasks\"])):\n",
    "        _, sup, qry = sample_episode(BUFFERS_TRAIN, rng, int(TASK_CFG[\"n_support\"]), int(TASK_CFG[\"n_query\"]))\n",
    "        fast_sd = adapt_one_task(base_sd, sup, inner_lr=float(META_CFG[\"inner_lr\"]), inner_steps=int(META_CFG[\"inner_steps\"]))\n",
    "\n",
    "        # logging-only query loss\n",
    "        with torch.no_grad():\n",
    "            fast = GRU4RecDropout(\n",
    "                vocab_size=VOCAB_SIZE_SOURCE,\n",
    "                emb_dim=int(META_CFG[\"emb_dim\"]),\n",
    "                hidden_dim=int(META_CFG[\"hidden_dim\"]),\n",
    "                pad_id=PAD_ID_SOURCE,\n",
    "                dropout=float(META_CFG[\"dropout\"]),\n",
    "            ).to(device)\n",
    "            load_state_dict_(fast, fast_sd)\n",
    "            fast.eval()\n",
    "            x_q, m_q, y_q = qry\n",
    "            x_q, m_q, y_q = x_q.to(device), m_q.to(device), y_q.to(device)\n",
    "            logits_q = fast(x_q, make_lengths(m_q))\n",
    "            meta_q_loss += float(F.cross_entropy(logits_q, y_q, ignore_index=PAD_ID_SOURCE).item())\n",
    "\n",
    "        for k in deltas:\n",
    "            deltas[k] += (fast_sd[k].detach() - base_sd[k].detach())\n",
    "\n",
    "    meta_q_loss /= float(META_CFG[\"meta_batch_tasks\"])\n",
    "\n",
    "    # Apply Reptile update\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in deltas:\n",
    "                p.add_(float(META_CFG[\"meta_lr\"]) * (deltas[name] / float(META_CFG[\"meta_batch_tasks\"])))\n",
    "\n",
    "    if step % int(META_CFG[\"log_every\"]) == 0:\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"[12B-09] step={step}/{META_CFG['meta_steps']} meta_query_loss~{meta_q_loss:.4f} elapsed={elapsed:.1f}s\")\n",
    "\n",
    "    if step % int(META_CFG[\"eval_every\"]) == 0:\n",
    "        val_res = eval_meta(model, BUFFERS_VAL, n_episodes=int(META_CFG[\"val_episodes\"]), seed=1000 + step)\n",
    "        hr20 = float(val_res[\"HR@20\"])\n",
    "        print(f\"[12B-09] EVAL step={step} | VAL(meta-adapt) HR@20={hr20:.6f} | full={val_res}\")\n",
    "        if hr20 > best_val + 1e-6:\n",
    "            best_val = hr20\n",
    "            best_step = step\n",
    "            best_sd = clone_state_dict(model.state_dict())\n",
    "\n",
    "if best_sd is not None:\n",
    "    load_state_dict_(model, best_sd)\n",
    "print(f\"[12B-09] ✅ Meta-train done. best_step={best_step} best_val_HR@20={best_val:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12B-09H) Save checkpoint + report\n",
    "# ----------------------------\n",
    "RUN_TAG = RUN_TAG if \"RUN_TAG\" in globals() else TASK_META.get(\"run_tag\", \"unknown_run_tag\")\n",
    "REPORT_DIR = Path(REPO_ROOT) / \"reports\" / \"12B_meta_train_on_source\" / str(RUN_TAG)\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt_path = REPORT_DIR / \"meta_model_source.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"run_tag\": str(RUN_TAG),\n",
    "        \"task_run_dir\": str(Path(REPO_ROOT) / \"reports\" / \"12A_task_builder_for_meta\" / TASK_META[\"run_tag\"]),\n",
    "        \"proto\": PROTO,\n",
    "        \"task_cfg\": TASK_CFG,\n",
    "        \"meta_cfg\": META_CFG,\n",
    "        \"vocab_size_source\": VOCAB_SIZE_SOURCE,\n",
    "        \"pad_id_source\": PAD_ID_SOURCE,\n",
    "        \"unk_id_source\": UNK_ID_SOURCE,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"best_step\": best_step,\n",
    "        \"best_val_hr20\": best_val,\n",
    "    },\n",
    "    ckpt_path\n",
    ")\n",
    "\n",
    "final_val = eval_meta(model, BUFFERS_VAL, n_episodes=100, seed=1234)\n",
    "report = {\n",
    "    \"run_tag\": str(RUN_TAG),\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "    \"proto\": PROTO,\n",
    "    \"task_cfg\": TASK_CFG,\n",
    "    \"meta_cfg\": META_CFG,\n",
    "    \"buffers\": {\n",
    "        \"train\": {\"tasks\": len(BUFFERS_TRAIN), **TRAIN_STATS},\n",
    "        \"val\":   {\"tasks\": len(BUFFERS_VAL),   **VAL_STATS},\n",
    "    },\n",
    "    \"best_step\": best_step,\n",
    "    \"best_val_hr20\": best_val,\n",
    "    \"final_val_meta_adapt\": final_val,\n",
    "    \"notes\": [\n",
    "        \"Tasks are multi-task within SOURCE derived from real session_length bins (len_bin).\",\n",
    "        \"UNK labels are filtered out during pair construction.\",\n",
    "        \"Reservoir sampling used for buffer cap to avoid recency bias.\",\n",
    "        \"Adaptation (support) runs under torch.enable_grad(); query eval uses torch.no_grad().\",\n",
    "    ],\n",
    "}\n",
    "(REPORT_DIR / \"report.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"[12B-09] ✅ Saved:\", ckpt_path)\n",
    "print(\"[12B-09] ✅ Wrote report:\", REPORT_DIR / \"report.json\")\n",
    "print(\"[12B-09] REPORT_DIR:\", str(REPORT_DIR))\n",
    "\n",
    "print(\"\\n[12B-09] CHECKPOINT Z\")\n",
    "print(\"Paste: REPORT_DIR + best_step/best_val_hr20 + final_val_meta_adapt.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
